{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumkh/ITI110_AgenticRAG/blob/main/LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVfMT6l1cobl"
      },
      "source": [
        "## AI Tutor Chatbot (Version 2.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmeoL-_lcs8Y"
      },
      "source": [
        "### Setting Up - Install Requirements (Restart Session after installation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MkI4XOWxSXvl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install install vllm huggingface_hub transformers langchain langchain_huggingface langgraph accelerate bitsandbytes langchain-core langchain-text-splitters langchain-community chromadb langchain-chroma langsmith docling langchain-docling sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#%pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "dx4Mp2j2NBep"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_2h8tWUc9yA"
      },
      "source": [
        "### Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lzrhAq9cbbU",
        "outputId": "a2618a1b-b201-4c51-f511-3dd790a05167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-14 06:23:54--  https://github.com/sumkh/NYP_Dataset/raw/refs/heads/main/Documents.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sumkh/NYP_Dataset/refs/heads/main/Documents.zip [following]\n",
            "--2025-02-14 06:23:55--  https://raw.githubusercontent.com/sumkh/NYP_Dataset/refs/heads/main/Documents.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18510909 (18M) [application/zip]\n",
            "Saving to: ‘Documents.zip.3’\n",
            "\n",
            "Documents.zip.3     100%[===================>]  17.65M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-02-14 06:23:55 (160 MB/s) - ‘Documents.zip.3’ saved [18510909/18510909]\n",
            "\n",
            "Archive:  /content/Documents.zip\n",
            "  inflating: Documents/general/Topic 1 Introduction to AI and AI on Azure.pdf  \n",
            "  inflating: Documents/general/deeplearningreview.docx  \n",
            "  inflating: Documents/general/slide_1.pptx  \n",
            "  inflating: Documents/mcq/mcq.csv   \n",
            "  inflating: Documents/mcq/mcq2.csv  \n",
            "  inflating: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/data_level0.bin  \n",
            "  inflating: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/header.bin  \n",
            "  inflating: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/length.bin  \n",
            " extracting: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/link_lists.bin  \n",
            "  inflating: general_db/chroma.sqlite3  \n",
            "  inflating: mcq_db/chroma.sqlite3   \n",
            "  inflating: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/data_level0.bin  \n",
            "  inflating: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/header.bin  \n",
            "  inflating: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/length.bin  \n",
            " extracting: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/link_lists.bin  \n",
            "  inflating: requirements.txt        \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Disable tokenizers parallelism, as it causes issues with multiprocessing\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # LangSmith for Observability\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"AgenticRAG\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=\"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\")\n",
        "\n",
        "# Download required files from Github repo\n",
        "!wget https://github.com/sumkh/NYP_Dataset/raw/refs/heads/main/Documents.zip\n",
        "!unzip -o /content/Documents.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Znk0gm1ce67",
        "outputId": "c04afc69-bc39-4442-8b13-6cbd06237e93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import json\n",
        "import hashlib\n",
        "import uuid\n",
        "import logging\n",
        "from typing import List, Optional, Union, Literal, Dict\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "# LangChain & related imports\n",
        "from langchain_core.tools import tool, StructuredTool\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever\n",
        "\n",
        "# Extraction for Documents\n",
        "from langchain_docling.loader import ExportType\n",
        "from langchain_docling import DoclingLoader\n",
        "from docling.chunking import HybridChunker\n",
        "\n",
        "# Extraction for HTML\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configurations and Get the API key from the environment variable\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_ki8T8DdE8C"
      },
      "source": [
        "### Load Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2SDpdN9Yck4w"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "#                         Document Extraction Functions\n",
        "# =============================================================================\n",
        "\n",
        "def extract_documents(doc_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Recursively collects all file paths from folder 'doc_path'.\n",
        "    Used by ExtractDocument.load_files() to find documents to parse.\n",
        "    \"\"\"\n",
        "    extracted_docs = []\n",
        "\n",
        "    for root, _, files in os.walk(doc_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            extracted_docs.append(file_path)\n",
        "    return extracted_docs\n",
        "\n",
        "\n",
        "def _generate_uuid(page_content: str) -> str:\n",
        "    \"\"\"Generate a UUID for a chunk of text using MD5 hashing.\"\"\"\n",
        "    md5_hash = hashlib.md5(page_content.encode()).hexdigest()\n",
        "    return str(uuid.UUID(md5_hash[0:32]))\n",
        "\n",
        "\n",
        "def load_file(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load a file from the given path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "\n",
        "    # Load the file and extract the text chunks\n",
        "    try:\n",
        "        loader = DoclingLoader(\n",
        "            file_path = file_path,\n",
        "            export_type = ExportType.DOC_CHUNKS,\n",
        "            chunker = HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
        "        )\n",
        "        docs = loader.load()\n",
        "        logger.info(f\"Total parsed doc-chunks: {len(docs)} from Source: {file_path}\")\n",
        "\n",
        "        for d in docs:\n",
        "            # Tag each document's chunk with the source file and a unique ID\n",
        "            doc = Document(\n",
        "                page_content=d.page_content,\n",
        "                metadata={\n",
        "                    \"source\": file_path,\n",
        "                    \"doc_id\": _generate_uuid(d.page_content),\n",
        "                    \"source_type\": \"file\",\n",
        "                }\n",
        "            )\n",
        "            _documents.append(doc)\n",
        "        logger.info(f\"Total generated LangChain document chunks: {len(_documents)}\\n.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {file_path}. Exception: {e}\\n.\")\n",
        "\n",
        "    return _documents\n",
        "\n",
        "\n",
        "# Define function to load documents from a folder\n",
        "def load_files_from_folder(doc_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load documents from the given folder path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "    # Extract all files path from the given folder\n",
        "    extracted_docs = extract_documents(doc_path)\n",
        "\n",
        "    # Iterate through each document and extract the text chunks\n",
        "    for file_path in extracted_docs:\n",
        "        _documents.extend(load_file(file_path))\n",
        "\n",
        "    return _documents\n",
        "\n",
        "# =============================================================================\n",
        "# Load structured data in csv file to LangChain Document format\n",
        "def load_mcq_csvfiles(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load structured data in mcq csv file from the given file path and return a list of Document object.\n",
        "    Expected format: each row of csv is comma separated into \"mcq_number\", \"mcq_type\", \"text_content\"\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "\n",
        "    # iterate through each csv file and load each row into _dict_per_question format\n",
        "    # Ensure we process only CSV files\n",
        "    if not file_path.endswith(\".csv\"):\n",
        "        return _documents  # Skip non-CSV files\n",
        "    try:\n",
        "        # Open and read the CSV file\n",
        "        with open(file_path, mode='r', encoding='utf-8') as file:\n",
        "            reader = csv.DictReader(file)\n",
        "            for row in reader:\n",
        "                # Ensure required columns exist in the row\n",
        "                if not all(k in row for k in [\"mcq_number\", \"mcq_type\", \"text_content\"]): # Ensure required columns exist and exclude header\n",
        "                    logger.error(f\"Skipping row due to missing fields: {row}\")\n",
        "                    continue\n",
        "                # Tag each row of csv is comma separated into \"mcq_number\", \"mcq_type\", \"text_content\"\n",
        "                doc = Document(\n",
        "                    page_content = row[\"text_content\"], # text_content segment is separated by \"|\"\n",
        "                    metadata={\n",
        "                        \"source\": f\"{file_path}_{row['mcq_number']}\",  # file_path + mcq_number\n",
        "                        \"doc_id\": _generate_uuid(f\"{file_path}_{row['mcq_number']}\"),  # Unique ID\n",
        "                        \"source_type\": row[\"mcq_type\"],  # MCQ type\n",
        "                    }\n",
        "                )\n",
        "                _documents.append(doc)\n",
        "            logger.info(f\"Successfully loaded {len(_documents)} LangChain document chunks from {file_path}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {file_path}. Exception: {e}\\n.\")\n",
        "\n",
        "    return _documents\n",
        "\n",
        "# Define function to load documents from a folder for structured data in csv file\n",
        "def load_files_from_folder_mcq(doc_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load mcq csv file from the given folder path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "    # Extract all files path from the given folder\n",
        "    extracted_docs = [\n",
        "        os.path.join(doc_path, file) for file in os.listdir(doc_path)\n",
        "        if file.endswith(\".csv\")  # Process only CSV files\n",
        "    ]\n",
        "\n",
        "    # Iterate through each document and extract the text chunks\n",
        "    for file_path in extracted_docs:\n",
        "        _documents.extend(load_mcq_csvfiles(file_path))\n",
        "\n",
        "    return _documents\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                         Website Extraction Functions\n",
        "# =============================================================================\n",
        "def _generate_uuid(page_content: str) -> str:\n",
        "    \"\"\"Generate a UUID for a chunk of text using MD5 hashing.\"\"\"\n",
        "    md5_hash = hashlib.md5(page_content.encode()).hexdigest()\n",
        "    return str(uuid.UUID(md5_hash[0:32]))\n",
        "\n",
        "def ensure_scheme(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    if not parsed_url.scheme:\n",
        "        return 'http://' + url  # Default to http, or use 'https://' if preferred\n",
        "    return url\n",
        "\n",
        "def extract_html(url: List[str]) -> List[Document]:\n",
        "    if isinstance(url, str):\n",
        "        url = [url]\n",
        "    \"\"\"\n",
        "    Extracts text from the HTML content of web pages listed in 'web_path'.\n",
        "    Returns a list of LangChain 'Document' objects.\n",
        "    \"\"\"\n",
        "    # Ensure all URLs have a scheme\n",
        "    web_paths = [ensure_scheme(u) for u in url]\n",
        "\n",
        "    loader = WebBaseLoader(web_paths)\n",
        "    loader.requests_per_second = 1\n",
        "    docs = loader.load()\n",
        "\n",
        "    # Iterate through each document, clean the content, removing excessive line return and store it in a LangChain Document\n",
        "    _documents = []\n",
        "    for doc in docs:\n",
        "        # Clean the concent\n",
        "        doc.page_content = doc.page_content.strip()\n",
        "        doc.page_content = doc.page_content.replace(\"\\n\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"\\r\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"\\t\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"  \", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"   \", \" \")\n",
        "\n",
        "        # Store it in a LangChain Document\n",
        "        web_doc = Document(\n",
        "            page_content=doc.page_content,\n",
        "            metadata={\n",
        "                \"source\": doc.metadata.get(\"source\"),\n",
        "                \"doc_id\": _generate_uuid(doc.page_content),\n",
        "                \"source_type\": \"web\"\n",
        "            }\n",
        "        )\n",
        "        _documents.append(web_doc)\n",
        "    return _documents\n",
        "\n",
        "# =============================================================================\n",
        "#                         Vector Store Initialisation\n",
        "# =============================================================================\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
        "\n",
        "# Initialise vector stores\n",
        "general_vs = Chroma(\n",
        "    collection_name=\"general_vstore\",\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"./general_db\"\n",
        ")\n",
        "\n",
        "mcq_vs = Chroma(\n",
        "    collection_name=\"mcq_vstore\",\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"./mcq_db\"\n",
        ")\n",
        "\n",
        "in_memory_vs = Chroma(\n",
        "    collection_name=\"in_memory_vstore\",\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "# Split the documents into smaller chunks for better embedding coverage\n",
        "def split_text_into_chunks(docs: List[Document]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Splits a list of Documents into smaller text chunks using\n",
        "    RecursiveCharacterTextSplitter while preserving metadata.\n",
        "    Returns a list of Document objects.\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000, # Split into chunks of 1000 characters\n",
        "        chunk_overlap=200, # Overlap by 200 characters\n",
        "        add_start_index=True\n",
        "    )\n",
        "    chunked_docs = splitter.split_documents(docs)\n",
        "    return chunked_docs # List of Document objects\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                         Retrieval Tools\n",
        "# =============================================================================\n",
        "\n",
        "# Define a simple similarity search retrieval tool on msq_vs\n",
        "class MCQRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"The input text to search for.\")\n",
        "    k: int = Field(10, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def mcq_retriever(input: str, k: int = 10) -> List[str]:\n",
        "    # Retrieve the top k most similar mcq question documents from the vector store\n",
        "    docs_func = mcq_vs.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\n",
        "        'k': k,\n",
        "        'filter':{\"source_type\": \"mcq_question\"}\n",
        "    },\n",
        "    )\n",
        "    docs_qns = docs_func.invoke(input, k=k)\n",
        "\n",
        "    # Extract the document IDs from the retrieved documents\n",
        "    doc_ids = [d.metadata.get(\"doc_id\") for d in docs_qns if \"doc_id\" in d.metadata]\n",
        "\n",
        "    # Retrieve full documents based on the doc_ids\n",
        "    docs = mcq_vs.get(where = {'doc_id': {\"$in\":doc_ids}})\n",
        "\n",
        "    qns_list = {}\n",
        "    for i, d in enumerate(docs['metadatas']):\n",
        "        qns_list[d['source'] + \" \" + d['source_type']] = docs['documents'][i]\n",
        "\n",
        "    return qns_list\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "mcq_retriever_tool = StructuredTool.from_function(\n",
        "    func = mcq_retriever,\n",
        "    name = \"MCQ Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve MCQ questions set when Human asks to generate a quiz related to a topic.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The search topic to retrieve MCQ questions set related to the topic.\n",
        "        - k (int): Number of question set to retrieve.\n",
        "        Example usage: input='What is AI?', k=5\n",
        "\n",
        "    Returns:\n",
        "    - A dict of MCQ questions:\n",
        "    Key: 'metadata of question' e.g. './Documents/mcq/mcq.csv_Qn31 mcq_question' with suffix ['question', 'answer', 'answer_reason', 'options', 'wrong_options_reason']\n",
        "    Value: Text Content\n",
        "\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = MCQRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Return the response as a list of strings\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Retrieve more documents with higher diversity using MMR (Maximal Marginal Relevance) from the general vector store\n",
        "# Useful if the dataset has many similar documents\n",
        "class GenRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"The input text to search for.\")\n",
        "    k: int = Field(10, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def gen_retriever(input: str, k: int = 10) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = general_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "general_retriever_tool = StructuredTool.from_function(\n",
        "    func = gen_retriever,\n",
        "    name = \"Assistant References Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve reference information from Assistant reference database for Human queries related to a topic or\n",
        "    and when Human asked to generate guides to learn or study about a topic.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "        Example usage: input='What is AI?', k=5\n",
        "    Returns:\n",
        "    - A list of retrieved document's content string.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = GenRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Return the content of the documents\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Retrieve more documents with higher diversity using MMR (Maximal Marginal Relevance) from the in-memory vector store\n",
        "# Query in-memory vector store only\n",
        "class InMemoryRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"The input text to search for.\")\n",
        "    k: int = Field(1, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def in_memory_retriever(input: str, k: int = 10) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "in_memory_retriever_tool = StructuredTool.from_function(\n",
        "    func = in_memory_retriever,\n",
        "    name = \"In-Memory Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool when Human ask Assistant to retrieve information from documents that Human has uploaded.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = InMemoryRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Whether to return the tool’s output directly\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Web Extraction Tool\n",
        "class WebExtractionRequest(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"The input text to search for.\")\n",
        "    url: str = Field(\n",
        "        ...,\n",
        "        title=\"url\",\n",
        "        description=\"urls to extract content from\"\n",
        "    )\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "# Extract content from a web URL, load into in_memory_vstore\n",
        "def extract_web_path_tool(input: str, url: str, k: int = 5) -> List[str]:\n",
        "    if isinstance(url, str):\n",
        "        url = [url]\n",
        "    \"\"\"\n",
        "    Extract content from the web URLs based on user's input.\n",
        "    Args:\n",
        "    - input: The input text to search for.\n",
        "    - url: URLs to extract content from.\n",
        "    - k: Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    # Extract content from the web\n",
        "    html_docs = extract_html(url)\n",
        "    if not html_docs:\n",
        "        return f\"No content extracted from {url}.\"\n",
        "\n",
        "    # Split the documents into smaller chunks for better embedding coverage\n",
        "    chunked_texts = split_text_into_chunks(html_docs)\n",
        "    in_memory_vs.add_documents(chunked_texts) # Add the chunked texts to the in-memory vector store\n",
        "\n",
        "    # Extract content from the in-memory vector store\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\n",
        "        'k': k,\n",
        "        'lambda_mult': 0.25,\n",
        "        'filter':{\"source\": {\"$in\": url}}\n",
        "    },\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "web_extraction_tool = StructuredTool.from_function(\n",
        "    func = extract_web_path_tool,\n",
        "    name = \"Web Extraction Tool\",\n",
        "    description = (\n",
        "        \"\"\"\n",
        "        Use this tool to extract content from web URLs based on Human messages,\n",
        "        \"\"\"\n",
        "    ),\n",
        "    args_schema = WebExtractionRequest,\n",
        "    return_direct = False, # Whether to return the tool’s output directly\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Ensemble Retrieval from General and In-Memory Vector Stores\n",
        "class EnsembleRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"The input text to search for.\")\n",
        "    k: int = Field(10, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def ensemble_retriever(input: str, k: int = 10) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    general_retrieval = general_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    in_memory_retrieval = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "        retrievers=[general_retrieval, in_memory_retrieval],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "    docs = ensemble_retriever.invoke(input)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "ensemble_retriever_tool = StructuredTool.from_function(\n",
        "    func = ensemble_retriever,\n",
        "    name = \"Ensemble Retriever Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve information from reference database and\n",
        "    extraction of documents that Human has uploaded.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = EnsembleRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model Using Huggingface (Experimental)"
      ],
      "metadata": {
        "id": "QNgX4ZVYrS8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "local_path = \"models/SmolLM2-Instruct\"\n",
        "\n",
        "snapshot_download(\n",
        "    repo_id=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
        "    local_dir=local_path,\n",
        "    revision=\"main\",\n",
        "    resume_download=True,\n",
        "    cache_dir=\"./cache\"\n",
        ")\n",
        "print(\"Model downloaded into \"+ local_path)"
      ],
      "metadata": {
        "id": "XA2wQpsTrZ8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/docs/transformers/main_classes/pipelines\n",
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=local_path)\n",
        "pipe(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ii5CWRZL-Yf",
        "outputId": "9d465320-ff9f-4172-acf6-e3abefb6534d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
              "   {'role': 'assistant',\n",
              "    'content': \"I'm SmolLM, an AI model trained by Hugging Face. I'm here to assist\"}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipe(messages)[0]['generated_text'][-1]['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivA2HbsXIpxN",
        "outputId": "aa316a11-d79c-465b-d9dc-acba6d63bf5f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm SmolLM, an AI model trained by Hugging Face. I'm here to assist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "local_path = \"models/SmolLM2-Instruct\"\n",
        "\n",
        "# https://python.langchain.com/docs/integrations/chat/huggingface/\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    #model_id=local_path,\n",
        "    model_id=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs=dict(\n",
        "        max_new_tokens=8192,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.03,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dA7Jd7daOvX",
        "outputId": "d9ca2abc-dda7-4240-c52d-ffd2a9f37e3a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | llm  # using llm directly from HuggingfacePipeline\n",
        "\n",
        "question = \"What is electroencephalography?\"\n",
        "\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UboTZh5YyH81",
        "outputId": "4f2c0fb9-d29e-4ae4-b75f-376bfcd6a7a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is electroencephalography?\n",
            "\n",
            "Answer: Let's think step by step. Electroencephalography (EEG) is a non-invasive neuroimaging technique used to measure the electrical activity of the brain. It is commonly used in clinical and research settings to diagnose and monitor neurological disorders, such as epilepsy, seizure disorders, and coma.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatHuggingFace(llm=llm, verbose=False)\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "\n",
        "events = model.invoke(messages)\n",
        "for event in events:\n",
        "    print(event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNHRrCY4ANLH",
        "outputId": "b645462d-9eba-4df9-e2de-8f33d7fd7254"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('content', \"<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nWho are you?<|im_end|>\\n<|im_start|>assistant\\nI'm SmolLM, an AI model trained on the Hugging Face library. I'm here to assist you with your questions and provide information on various topics. I'm designed to be friendly, helpful, and easy to understand. Feel free to ask me anything you'd like to know!\")\n",
            "('additional_kwargs', {})\n",
            "('response_metadata', {})\n",
            "('type', 'ai')\n",
            "('name', None)\n",
            "('id', 'run-89bd3b9d-fb08-4f25-bf3a-14e65b6147d1-0')\n",
            "('example', False)\n",
            "('tool_calls', [])\n",
            "('invalid_tool_calls', [])\n",
            "('usage_metadata', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"You are a helpful assistant.\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "])\n",
        "\n",
        "messages= template.format_messages(input=\"Who are you?\")\n",
        "\n",
        "events = model.invoke(messages)\n",
        "for event in events:\n",
        "    print(event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztFlzuyd05Db",
        "outputId": "6566d6b9-4d00-4148-8f19-953e3a398fb6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('content', \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nWho are you?<|im_end|>\\n<|im_start|>assistant\\nI'm an AI language model, developed to assist and answer questions to the best of my ability. I don't have personal experiences or opinions, but I can provide information on a wide range of topics. I'm here to help you with your queries, answer your questions, and provide information based on my training data.\")\n",
            "('additional_kwargs', {})\n",
            "('response_metadata', {})\n",
            "('type', 'ai')\n",
            "('name', None)\n",
            "('id', 'run-3a7d49ee-3ac1-4a18-b1b0-2414f7cd2933-0')\n",
            "('example', False)\n",
            "('tool_calls', [])\n",
            "('invalid_tool_calls', [])\n",
            "('usage_metadata', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "question = \"What is electroencephalography?\"\n",
        "\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYDtAHifi3fs",
        "outputId": "5d2f8642-d9de-4288-d826-94c68aa560c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is electroencephalography?\n",
            "\n",
            "Answer: Let's think step by step. Electroencephalography (EEG) is a non-invasive neuroimaging technique used to measure the electrical activity of the brain. It is commonly used in clinical and research settings to diagnose and monitor neurological disorders, such as epilepsy, seizure disorders, and coma.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chain.stream(question):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eM2O-qCj9sw",
        "outputId": "b959618b-89ff-438b-befa-f8dee2118176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Electroencephalography (EEG) is a non-invasive neuroimaging technique used to measure the electrical activity of the brain. It is commonly used in clinical and research settings to diagnose and monitor neurological disorders, such as epilepsy, seizure disorders, and coma."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuhaRfR6dLno"
      },
      "source": [
        "### Load Model and Serve Online (Experimental)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and run the model:\n",
        "!vllm serve \"HuggingFaceTB/SmolLM2-1.7B-Instruct\" --dtype=half"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdnTPhDHWUy1",
        "outputId": "144aaed9-42bb-44fd-edb1-15a73d885069"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-13 17:35:29.364457: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739468129.384918   48223 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739468129.391108   48223 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-13 17:35:29.418049: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 02-13 17:35:32 __init__.py:190] Automatically detected platform cuda.\n",
            "INFO 02-13 17:35:33 api_server.py:840] vLLM API server version 0.7.2\n",
            "INFO 02-13 17:35:33 api_server.py:841] args: Namespace(subparser='serve', model_tag='HuggingFaceTB/SmolLM2-1.7B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='HuggingFaceTB/SmolLM2-1.7B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='half', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7fdbece63ba0>)\n",
            "INFO 02-13 17:35:34 api_server.py:206] Started engine process with PID 48288\n",
            "WARNING 02-13 17:35:35 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "2025-02-13 17:35:41.712717: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739468141.745171   48288 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739468141.755121   48288 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "INFO 02-13 17:35:46 __init__.py:190] Automatically detected platform cuda.\n",
            "WARNING 02-13 17:35:50 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 02-13 17:35:52 config.py:542] This model supports multiple tasks: {'embed', 'score', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\n",
            "INFO 02-13 17:36:03 config.py:542] This model supports multiple tasks: {'reward', 'score', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n",
            "INFO 02-13 17:36:03 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='HuggingFaceTB/SmolLM2-1.7B-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-1.7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-1.7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n",
            "INFO 02-13 17:36:04 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 02-13 17:36:04 cuda.py:227] Using XFormers backend.\n",
            "INFO 02-13 17:36:05 model_runner.py:1110] Starting to load model HuggingFaceTB/SmolLM2-1.7B-Instruct...\n",
            "INFO 02-13 17:36:05 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-13 17:36:06 weight_utils.py:297] No model.safetensors.index.json found in remote.\n",
            "Loading safetensors checkpoint shards: 100% 1/1 [00:18<00:00, 18.23s/it]\n",
            "INFO 02-13 17:36:24 model_runner.py:1115] Loading model weights took 3.1887 GB\n",
            "INFO 02-13 17:36:27 worker.py:267] Memory profiling takes 1.98 seconds\n",
            "INFO 02-13 17:36:27 worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
            "INFO 02-13 17:36:27 worker.py:267] model weights take 3.19GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.48GiB; the rest of the memory reserved for KV Cache is 9.57GiB.\n",
            "INFO 02-13 17:36:27 executor_base.py:110] # CUDA blocks: 3265, # CPU blocks: 1365\n",
            "INFO 02-13 17:36:27 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 6.38x\n",
            "INFO 02-13 17:36:31 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "Capturing CUDA graph shapes: 100% 35/35 [00:31<00:00,  1.11it/s]\n",
            "INFO 02-13 17:37:03 model_runner.py:1562] Graph capturing finished in 32 secs, took 0.16 GiB\n",
            "INFO 02-13 17:37:03 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 38.48 seconds\n",
            "INFO 02-13 17:37:03 api_server.py:756] Using supplied chat template:\n",
            "INFO 02-13 17:37:03 api_server.py:756] None\n",
            "INFO 02-13 17:37:03 launcher.py:21] Available routes are:\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /openapi.json, Methods: HEAD, GET\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /docs, Methods: HEAD, GET\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /redoc, Methods: HEAD, GET\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /health, Methods: GET\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /ping, Methods: GET, POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /tokenize, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /detokenize, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /v1/models, Methods: GET\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /version, Methods: GET\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /v1/completions, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /v1/embeddings, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /pooling, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /score, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /v1/score, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /rerank, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /v1/rerank, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /v2/rerank, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /invocations, Methods: POST\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m48223\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "INFO 02-13 17:38:18 launcher.py:59] Shutting down FastAPI HTTP server.\n",
            "Exception ignored in: <function Socket.__del__ at 0x7b9f2a52c9a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/zmq/sugar/socket.py\", line 111, in __del__\n",
            "    def __del__(self):\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 374, in signal_handler\n",
            "    raise KeyboardInterrupt(\"MQLLMEngine terminated\")\n",
            "KeyboardInterrupt: MQLLMEngine terminated\n",
            "[rank0]:[W213 17:38:19.073703183 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
            "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "inference_server_url = \"http://localhost:8000/v1\"\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
        "    openai_api_key=\"EMPTY\", # max context window of 8192\n",
        "    openai_api_base=inference_server_url,\n",
        "    max_tokens=512,\n",
        "    temperature=0.5\n",
        ")\n"
      ],
      "metadata": {
        "id": "lVs_BOIZS2jm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model Offline Locally"
      ],
      "metadata": {
        "id": "aVe1Tbz2pY4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define a Customised Wrapper from Groq Wrapper"
      ],
      "metadata": {
        "id": "aKOLuuFsqwT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from typing import Any, Dict, Iterator, List, Optional\n",
        "\n",
        "import torch  # only if you need torch\n",
        "from pydantic import Field\n",
        "\n",
        "from langchain_community.llms import VLLM\n",
        "\n",
        "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    AIMessageChunk,\n",
        "    BaseMessage,\n",
        "    HumanMessage,\n",
        ")\n",
        "from langchain_core.messages.ai import UsageMetadata\n",
        "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class ChatVLLMWrapper(BaseChatModel):\n",
        "    model_name: str = Field(alias=\"model\")\n",
        "    trust_remote_code: bool = True\n",
        "    max_new_tokens: int = 512\n",
        "    top_k: int = 10\n",
        "    top_p: float = 0.95\n",
        "    temperature: float = 0.8\n",
        "    dtype: str = \"float16\"\n",
        "    max_retries: int = 2\n",
        "    timeout: Optional[int] = None\n",
        "    stop: Optional[List[str]] = None\n",
        "    _vllm: Any = None\n",
        "\n",
        "    def __init__(self, **kwargs: Any):\n",
        "        super().__init__(**kwargs)\n",
        "        logger.debug(f\"Initializing VLLM with model={self.model_name}\")\n",
        "        self._vllm = VLLM(\n",
        "            model=self.model_name,\n",
        "            trust_remote_code=self.trust_remote_code,\n",
        "            max_new_tokens=self.max_new_tokens,\n",
        "            top_k=self.top_k,\n",
        "            top_p=self.top_p,\n",
        "            temperature=self.temperature,\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "\n",
        "        prompt = self._prompt_from_messages(messages)\n",
        "        logger.debug(\"Invoking VLLM with input=%s\", prompt)\n",
        "\n",
        "        # NOTE: VLLM's signature is invoke(input=..., stop=..., etc.)\n",
        "        # so we call it with \"input=\":\n",
        "        output_text = self._vllm.invoke(\n",
        "            input=prompt,\n",
        "            stop=stop,\n",
        "            max_new_tokens=self.max_new_tokens,\n",
        "            top_k=self.top_k,\n",
        "            top_p=self.top_p,\n",
        "            temperature=self.temperature,\n",
        "        )\n",
        "\n",
        "        input_tokens = len(prompt)  # naive measure\n",
        "        output_tokens = len(output_text)\n",
        "        usage_metadata = UsageMetadata(\n",
        "            {\n",
        "                \"input_tokens\": input_tokens,\n",
        "                \"output_tokens\": output_tokens,\n",
        "                \"total_tokens\": input_tokens + output_tokens,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        message = AIMessage(\n",
        "            content=output_text,\n",
        "            additional_kwargs={},\n",
        "            response_metadata={\"time_in_seconds\": 1},\n",
        "            usage_metadata=usage_metadata,\n",
        "        )\n",
        "        return ChatResult(generations=[ChatGeneration(message=message)])\n",
        "\n",
        "    def _stream(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> Iterator[ChatGenerationChunk]:\n",
        "\n",
        "        prompt = self._prompt_from_messages(messages)\n",
        "        logger.debug(\"Streaming from VLLM with input=%s\", prompt)\n",
        "\n",
        "        output_text = self._vllm.invoke(\n",
        "            input=prompt,\n",
        "            stop=stop,\n",
        "            max_new_tokens=self.max_new_tokens,\n",
        "            top_k=self.top_k,\n",
        "            top_p=self.top_p,\n",
        "            temperature=self.temperature,\n",
        "        )\n",
        "\n",
        "        input_tokens = len(prompt)\n",
        "        for char in output_text:\n",
        "            usage_metadata = UsageMetadata(\n",
        "                {\n",
        "                    \"input_tokens\": input_tokens,\n",
        "                    \"output_tokens\": 1,\n",
        "                    \"total_tokens\": input_tokens + 1,\n",
        "                }\n",
        "            )\n",
        "            chunk = ChatGenerationChunk(\n",
        "                message=AIMessageChunk(content=char, usage_metadata=usage_metadata)\n",
        "            )\n",
        "            if run_manager:\n",
        "                run_manager.on_llm_new_token(char, chunk=chunk)\n",
        "            yield chunk\n",
        "            # Avoid double-counting\n",
        "            input_tokens = 0\n",
        "\n",
        "        # final chunk\n",
        "        chunk = ChatGenerationChunk(\n",
        "            message=AIMessageChunk(content=\"\", response_metadata={\"stream_done\": True})\n",
        "        )\n",
        "        if run_manager:\n",
        "            run_manager.on_llm_new_token(\"\", chunk=chunk)\n",
        "        yield chunk\n",
        "\n",
        "    ## Approach 1: Minimal override that just stores `tools` and returns self.\n",
        "    # def bind_tools(self, tools, **kwargs):\n",
        "    #     self._tools = tools\n",
        "    #     return self\n",
        "\n",
        "    ## Approach 2: Return a Runnable\n",
        "    def bind_tools(self, tools, **kwargs):\n",
        "        \"\"\"\n",
        "        Return a Runnable that calls this LLM.\n",
        "        (Adjust as needed if you want a tool-using agent approach.)\n",
        "        \"\"\"\n",
        "        from langchain.schema.runnable import Runnable, RunnableLambda, RunnableConfig\n",
        "\n",
        "        logger.debug(f\"Binding tools in ChatVLLMWrapper. Tools: {tools}\")\n",
        "        self._tools = tools\n",
        "\n",
        "        def _call_llm(input_str: str, config: RunnableConfig = None) -> str:\n",
        "            # If you want real tool usage, you'd parse input_str or augment it with tools.\n",
        "            response = self.invoke([HumanMessage(content=input_str)])\n",
        "            return response.content\n",
        "\n",
        "        return RunnableLambda(func=_call_llm)\n",
        "\n",
        "    def _prompt_from_messages(self, messages: List[BaseMessage]) -> str:\n",
        "        return \"\\n\".join([msg.content for msg in messages])\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"vllm-chat-model\"\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "            \"max_new_tokens\": self.max_new_tokens,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"top_k\": self.top_k,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"dtype\": self.dtype,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "4I3MjgGGcDWp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Executing the Wrapper"
      ],
      "metadata": {
        "id": "FUl4Dbhsq5xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://lunary.ai/blog/vllm-langchain-tutorial#leveraging-quantization-for-improved-efficiency\n",
        "# Original implementation from LangChain for non-Chat vLLM\n",
        "# import torch\n",
        "# from langchain_community.llms import VLLM\n",
        "\n",
        "# # Initializing the vLLM model\n",
        "# model = VLLM(\n",
        "#     model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
        "#     trust_remote_code=True,  # mandatory for Hugging Face models\n",
        "#     max_new_tokens=512, # max context window of 8192\n",
        "#     top_k=10,\n",
        "#     top_p=0.95,\n",
        "#     temperature=0.8,\n",
        "#     dtype=\"float16\",\n",
        "# )\n",
        "\n",
        "# Instantiate your custom chat model\n",
        "model = ChatVLLMWrapper(\n",
        "    model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
        "    trust_remote_code=True,\n",
        "    max_new_tokens=256,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    temperature=0.8,\n",
        "    dtype=\"float16\",\n",
        ")\n",
        "\n",
        "# Invoke a single question\n",
        "#result = llm.invoke([HumanMessage(content=\"What is Deep Learning?\")])\n",
        "#print(\"Response:\", result.content)\n",
        "\n",
        "# Or call batch\n",
        "# batch_inputs = [\n",
        "#     [HumanMessage(content=\"What is Deep Learning?\")],\n",
        "#     [HumanMessage(content=\"Explain the concept of attention in Transformers.\")],\n",
        "# ]\n",
        "# batch_outputs = llm.batch(batch_inputs)\n",
        "# for i, out in enumerate(batch_outputs):\n",
        "#     print(f\"Response #{i+1}:\", out.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392,
          "referenced_widgets": [
            "d2454680f9074afcb7024be0e16814dd",
            "0e65b85b06e540c9aaf04d5201ee14ac",
            "9c2a0f2b635544b7926138ae4639e8b0",
            "339a5d2154bb4ba986531bfbc5b1bb13",
            "25fd971f91704f71be2d4e61e5bd4783",
            "cc03fdb1bf6442febc14312d01401b55",
            "73a8cc1942ba41949f5c6b717abdac33",
            "8993482f1c8a4ffa883e430642677197",
            "f9ade6668cf24546a5a68d6770eb128f",
            "6116d0a018384e979efd4eca21158680",
            "afa2694cb41e4d5d9e63aa95ca4e85e8"
          ]
        },
        "id": "AzGYC78jcI-i",
        "outputId": "7840e111-c3b2-4ea5-9ea8-f17d3f13641b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-14 06:24:23 __init__.py:190] Automatically detected platform cuda.\n",
            "WARNING 02-14 06:24:25 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 02-14 06:24:38 config.py:542] This model supports multiple tasks: {'classify', 'score', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n",
            "INFO 02-14 06:24:38 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='HuggingFaceTB/SmolLM2-1.7B-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-1.7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-1.7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 02-14 06:24:39 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 02-14 06:24:39 cuda.py:227] Using XFormers backend.\n",
            "INFO 02-14 06:24:40 model_runner.py:1110] Starting to load model HuggingFaceTB/SmolLM2-1.7B-Instruct...\n",
            "INFO 02-14 06:24:40 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-14 06:24:40 weight_utils.py:297] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2454680f9074afcb7024be0e16814dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-14 06:24:56 model_runner.py:1115] Loading model weights took 3.1887 GB\n",
            "INFO 02-14 06:24:58 worker.py:267] Memory profiling takes 2.04 seconds\n",
            "INFO 02-14 06:24:58 worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
            "INFO 02-14 06:24:58 worker.py:267] model weights take 3.19GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.48GiB; the rest of the memory reserved for KV Cache is 9.57GiB.\n",
            "INFO 02-14 06:24:59 executor_base.py:110] # CUDA blocks: 3265, # CPU blocks: 1365\n",
            "INFO 02-14 06:24:59 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 6.38x\n",
            "INFO 02-14 06:25:05 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:39<00:00,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-14 06:25:44 model_runner.py:1562] Graph capturing finished in 39 secs, took 0.16 GiB\n",
            "INFO 02-14 06:25:44 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 48.41 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running a simple query\n",
        "model.invoke(\"What is Deep Learning?\").pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9mSwucUd1yn",
        "outputId": "b1720252-52b2-464e-a2a0-781a6e004e55"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.54s/it, est. speed input: 1.10 toks/s, output: 56.41 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "\n",
            "Deep learning is a subset of machine learning that uses artificial neural networks to model complex patterns in data. These neural networks are designed to mimic the structure and function of the human brain, with multiple layers of interconnected nodes or \"neurons\" that process and transform the input data.\n",
            "\n",
            "The key characteristics of deep learning include:\n",
            "\n",
            "1. **Large amounts of data**: Deep learning requires massive amounts of data to train the neural network, which can be difficult to obtain.\n",
            "2. **Complex data structures**: Deep learning is particularly well-suited for dealing with complex data structures, such as images, audio, and text.\n",
            "3. **Large computational resources**: Deep learning requires significant computational resources to train and deploy the models.\n",
            "\n",
            "Some of the most common deep learning models include:\n",
            "\n",
            "1. **Convolutional Neural Networks (CNNs)**: Used for image classification, object detection, and segmentation.\n",
            "2. **Recurrent Neural Networks (RNNs)**: Used for natural language processing, speech recognition, and time-series forecasting.\n",
            "3. **Long Short-Term Memory (LSTM) Networks**: A type of RNN that is well-suited for long-term dependencies in sequential data.\n",
            "4. **Graph Neural Networks**: Used for graph\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangGraph"
      ],
      "metadata": {
        "id": "k7soxxBDs_qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain-groq"
      ],
      "metadata": {
        "id": "zfpYXOlODQ-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getenv(\"GROQ_API_KEY\"))"
      ],
      "metadata": {
        "id": "ojRFb-4nDuYa",
        "outputId": "72ba8d9b-255c-470e-9884-409cb39c98df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Initialize Groq LLM\n",
        "model = ChatGroq(\n",
        "    model_name=\"llama-3.2-3b-preview\",\n",
        "    temperature=0.6,\n",
        "    api_key=\"key\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(model.invoke(\"Who are you?\"))"
      ],
      "metadata": {
        "id": "oyup3FLGDE34",
        "outputId": "0373fefb-8b0b-4980-8e87-4eab1a819b38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 39, 'total_tokens': 62, 'completion_time': 0.014289574, 'prompt_time': 0.006796052, 'queue_time': 0.11902621800000002, 'total_time': 0.021085626}, 'model_name': 'llama-3.2-3b-preview', 'system_fingerprint': 'fp_a926bfdce1', 'finish_reason': 'stop', 'logprobs': None} id='run-e254438e-0669-46d5-b0bc-5a0d1d4aa552-0' usage_metadata={'input_tokens': 39, 'output_tokens': 23, 'total_tokens': 62}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "SXMyAFM6bSiz",
        "outputId": "440ac4ec-3a56-4354-c41a-05822dec75cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-a49398107efb>:36: LangChainBetaWarning: The function `init_embeddings` is in beta. It is actively being worked on, so the API may change.\n",
            "  \"embed\": init_embeddings(\"huggingface:sentence-transformers/all-MiniLM-L6-v2\"),\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import json\n",
        "from typing import (\n",
        "    Annotated,\n",
        "    Sequence,\n",
        "    TypedDict,\n",
        "    List,\n",
        ")\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# LangChain / LangGraph imports\n",
        "from langchain_core.messages import (\n",
        "    SystemMessage,\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    BaseMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "from langchain_core.tools import StructuredTool\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "from langgraph.prebuilt import InjectedStore\n",
        "from langgraph.store.base import BaseStore\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain.embeddings import init_embeddings\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "###############################################################################\n",
        "# 1. Initialize memory + config\n",
        "###############################################################################\n",
        "in_memory_store = InMemoryStore(\n",
        "    index={\n",
        "        \"embed\": init_embeddings(\"huggingface:sentence-transformers/all-MiniLM-L6-v2\"),\n",
        "        \"dims\": 384,  # Embedding dimensions\n",
        "    }\n",
        ")\n",
        "\n",
        "# A memory saver to checkpoint conversation states\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "# Initialize config with user & thread info\n",
        "config = {}\n",
        "config[\"configurable\"] = {\n",
        "    \"user_id\": \"user_1\",\n",
        "    \"thread_id\": 0,\n",
        "}\n",
        "\n",
        "###############################################################################\n",
        "# 2. Define MessagesState\n",
        "###############################################################################\n",
        "class MessagesState(TypedDict):\n",
        "    \"\"\"The state of the agent.\n",
        "\n",
        "    The key 'messages' uses add_messages as a reducer,\n",
        "    so each time this state is updated, new messages are appended.\n",
        "    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "###############################################################################\n",
        "# 3. Memory Tools\n",
        "###############################################################################\n",
        "def save_memory(memory: str, *, config: RunnableConfig, store: Annotated[BaseStore, InjectedStore()]) -> str:\n",
        "    \"\"\"Save the given memory for the current user.\"\"\"\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\", \"unknown_user\")\n",
        "    namespace = (\"memories\", user_id)\n",
        "    store.put(namespace, f\"memory_{len(store.search(namespace))}\", {\"data\": memory})\n",
        "    return f\"Saved memory: {memory}\"\n",
        "\n",
        "# Define a Pydantic schema for the save_memory tool\n",
        "class RecallMemory(BaseModel):\n",
        "    query: str = Field(..., title=\"Search Text\", description=\"The text to search from memories for similiar records.\")\n",
        "    top_k: int = Field(10, title=\"Number of Results\", description=\"Number of results to retrieve.\")\n",
        "\n",
        "def recall_memory(query: str, top_k: int = 10) -> str:\n",
        "    \"\"\"Retrieve user memories from in_memory_store.\"\"\"\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\", \"unknown_user\")\n",
        "    namespace = (\"memories\", user_id)\n",
        "\n",
        "    memories = [m.value[\"data\"] for m in in_memory_store.search(namespace, query, limit=top_k)]\n",
        "    joined = f\"User memories: {', '.join(memories)}\"\n",
        "    return joined\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "recall_memory_tool = StructuredTool.from_function(\n",
        "    func=recall_memory,\n",
        "    name=\"Recall Memory Tool\",\n",
        "    description=\"\"\"\n",
        "Retrieve memories relevant to the user's query from the vector store of saved data.\n",
        "Input must be a JSON string with the schema:\n",
        "{\n",
        "  \"query\": \"<text to search>\",\n",
        "  \"top_k\": 10\n",
        "}\n",
        "\"\"\",\n",
        "    args_schema=RecallMemory,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "###############################################################################\n",
        "# 4. Additional Tools (If you have them)\n",
        "###############################################################################\n",
        "\n",
        "tools = [\n",
        "    mcq_retriever_tool,\n",
        "    web_extraction_tool,\n",
        "    ensemble_retriever_tool,\n",
        "    general_retriever_tool,\n",
        "    in_memory_retriever_tool,\n",
        "    recall_memory_tool,\n",
        "]\n",
        "\n",
        "###############################################################################\n",
        "# 5. Build the Graph\n",
        "###############################################################################\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "\n",
        "# The built-in ToolNode from langgraph that calls any declared tools\n",
        "tool_node = ToolNode(tools=tools)\n",
        "\n",
        "# Wrap your model with tools\n",
        "llm_with_tools = model.bind_tools(tools)\n",
        "\n",
        "###############################################################################\n",
        "# 6. The agent's main node: call_model\n",
        "###############################################################################\n",
        "\n",
        "def call_model(state: MessagesState, config: RunnableConfig):\n",
        "    \"\"\"\n",
        "    The main agent node that calls the LLM with the user + system messages.\n",
        "    If the LLM requests a tool call, we'll route to the 'tools' node next\n",
        "    (depending on the condition).\n",
        "    \"\"\"\n",
        "    response = llm_with_tools.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "###############################################################################\n",
        "# 7. Summarize Node -> updates thread_id and ends\n",
        "###############################################################################\n",
        "def summarise_node(state: MessagesState, config: RunnableConfig):\n",
        "    \"\"\"\n",
        "    Final node that summarizes the entire conversation for the current thread,\n",
        "    saves it in memory, increments the thread_id, and ends the conversation.\n",
        "    \"\"\"\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "    current_thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "    new_thread_id = current_thread_id + 1\n",
        "\n",
        "    prompt = \"\\n\".join([msg.content for msg in [\n",
        "        SystemMessage(\n",
        "            content=\"\"\"Please summarize the current conversation in no more than\n",
        "            1000 tokens. Also retain any unanswered quiz questions along with\n",
        "            your internal answers so the next conversation thread can continue.\n",
        "            Do not reveal solutions to the user yet.\"\"\"), *state[\"messages\"]]])\n",
        "\n",
        "    summarise_response = model.invoke(prompt)  # Pass the prompt string\n",
        "    summary_text = summarise_response.content\n",
        "\n",
        "    # Save summary to the user's memory under the new thread ID\n",
        "    config_for_saving = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": user_id,\n",
        "            \"thread_id\": new_thread_id\n",
        "        }\n",
        "    }\n",
        "    save_memory(summary_text, config=config_for_saving, store=in_memory_store)\n",
        "\n",
        "    # Update the original config\n",
        "    #config[\"configurable\"][\"thread_id\"] = new_thread_id\n",
        "\n",
        "    # Return a final message\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            AIMessage(\n",
        "                content=(\n",
        "                    f\"Conversation ended. Summary saved to thread_id {new_thread_id}. \"\n",
        "                    \"Please start a new conversation if you'd like to continue.\"\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    }\n",
        "\n",
        "###############################################################################\n",
        "# 8. Add Nodes & Edges, Then Compile\n",
        "###############################################################################\n",
        "graph_builder.add_node(\"agent\", call_model)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "graph_builder.add_node(\"summary\", summarise_node)\n",
        "\n",
        "# Entry point\n",
        "graph_builder.set_entry_point(\"agent\")\n",
        "\n",
        "# If LLM requests a tool, go to \"tools\", otherwise go to \"summary\"\n",
        "graph_builder.add_conditional_edges(\"agent\", tools_condition, \"summary\")\n",
        "\n",
        "# If we used a tool, return to the agent for final answer or more tools\n",
        "graph_builder.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "# Mark \"summary\" as the conversation finish point\n",
        "graph_builder.set_finish_point(\"summary\")\n",
        "\n",
        "# Compile\n",
        "graph = graph_builder.compile(checkpointer=checkpointer, store=in_memory_store)\n",
        "\n",
        "###############################################################################\n",
        "# 9. Visualize the Graph\n",
        "###############################################################################\n",
        "from IPython.display import Image, display\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "\n",
        "###############################################################################\n",
        "# 10. Helper Function: chat_query\n",
        "###############################################################################\n",
        "def chat_query(question: str, config: RunnableConfig):\n",
        "    \"\"\"Send a user question into the graph, return final response.\"\"\"\n",
        "    # Build the input state with a single user message\n",
        "    input_state = {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=question)\n",
        "        ]\n",
        "    }\n",
        "    # The config is the **second positional argument** to stream() or invoke()!\n",
        "    events = graph.stream(\n",
        "        input_state,\n",
        "        config, # Pass the thread-level persistence to the graph\n",
        "        stream_mode=\"values\",\n",
        "    )\n",
        "    for event in events:\n",
        "        return event[\"messages\"][-1].pretty_print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hbnTzV8d8po",
        "outputId": "2da5d895-b52d-42db-8a69-a5b6277df4e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Find out about Deep Learning from databases. Then search from website: https://www.ibm.com/think/topics/artificial-intelligence to answer difference between machine learning vs artificial intelligence? https://www.ibm.com/think/topics/machine-learning\n"
          ]
        }
      ],
      "source": [
        "# URL\n",
        "url1 = \"https://www.ibm.com/think/topics/artificial-intelligence\"\n",
        "url2 = \"https://www.ibm.com/think/topics/machine-learning\"\n",
        "\n",
        "question_1 = (\n",
        "    f\"Find out about Deep Learning from databases. Then search from website: {url1} \"\n",
        "    f\"to answer difference between machine learning vs artificial intelligence? {url2}\"\n",
        ")\n",
        "question_2 = \"Provide 5 MCQ questions on Artificial Intelligence to help me with practice. Do not give the answer.\"\n",
        "question_3 = \"Here are my answers: 1. A, 2. B, 3. C, 4. D, 5. E. Please check mu answer, provide reasons for my wrong answers and provide the correct answers with explanations.\"\n",
        "question_4 = \"Provide another 5 MCQ questions on Artificial Intelligence to help me with practice. Do not give the answer.\"\n",
        "question_5 = \"Here are my answers: 1. A, 2. B, 3. C, 4. D, 5. E. Please check mu answer, provide reasons for my wrong answers and provide the correct answers with explanations.\"\n",
        "question_6 = \"Provide a study quide to help me learn for my wrong answers for the MCQ questions.\"\n",
        "question_7 = \"Based on your reference databases only, provide a study quide on Deep Learning.\"\n",
        "question_8 = \"Based on your memory, provide a summary of our conversation.\"\n",
        "\n",
        "# Grab the current user_id and thread_id from config\n",
        "user_id = \"user_1\"\n",
        "thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "\n",
        "# Update the config with the new thread_id\n",
        "runtime_config = {\n",
        "    \"configurable\": {\n",
        "        \"user_id\": user_id,\n",
        "        \"thread_id\": thread_id,\n",
        "    }\n",
        "}\n",
        "\n",
        "answer = chat_query(question_1, runtime_config)\n",
        "\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "# events = graph.stream(\n",
        "#     {\"messages\": [{\"role\": \"user\", \"content\": question_1}]},\n",
        "#     config, # Pass the thread-level persistence to the graph\n",
        "#     stream_mode=\"values\",\n",
        "# )\n",
        "# for event in events:\n",
        "#     event[\"messages\"][-1].pretty_print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# URL\n",
        "url1 = \"https://www.ibm.com/think/topics/artificial-intelligence\"\n",
        "url2 = \"https://www.ibm.com/think/topics/machine-learning\"\n",
        "\n",
        "question_1 = f\"Find out about Deep Learning from databases. Then search from website to answer different between machine learning vs artificial intelligence? {url1} and {url2}\"\n",
        "question_2 = \"Provide 5 MCQ questions on Artificial Intelligence to help me with practice. Do not give the answer.\"\n",
        "\n",
        "# Invoke the graph with configuration\n",
        "user_id = \"1\"\n",
        "\n",
        "# Get last thread_id\n",
        "last_thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "thread_id = str(int(last_thread_id) + 1)\n",
        "\n",
        "# Update the config with the new thread_id\n",
        "config = {\"configurable\": {\"thread_id\": thread_id, \"user_id\": user_id}}\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": question_2}]},\n",
        "    config, # Pass the thread-level persistence to the graph\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "# Save the AI last messages\n",
        "last_AI_message = f\"Last Assistant Response: \"+ event[\"messages\"][-1].content"
      ],
      "metadata": {
        "id": "HRgfSPdrBwn_",
        "outputId": "34946800-0114-458f-c6b0-8c2ad4146895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Provide 5 MCQ questions on Artificial Intelligence to help me with practice. Do not give the answer.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "3 validation errors for HumanMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value=[HumanMessage(content='Pr...b1e-a5e8-9011f4527881')], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].0.str\n  Input should be a valid string [type=string_type, input_value=HumanMessage(content='Pro...4b1e-a5e8-9011f4527881'), input_type=HumanMessage]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].0.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=HumanMessage(content='Pro...4b1e-a5e8-9011f4527881'), input_type=HumanMessage]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-02a0f289c2cc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mstream_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"values\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   1725\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    231\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m                 )\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-a49398107efb>\u001b[0m in \u001b[0;36mcall_model\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mdepending\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \"\"\"\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_with_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4719\u001b[0m         \"\"\"\n\u001b[1;32m   4720\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"func\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4721\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m   4722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4723\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1920\u001b[0m             output = cast(\n\u001b[1;32m   1921\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1922\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1923\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1924\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   4573\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4575\u001b[0;31m             output = call_func_with_variable_args(\n\u001b[0m\u001b[1;32m   4576\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4577\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-f23e936b602e>\u001b[0m in \u001b[0;36m_call_llm\u001b[0;34m(input_str, config)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_call_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_str\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunnableConfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# If you want real tool usage, you'd parse input_str or augment it with tools.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/messages/human.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, content, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAdditional\u001b[0m \u001b[0mfields\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mpass\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/messages/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, content, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAdditional\u001b[0m \u001b[0mfields\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mpass\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 3 validation errors for HumanMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value=[HumanMessage(content='Pr...b1e-a5e8-9011f4527881')], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].0.str\n  Input should be a valid string [type=string_type, input_value=HumanMessage(content='Pro...4b1e-a5e8-9011f4527881'), input_type=HumanMessage]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].0.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=HumanMessage(content='Pro...4b1e-a5e8-9011f4527881'), input_type=HumanMessage]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KmeoL-_lcs8Y",
        "s_2h8tWUc9yA",
        "T_ki8T8DdE8C"
      ],
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMcY7cZryZDRxpa8j/FHZ4e",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d2454680f9074afcb7024be0e16814dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e65b85b06e540c9aaf04d5201ee14ac",
              "IPY_MODEL_9c2a0f2b635544b7926138ae4639e8b0",
              "IPY_MODEL_339a5d2154bb4ba986531bfbc5b1bb13"
            ],
            "layout": "IPY_MODEL_25fd971f91704f71be2d4e61e5bd4783"
          }
        },
        "0e65b85b06e540c9aaf04d5201ee14ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc03fdb1bf6442febc14312d01401b55",
            "placeholder": "​",
            "style": "IPY_MODEL_73a8cc1942ba41949f5c6b717abdac33",
            "value": ""
          }
        },
        "9c2a0f2b635544b7926138ae4639e8b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8993482f1c8a4ffa883e430642677197",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9ade6668cf24546a5a68d6770eb128f",
            "value": 1
          }
        },
        "339a5d2154bb4ba986531bfbc5b1bb13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6116d0a018384e979efd4eca21158680",
            "placeholder": "​",
            "style": "IPY_MODEL_afa2694cb41e4d5d9e63aa95ca4e85e8",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14&lt;00:00, 14.98s/it]\n"
          }
        },
        "25fd971f91704f71be2d4e61e5bd4783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc03fdb1bf6442febc14312d01401b55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73a8cc1942ba41949f5c6b717abdac33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8993482f1c8a4ffa883e430642677197": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9ade6668cf24546a5a68d6770eb128f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6116d0a018384e979efd4eca21158680": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa2694cb41e4d5d9e63aa95ca4e85e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumkh/ITI110_AgenticRAG/blob/main/LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVfMT6l1cobl"
      },
      "source": [
        "## PRE-PRODUCTION - AI Tutor Chatbot (Version 2.19)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmeoL-_lcs8Y"
      },
      "source": [
        "### Setting Up - Install Requirements (Restart Session after installation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkI4XOWxSXvl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -qU vllm accelerate bitsandbytes langchain-openai langchain-groq huggingface_hub transformers langchain langchain_huggingface langgraph langchain-core langchain-text-splitters langchain-community chromadb langchain-chroma langsmith docling langchain-docling sentence_transformers gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_2h8tWUc9yA"
      },
      "source": [
        "### Load Packages and Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lzrhAq9cbbU",
        "outputId": "559d4a64-c015-46e1-cfdc-6f657aae3e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-23 17:22:53--  https://github.com/sumkh/NYP_Dataset/raw/refs/heads/main/Documents.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sumkh/NYP_Dataset/refs/heads/main/Documents.zip [following]\n",
            "--2025-02-23 17:22:53--  https://raw.githubusercontent.com/sumkh/NYP_Dataset/refs/heads/main/Documents.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18511560 (18M) [application/zip]\n",
            "Saving to: ‘Documents.zip.1’\n",
            "\n",
            "Documents.zip.1     100%[===================>]  17.65M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-02-23 17:22:54 (371 MB/s) - ‘Documents.zip.1’ saved [18511560/18511560]\n",
            "\n",
            "Archive:  /content/Documents.zip\n",
            "  inflating: Documents/general/Topic 1 Introduction to AI and AI on Azure.pdf  \n",
            "  inflating: Documents/general/deeplearningreview.docx  \n",
            "  inflating: Documents/general/slide_1.pptx  \n",
            "  inflating: Documents/mcq/mcq.csv   \n",
            "  inflating: Documents/mcq/mcq2.csv  \n",
            "  inflating: app/Dockerfile          \n",
            "  inflating: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/data_level0.bin  \n",
            "  inflating: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/header.bin  \n",
            "  inflating: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/length.bin  \n",
            " extracting: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/link_lists.bin  \n",
            "  inflating: general_db/chroma.sqlite3  \n",
            "  inflating: mcq_db/chroma.sqlite3   \n",
            "  inflating: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/data_level0.bin  \n",
            "  inflating: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/header.bin  \n",
            "  inflating: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/length.bin  \n",
            " extracting: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/link_lists.bin  \n",
            "  inflating: requirements.txt        \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "LANGSMITH_API_KEY=userdata.get(\"LANGSMITH_API_KEY\")\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Disable tokenizers parallelism, as it causes issues with multiprocessing\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # LangSmith for Observability\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"AgenticRAG\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY # Optional\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=HF_TOKEN) # May be optional for getting model download from Huggingface\n",
        "\n",
        "# Download required files from Github repo\n",
        "!wget https://github.com/sumkh/NYP_Dataset/raw/refs/heads/main/Documents.zip\n",
        "!unzip -o /content/Documents.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Znk0gm1ce67"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import json\n",
        "import hashlib\n",
        "import uuid\n",
        "import logging\n",
        "from typing import List, Optional, Union, Literal, Dict\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "# LangChain & related imports\n",
        "from langchain_core.tools import tool, StructuredTool\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever\n",
        "\n",
        "# Extraction for Documents\n",
        "from langchain_docling.loader import ExportType\n",
        "from langchain_docling import DoclingLoader\n",
        "from docling.chunking import HybridChunker\n",
        "\n",
        "# Extraction for HTML\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configurations and Get the API key from the environment variable\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SDpdN9Yck4w"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "#                         Document Extraction Functions\n",
        "# =============================================================================\n",
        "\n",
        "def extract_documents(doc_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Recursively collects all file paths from folder 'doc_path'.\n",
        "    Used by ExtractDocument.load_files() to find documents to parse.\n",
        "    \"\"\"\n",
        "    extracted_docs = []\n",
        "\n",
        "    for root, _, files in os.walk(doc_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            extracted_docs.append(file_path)\n",
        "    return extracted_docs\n",
        "\n",
        "\n",
        "def _generate_uuid(page_content: str) -> str:\n",
        "    \"\"\"Generate a UUID for a chunk of text using MD5 hashing.\"\"\"\n",
        "    md5_hash = hashlib.md5(page_content.encode()).hexdigest()\n",
        "    return str(uuid.UUID(md5_hash[0:32]))\n",
        "\n",
        "\n",
        "def load_file(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load a file from the given path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "\n",
        "    # Load the file and extract the text chunks\n",
        "    try:\n",
        "        loader = DoclingLoader(\n",
        "            file_path = file_path,\n",
        "            export_type = ExportType.DOC_CHUNKS,\n",
        "            chunker = HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
        "        )\n",
        "        docs = loader.load()\n",
        "        logger.info(f\"Total parsed doc-chunks: {len(docs)} from Source: {file_path}\")\n",
        "\n",
        "        for d in docs:\n",
        "            # Tag each document's chunk with the source file and a unique ID\n",
        "            doc = Document(\n",
        "                page_content=d.page_content,\n",
        "                metadata={\n",
        "                    \"source\": file_path,\n",
        "                    \"doc_id\": _generate_uuid(d.page_content),\n",
        "                    \"source_type\": \"file\",\n",
        "                }\n",
        "            )\n",
        "            _documents.append(doc)\n",
        "        logger.info(f\"Total generated LangChain document chunks: {len(_documents)}\\n.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {file_path}. Exception: {e}\\n.\")\n",
        "\n",
        "    return _documents\n",
        "\n",
        "\n",
        "# Define function to load documents from a folder\n",
        "def load_files_from_folder(doc_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load documents from the given folder path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "    # Extract all files path from the given folder\n",
        "    extracted_docs = extract_documents(doc_path)\n",
        "\n",
        "    # Iterate through each document and extract the text chunks\n",
        "    for file_path in extracted_docs:\n",
        "        _documents.extend(load_file(file_path))\n",
        "\n",
        "    return _documents\n",
        "\n",
        "# =============================================================================\n",
        "# Load structured data in csv file to LangChain Document format\n",
        "def load_mcq_csvfiles(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load structured data in mcq csv file from the given file path and return a list of Document object.\n",
        "    Expected format: each row of csv is comma separated into \"mcq_number\", \"mcq_type\", \"text_content\"\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "\n",
        "    # iterate through each csv file and load each row into _dict_per_question format\n",
        "    # Ensure we process only CSV files\n",
        "    if not file_path.endswith(\".csv\"):\n",
        "        return _documents  # Skip non-CSV files\n",
        "    try:\n",
        "        # Open and read the CSV file\n",
        "        with open(file_path, mode='r', encoding='utf-8') as file:\n",
        "            reader = csv.DictReader(file)\n",
        "            for row in reader:\n",
        "                # Ensure required columns exist in the row\n",
        "                if not all(k in row for k in [\"mcq_number\", \"mcq_type\", \"text_content\"]): # Ensure required columns exist and exclude header\n",
        "                    logger.error(f\"Skipping row due to missing fields: {row}\")\n",
        "                    continue\n",
        "                # Tag each row of csv is comma separated into \"mcq_number\", \"mcq_type\", \"text_content\"\n",
        "                doc = Document(\n",
        "                    page_content = row[\"text_content\"], # text_content segment is separated by \"|\"\n",
        "                    metadata={\n",
        "                        \"source\": f\"{file_path}_{row['mcq_number']}\",  # file_path + mcq_number\n",
        "                        \"doc_id\": _generate_uuid(f\"{file_path}_{row['mcq_number']}\"),  # Unique ID\n",
        "                        \"source_type\": row[\"mcq_type\"],  # MCQ type\n",
        "                    }\n",
        "                )\n",
        "                _documents.append(doc)\n",
        "            logger.info(f\"Successfully loaded {len(_documents)} LangChain document chunks from {file_path}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {file_path}. Exception: {e}\\n.\")\n",
        "\n",
        "    return _documents\n",
        "\n",
        "# Define function to load documents from a folder for structured data in csv file\n",
        "def load_files_from_folder_mcq(doc_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load mcq csv file from the given folder path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "    # Extract all files path from the given folder\n",
        "    extracted_docs = [\n",
        "        os.path.join(doc_path, file) for file in os.listdir(doc_path)\n",
        "        if file.endswith(\".csv\")  # Process only CSV files\n",
        "    ]\n",
        "\n",
        "    # Iterate through each document and extract the text chunks\n",
        "    for file_path in extracted_docs:\n",
        "        _documents.extend(load_mcq_csvfiles(file_path))\n",
        "\n",
        "    return _documents\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                         Website Extraction Functions\n",
        "# =============================================================================\n",
        "def _generate_uuid(page_content: str) -> str:\n",
        "    \"\"\"Generate a UUID for a chunk of text using MD5 hashing.\"\"\"\n",
        "    md5_hash = hashlib.md5(page_content.encode()).hexdigest()\n",
        "    return str(uuid.UUID(md5_hash[0:32]))\n",
        "\n",
        "def ensure_scheme(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    if not parsed_url.scheme:\n",
        "        return 'http://' + url  # Default to http, or use 'https://' if preferred\n",
        "    return url\n",
        "\n",
        "def extract_html(url: List[str]) -> List[Document]:\n",
        "    if isinstance(url, str):\n",
        "        url = [url]\n",
        "    \"\"\"\n",
        "    Extracts text from the HTML content of web pages listed in 'web_path'.\n",
        "    Returns a list of LangChain 'Document' objects.\n",
        "    \"\"\"\n",
        "    # Ensure all URLs have a scheme\n",
        "    web_paths = [ensure_scheme(u) for u in url]\n",
        "\n",
        "    loader = WebBaseLoader(web_paths)\n",
        "    loader.requests_per_second = 1\n",
        "    docs = loader.load()\n",
        "\n",
        "    # Iterate through each document, clean the content, removing excessive line return and store it in a LangChain Document\n",
        "    _documents = []\n",
        "    for doc in docs:\n",
        "        # Clean the concent\n",
        "        doc.page_content = doc.page_content.strip()\n",
        "        doc.page_content = doc.page_content.replace(\"\\n\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"\\r\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"\\t\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"  \", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"   \", \" \")\n",
        "\n",
        "        # Store it in a LangChain Document\n",
        "        web_doc = Document(\n",
        "            page_content=doc.page_content,\n",
        "            metadata={\n",
        "                \"source\": doc.metadata.get(\"source\"),\n",
        "                \"doc_id\": _generate_uuid(doc.page_content),\n",
        "                \"source_type\": \"web\"\n",
        "            }\n",
        "        )\n",
        "        _documents.append(web_doc)\n",
        "    return _documents\n",
        "\n",
        "# =============================================================================\n",
        "#                         Vector Store Initialisation\n",
        "# =============================================================================\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
        "\n",
        "# Initialise vector stores\n",
        "general_vs = Chroma(\n",
        "    collection_name=\"general_vstore\",\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"./general_db\"\n",
        ")\n",
        "\n",
        "mcq_vs = Chroma(\n",
        "    collection_name=\"mcq_vstore\",\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"./mcq_db\"\n",
        ")\n",
        "\n",
        "in_memory_vs = Chroma(\n",
        "    collection_name=\"in_memory_vstore\",\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "# Split the documents into smaller chunks for better embedding coverage\n",
        "def split_text_into_chunks(docs: List[Document]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Splits a list of Documents into smaller text chunks using\n",
        "    RecursiveCharacterTextSplitter while preserving metadata.\n",
        "    Returns a list of Document objects.\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000, # Split into chunks of 1000 characters\n",
        "        chunk_overlap=200, # Overlap by 200 characters\n",
        "        add_start_index=True\n",
        "    )\n",
        "    chunked_docs = splitter.split_documents(docs)\n",
        "    return chunked_docs # List of Document objects\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                         Retrieval Tools\n",
        "# =============================================================================\n",
        "\n",
        "# Define a simple similarity search retrieval tool on msq_vs\n",
        "class MCQRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"Search topic.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def mcq_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Retrieve the top k most similar mcq question documents from the vector store\n",
        "    docs_func = mcq_vs.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\n",
        "        'k': k,\n",
        "        'filter':{\"source_type\": \"mcq_question\"}\n",
        "    },\n",
        "    )\n",
        "    docs_qns = docs_func.invoke(input, k=k)\n",
        "\n",
        "    # Extract the document IDs from the retrieved documents\n",
        "    doc_ids = [d.metadata.get(\"doc_id\") for d in docs_qns if \"doc_id\" in d.metadata]\n",
        "\n",
        "    # Retrieve full documents based on the doc_ids\n",
        "    docs = mcq_vs.get(where = {'doc_id': {\"$in\":doc_ids}})\n",
        "\n",
        "    qns_list = {}\n",
        "    for i, d in enumerate(docs['metadatas']):\n",
        "        qns_list[d['source'] + \" \" + d['source_type']] = docs['documents'][i]\n",
        "\n",
        "    return qns_list\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "mcq_retriever_tool = StructuredTool.from_function(\n",
        "    func = mcq_retriever,\n",
        "    name = \"MCQ Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve MCQ questions set when Human asks to generate a quiz related to a topic.\n",
        "    DO NOT GIVE THE ANSWERS to Human before Human has answered all the questions.\n",
        "\n",
        "    If Human give answers for questions you do not know, SAY you do not have the questions for the answer\n",
        "    and ASK if the Human want you to generate a new quiz and then SAVE THE QUIZ with Summary Tool before ending the conversation.\n",
        "\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The search topic to retrieve MCQ questions set related to the topic.\n",
        "        - k (int): Number of question set to retrieve.\n",
        "        Example usage: input='What is AI?', k=5\n",
        "\n",
        "    Returns:\n",
        "    - A dict of MCQ questions:\n",
        "    Key: 'metadata of question' e.g. './Documents/mcq/mcq.csv_Qn31 mcq_question' with suffix ['question', 'answer', 'answer_reason', 'options', 'wrong_options_reason']\n",
        "    Value: Text Content\n",
        "\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = MCQRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Return the response as a list of strings\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Retrieve more documents with higher diversity using MMR (Maximal Marginal Relevance) from the general vector store\n",
        "# Useful if the dataset has many similar documents\n",
        "class GenRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def gen_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = general_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "general_retriever_tool = StructuredTool.from_function(\n",
        "    func = gen_retriever,\n",
        "    name = \"Assistant References Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve reference information from Assistant reference database for Human queries related to a topic or\n",
        "    and when Human asked to generate guides to learn or study about a topic.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "        Example usage: input='What is AI?', k=5\n",
        "    Returns:\n",
        "    - A list of retrieved document's content string.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = GenRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Return the content of the documents\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Retrieve more documents with higher diversity using MMR (Maximal Marginal Relevance) from the in-memory vector store\n",
        "# Query in-memory vector store only\n",
        "class InMemoryRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def in_memory_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "in_memory_retriever_tool = StructuredTool.from_function(\n",
        "    func = in_memory_retriever,\n",
        "    name = \"In-Memory Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool when Human ask Assistant to retrieve information from documents that Human has uploaded.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = InMemoryRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Whether to return the tool’s output directly\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Web Extraction Tool\n",
        "class WebExtractionRequest(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"Search text.\")\n",
        "    url: str = Field(\n",
        "        ...,\n",
        "        title=\"url\",\n",
        "        description=\"Web URL(s) to extract content from. If multiple URLs, separate them with a comma.\"\n",
        "    )\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "# Extract content from a web URL, load into in_memory_vstore\n",
        "def extract_web_path_tool(input: str, url: str, k: int = 5) -> List[str]:\n",
        "    if isinstance(url, str):\n",
        "        url = [url]\n",
        "    \"\"\"\n",
        "    Extract content from the web URLs based on user's input.\n",
        "    Args:\n",
        "    - input: The input text to search for.\n",
        "    - url: URLs to extract content from.\n",
        "    - k: Number of results to retrieve.\n",
        "    Returns:\n",
        "     - A list of retrieved document's content string.\n",
        "    \"\"\"\n",
        "    # Extract content from the web\n",
        "    html_docs = extract_html(url)\n",
        "    if not html_docs:\n",
        "        return f\"No content extracted from {url}.\"\n",
        "\n",
        "    # Split the documents into smaller chunks for better embedding coverage\n",
        "    chunked_texts = split_text_into_chunks(html_docs)\n",
        "    in_memory_vs.add_documents(chunked_texts) # Add the chunked texts to the in-memory vector store\n",
        "\n",
        "    # Extract content from the in-memory vector store\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\n",
        "        'k': k,\n",
        "        'lambda_mult': 0.25,\n",
        "        'filter':{\"source\": {\"$in\": url}}\n",
        "    },\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "web_extraction_tool = StructuredTool.from_function(\n",
        "    func = extract_web_path_tool,\n",
        "    name = \"Web Extraction Tool\",\n",
        "    description = (\n",
        "        \"Assistant should use this tool to extract content from web URLs based on user's input, \"\n",
        "        \"Web extraction is initially load into database and then return k: Number of results to retrieve\"\n",
        "    ),\n",
        "    args_schema = WebExtractionRequest,\n",
        "    return_direct = False, # Whether to return the tool’s output directly\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Ensemble Retrieval from General and In-Memory Vector Stores\n",
        "class EnsembleRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"Number of results.\")\n",
        "\n",
        "def ensemble_retriever(input: str, k: int = 5) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    general_retrieval = general_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    in_memory_retrieval = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "        retrievers=[general_retrieval, in_memory_retrieval],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "    docs = ensemble_retriever.invoke(input)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "ensemble_retriever_tool = StructuredTool.from_function(\n",
        "    func = ensemble_retriever,\n",
        "    name = \"Ensemble Retriever Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve information from reference database and\n",
        "    extraction of documents that Human has uploaded.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = EnsembleRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuhaRfR6dLno"
      },
      "source": [
        "### Load vLLM Model and Serving Online using OpenAI Wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upf7Jhj5KkIZ"
      },
      "source": [
        "#### Run vLLM SubProcess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "226HJOl4KmWc",
        "outputId": "47db0b19-9d7a-4650-e8b3-b2eb0cb56867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ],
      "source": [
        "# https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/#-tool-calling-(8b/70b/405b)-\n",
        "# https://medium.com/@hakimnaufal/trying-out-vllm-deepseek-r1-in-google-colab-a-quick-guide-a4fe682b8665\n",
        "# https://github.com/naufalhakim23/deepseek-r1-playground/blob/main/deepseek_r1_distill_qwen_fast_api.ipynb\n",
        "# https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/vllm_inference_engine.ipynb\n",
        "# https://docs.vllm.ai/_/downloads/en/v0.4.2/pdf/\n",
        "# https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
        "# https://docs.vllm.ai/en/latest/serving/env_vars.html\n",
        "# https://docs.vllm.ai/en/latest/deployment/docker.html\n",
        "# https://docs.vllm.ai/en/latest/features/quantization/bnb.html\n",
        "\n",
        "# https://huggingface.co/casperhansen/llama-3-8b-instruct-awq/tree/main\n",
        "# https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit\n",
        "\n",
        "!wget -q -P examples/ https://github.com/vllm-project/vllm/raw/refs/heads/main/examples/tool_chat_template_llama3.1_json.jinja\n",
        "\n",
        "# we prepend \"nohup\" and postpend \"&\" to make the Colab cell run in background\n",
        "! nohup python -m vllm.entrypoints.openai.api_server \\\n",
        "                  --model unsloth/llama-3-8b-Instruct-bnb-4bit \\\n",
        "                  --enable-auto-tool-choice \\\n",
        "                  --tool-call-parser llama3_json \\\n",
        "                  --chat-template examples/tool_chat_template_llama3.1_json.jinja \\\n",
        "                  --quantization bitsandbytes \\\n",
        "                  --load-format bitsandbytes \\\n",
        "                  --dtype half \\\n",
        "                  --max-model-len 8192 \\\n",
        "                  --download-dir models/vllm \\\n",
        "                  > vllm.log &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29Eis09vLIyZ",
        "outputId": "49f8f4f2-e935-4db5-eb38-20021a9db5cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING 02-23 19:05:51 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
            "E0000 00:00:1740337555.256425    6863 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "WARNING 02-23 19:06:00 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
            "WARNING 02-23 19:06:05 config.py:628] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
            "WARNING 02-23 19:06:05 config.py:628] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-23 19:06:16 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "INFO 02-23 19:08:41 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 13.35x\n",
            "Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:32<00:04,  1.01s/it]"
          ]
        }
      ],
      "source": [
        "# we check the logs until the server has been started correctly\n",
        "!while ! grep -q \"Application startup complete\" vllm.log; do tail -n 1 vllm.log; sleep 5; done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlJpisBPGUsC"
      },
      "source": [
        "find the process ID (PID) using a command like ps aux | grep vllm and then kill it using kill -9 <PID>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaqw6s9FETtA",
        "outputId": "9af4e5be-de72-48b8-fafc-5cefcac43c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        6741  8.0  2.6 7389368 1447364 ?     Sl   19:05   0:18 python3 -m vllm.entrypoints.opena\n",
            "root        8021  0.0  0.0   7376  3372 ?        S    19:09   0:00 /bin/bash -c ps aux | grep vllm\n",
            "root        8023  0.0  0.0   6484  2424 ?        S    19:09   0:00 grep vllm\n"
          ]
        }
      ],
      "source": [
        "# Find the process ID (PID)\n",
        "!ps aux | grep vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMDeFYy0Gjlt"
      },
      "outputs": [],
      "source": [
        "# To kill the process, look for the first set of digits\n",
        "#!kill -9 2120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k-GdaBnm2W6",
        "outputId": "b41f5fb8-9cce-47d5-97a8-422910355535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vllm server is running\n",
            "The vllm server is ready to serve.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def check_vllm_status():\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:8000/health\")\n",
        "        if response.status_code == 200:\n",
        "            print(\"vllm server is running\")\n",
        "            return True\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"vllm server is not running\")\n",
        "        return False\n",
        "\n",
        "try:\n",
        "    # Monitor the process\n",
        "    while True:\n",
        "        if check_vllm_status() == True:\n",
        "            print(\"The vllm server is ready to serve.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"The vllm server has stopped.\")\n",
        "            stdout, stderr = vllm_process.communicate(timeout=10)\n",
        "            print(f\"STDOUT: {stdout.decode('utf-8')}\")\n",
        "            print(f\"STDERR: {stderr.decode('utf-8')}\")\n",
        "            break\n",
        "        time.sleep(5)  # Check every second\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Stopping the check of vllm...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"HF_TOKEN\"]  = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"GROQ_API_KEY\"]  = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "# Download required files from Github repo\n",
        "!wget -q https://github.com/sumkh/NYP_Dataset/raw/refs/heads/main/Documents.zip\n",
        "!unzip -o -q /content/Documents.zip\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qKUR6G_eT80y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "id": "dBrW25szTcpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "import sys\n",
        "\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "import gradio as gr\n",
        "import json\n",
        "import csv\n",
        "import hashlib\n",
        "import uuid\n",
        "import logging\n",
        "from typing import Annotated, List, Dict, Sequence, TypedDict\n",
        "\n",
        "# LangChain & related imports\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langchain_core.tools import tool, StructuredTool\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "# Extraction for Documents\n",
        "from langchain_docling.loader import ExportType\n",
        "from langchain_docling import DoclingLoader\n",
        "from docling.chunking import HybridChunker\n",
        "\n",
        "# Extraction for HTML\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import InjectedStore\n",
        "from langgraph.store.base import BaseStore\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain.embeddings import init_embeddings\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langchain_core.messages import (\n",
        "    SystemMessage,\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    BaseMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress all library logs at or below WARNING for user experience:\n",
        "logging.disable(logging.WARNING)\n",
        "\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")  # Read from environment variable\n",
        "if HF_TOKEN:\n",
        "    login(token=HF_TOKEN)  # Log in to Hugging Face Hub\n",
        "else:\n",
        "    print(\"Warning: HF_TOKEN not found in environment variables.\")\n",
        "\n",
        "# GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")  # Read from environment variable\n",
        "# if not GROQ_API_KEY:\n",
        "#     print(\"Warning: GROQ_API_KEY not found in environment variables.\")\n",
        "\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# =============================================================================\n",
        "#                         Document Extraction Functions\n",
        "# =============================================================================\n",
        "\n",
        "def extract_documents(doc_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Recursively collects all file paths from folder 'doc_path'.\n",
        "    Used by ExtractDocument.load_files() to find documents to parse.\n",
        "    \"\"\"\n",
        "    extracted_docs = []\n",
        "\n",
        "    for root, _, files in os.walk(doc_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            extracted_docs.append(file_path)\n",
        "    return extracted_docs\n",
        "\n",
        "\n",
        "def _generate_uuid(page_content: str) -> str:\n",
        "    \"\"\"Generate a UUID for a chunk of text using MD5 hashing.\"\"\"\n",
        "    md5_hash = hashlib.md5(page_content.encode()).hexdigest()\n",
        "    return str(uuid.UUID(md5_hash[0:32]))\n",
        "\n",
        "\n",
        "def load_file(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load a file from the given path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "\n",
        "    # Load the file and extract the text chunks\n",
        "    try:\n",
        "        loader = DoclingLoader(\n",
        "            file_path = file_path,\n",
        "            export_type = ExportType.DOC_CHUNKS,\n",
        "            chunker = HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
        "        )\n",
        "        docs = loader.load()\n",
        "        logger.info(f\"Total parsed doc-chunks: {len(docs)} from Source: {file_path}\")\n",
        "\n",
        "        for d in docs:\n",
        "            # Tag each document's chunk with the source file and a unique ID\n",
        "            doc = Document(\n",
        "                page_content=d.page_content,\n",
        "                metadata={\n",
        "                    \"source\": file_path,\n",
        "                    \"doc_id\": _generate_uuid(d.page_content),\n",
        "                    \"source_type\": \"file\",\n",
        "                }\n",
        "            )\n",
        "            _documents.append(doc)\n",
        "        logger.info(f\"Total generated LangChain document chunks: {len(_documents)}\\n.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {file_path}. Exception: {e}\\n.\")\n",
        "\n",
        "    return _documents\n",
        "\n",
        "\n",
        "# Define function to load documents from a folder\n",
        "def load_files_from_folder(doc_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load documents from the given folder path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "    # Extract all files path from the given folder\n",
        "    extracted_docs = extract_documents(doc_path)\n",
        "\n",
        "    # Iterate through each document and extract the text chunks\n",
        "    for file_path in extracted_docs:\n",
        "        _documents.extend(load_file(file_path))\n",
        "\n",
        "    return _documents\n",
        "\n",
        "# =============================================================================\n",
        "# Load structured data in csv file to LangChain Document format\n",
        "def load_mcq_csvfiles(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load structured data in mcq csv file from the given file path and return a list of Document object.\n",
        "    Expected format: each row of csv is comma separated into \"mcq_number\", \"mcq_type\", \"text_content\"\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "\n",
        "    # iterate through each csv file and load each row into _dict_per_question format\n",
        "    # Ensure we process only CSV files\n",
        "    if not file_path.endswith(\".csv\"):\n",
        "        return _documents  # Skip non-CSV files\n",
        "    try:\n",
        "        # Open and read the CSV file\n",
        "        with open(file_path, mode='r', encoding='utf-8') as file:\n",
        "            reader = csv.DictReader(file)\n",
        "            for row in reader:\n",
        "                # Ensure required columns exist in the row\n",
        "                if not all(k in row for k in [\"mcq_number\", \"mcq_type\", \"text_content\"]): # Ensure required columns exist and exclude header\n",
        "                    logger.error(f\"Skipping row due to missing fields: {row}\")\n",
        "                    continue\n",
        "                # Tag each row of csv is comma separated into \"mcq_number\", \"mcq_type\", \"text_content\"\n",
        "                doc = Document(\n",
        "                    page_content = row[\"text_content\"], # text_content segment is separated by \"|\"\n",
        "                    metadata={\n",
        "                        \"source\": f\"{file_path}_{row['mcq_number']}\",  # file_path + mcq_number\n",
        "                        \"doc_id\": _generate_uuid(f\"{file_path}_{row['mcq_number']}\"),  # Unique ID\n",
        "                        \"source_type\": row[\"mcq_type\"],  # MCQ type\n",
        "                    }\n",
        "                )\n",
        "                _documents.append(doc)\n",
        "            logger.info(f\"Successfully loaded {len(_documents)} LangChain document chunks from {file_path}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {file_path}. Exception: {e}\\n.\")\n",
        "\n",
        "    return _documents\n",
        "\n",
        "# Define function to load documents from a folder for structured data in csv file\n",
        "def load_files_from_folder_mcq(doc_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load mcq csv file from the given folder path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "    # Extract all files path from the given folder\n",
        "    extracted_docs = [\n",
        "        os.path.join(doc_path, file) for file in os.listdir(doc_path)\n",
        "        if file.endswith(\".csv\")  # Process only CSV files\n",
        "    ]\n",
        "\n",
        "    # Iterate through each document and extract the text chunks\n",
        "    for file_path in extracted_docs:\n",
        "        _documents.extend(load_mcq_csvfiles(file_path))\n",
        "\n",
        "    return _documents\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                         Website Extraction Functions\n",
        "# =============================================================================\n",
        "def _generate_uuid(page_content: str) -> str:\n",
        "    \"\"\"Generate a UUID for a chunk of text using MD5 hashing.\"\"\"\n",
        "    md5_hash = hashlib.md5(page_content.encode()).hexdigest()\n",
        "    return str(uuid.UUID(md5_hash[0:32]))\n",
        "\n",
        "def ensure_scheme(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    if not parsed_url.scheme:\n",
        "        return 'http://' + url  # Default to http, or use 'https://' if preferred\n",
        "    return url\n",
        "\n",
        "def extract_html(url: List[str]) -> List[Document]:\n",
        "    if isinstance(url, str):\n",
        "        url = [url]\n",
        "    \"\"\"\n",
        "    Extracts text from the HTML content of web pages listed in 'web_path'.\n",
        "    Returns a list of LangChain 'Document' objects.\n",
        "    \"\"\"\n",
        "    # Ensure all URLs have a scheme\n",
        "    web_paths = [ensure_scheme(u) for u in url]\n",
        "\n",
        "    loader = WebBaseLoader(web_paths)\n",
        "    loader.requests_per_second = 1\n",
        "    docs = loader.load()\n",
        "\n",
        "    # Iterate through each document, clean the content, removing excessive line return and store it in a LangChain Document\n",
        "    _documents = []\n",
        "    for doc in docs:\n",
        "        # Clean the concent\n",
        "        doc.page_content = doc.page_content.strip()\n",
        "        doc.page_content = doc.page_content.replace(\"\\n\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"\\r\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"\\t\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"  \", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"   \", \" \")\n",
        "\n",
        "        # Store it in a LangChain Document\n",
        "        web_doc = Document(\n",
        "            page_content=doc.page_content,\n",
        "            metadata={\n",
        "                \"source\": doc.metadata.get(\"source\"),\n",
        "                \"doc_id\": _generate_uuid(doc.page_content),\n",
        "                \"source_type\": \"web\"\n",
        "            }\n",
        "        )\n",
        "        _documents.append(web_doc)\n",
        "    return _documents\n",
        "\n",
        "# =============================================================================\n",
        "#                         Vector Store Initialisation\n",
        "# =============================================================================\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
        "\n",
        "# Initialise vector stores\n",
        "general_vs = Chroma(\n",
        "    collection_name=\"general_vstore\",\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"./general_db\"\n",
        ")\n",
        "\n",
        "mcq_vs = Chroma(\n",
        "    collection_name=\"mcq_vstore\",\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"./mcq_db\"\n",
        ")\n",
        "\n",
        "in_memory_vs = Chroma(\n",
        "    collection_name=\"in_memory_vstore\",\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "# Split the documents into smaller chunks for better embedding coverage\n",
        "def split_text_into_chunks(docs: List[Document]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Splits a list of Documents into smaller text chunks using\n",
        "    RecursiveCharacterTextSplitter while preserving metadata.\n",
        "    Returns a list of Document objects.\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000, # Split into chunks of 1000 characters\n",
        "        chunk_overlap=200, # Overlap by 200 characters\n",
        "        add_start_index=True\n",
        "    )\n",
        "    chunked_docs = splitter.split_documents(docs)\n",
        "    return chunked_docs # List of Document objects\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                         Retrieval Tools\n",
        "# =============================================================================\n",
        "\n",
        "# Define a simple similarity search retrieval tool on msq_vs\n",
        "class MCQRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"Search topic.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def mcq_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Retrieve the top k most similar mcq question documents from the vector store\n",
        "    docs_func = mcq_vs.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\n",
        "        'k': k,\n",
        "        'filter':{\"source_type\": \"mcq_question\"}\n",
        "    },\n",
        "    )\n",
        "    docs_qns = docs_func.invoke(input, k=k)\n",
        "\n",
        "    # Extract the document IDs from the retrieved documents\n",
        "    doc_ids = [d.metadata.get(\"doc_id\") for d in docs_qns if \"doc_id\" in d.metadata]\n",
        "\n",
        "    # Retrieve full documents based on the doc_ids\n",
        "    docs = mcq_vs.get(where = {'doc_id': {\"$in\":doc_ids}})\n",
        "\n",
        "    qns_list = {}\n",
        "    for i, d in enumerate(docs['metadatas']):\n",
        "        qns_list[d['source'] + \" \" + d['source_type']] = docs['documents'][i]\n",
        "\n",
        "    return qns_list\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "mcq_retriever_tool = StructuredTool.from_function(\n",
        "    func = mcq_retriever,\n",
        "    name = \"MCQ Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve MCQ questions set when Human asks to generate a quiz related to a topic.\n",
        "    DO NOT GIVE THE ANSWERS to Human before Human has answered all the questions.\n",
        "\n",
        "    If Human give answers for questions you do not know, SAY you do not have the questions for the answer\n",
        "    and ASK if the Human want you to generate a new quiz and then SAVE THE QUIZ with Summary Tool before ending the conversation.\n",
        "\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The search topic to retrieve MCQ questions set related to the topic.\n",
        "        - k (int): Number of question set to retrieve.\n",
        "        Example usage: input='What is AI?', k=5\n",
        "\n",
        "    Returns:\n",
        "    - A dict of MCQ questions:\n",
        "    Key: 'metadata of question' e.g. './Documents/mcq/mcq.csv_Qn31 mcq_question' with suffix ['question', 'answer', 'answer_reason', 'options', 'wrong_options_reason']\n",
        "    Value: Text Content\n",
        "\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = MCQRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Return the response as a list of strings\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Retrieve more documents with higher diversity using MMR (Maximal Marginal Relevance) from the general vector store\n",
        "# Useful if the dataset has many similar documents\n",
        "class GenRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def gen_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = general_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "general_retriever_tool = StructuredTool.from_function(\n",
        "    func = gen_retriever,\n",
        "    name = \"Assistant References Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve reference information from Assistant reference database for Human queries related to a topic or\n",
        "    and when Human asked to generate guides to learn or study about a topic.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "        Example usage: input='What is AI?', k=5\n",
        "    Returns:\n",
        "    - A list of retrieved document's content string.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = GenRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Return the content of the documents\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Retrieve more documents with higher diversity using MMR (Maximal Marginal Relevance) from the in-memory vector store\n",
        "# Query in-memory vector store only\n",
        "class InMemoryRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def in_memory_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "in_memory_retriever_tool = StructuredTool.from_function(\n",
        "    func = in_memory_retriever,\n",
        "    name = \"In-Memory Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool when Human ask Assistant to retrieve information from documents that Human has uploaded.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = InMemoryRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Whether to return the tool’s output directly\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Web Extraction Tool\n",
        "class WebExtractionRequest(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"Search text.\")\n",
        "    url: str = Field(\n",
        "        ...,\n",
        "        title=\"url\",\n",
        "        description=\"Web URL(s) to extract content from. If multiple URLs, separate them with a comma.\"\n",
        "    )\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "# Extract content from a web URL, load into in_memory_vstore\n",
        "def extract_web_path_tool(input: str, url: str, k: int = 5) -> List[str]:\n",
        "    if isinstance(url, str):\n",
        "        url = [url]\n",
        "    \"\"\"\n",
        "    Extract content from the web URLs based on user's input.\n",
        "    Args:\n",
        "    - input: The input text to search for.\n",
        "    - url: URLs to extract content from.\n",
        "    - k: Number of results to retrieve.\n",
        "    Returns:\n",
        "     - A list of retrieved document's content string.\n",
        "    \"\"\"\n",
        "    # Extract content from the web\n",
        "    html_docs = extract_html(url)\n",
        "    if not html_docs:\n",
        "        return f\"No content extracted from {url}.\"\n",
        "\n",
        "    # Split the documents into smaller chunks for better embedding coverage\n",
        "    chunked_texts = split_text_into_chunks(html_docs)\n",
        "    in_memory_vs.add_documents(chunked_texts) # Add the chunked texts to the in-memory vector store\n",
        "\n",
        "    # Extract content from the in-memory vector store\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\n",
        "        'k': k,\n",
        "        'lambda_mult': 0.25,\n",
        "        'filter':{\"source\": {\"$in\": url}}\n",
        "    },\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "web_extraction_tool = StructuredTool.from_function(\n",
        "    func = extract_web_path_tool,\n",
        "    name = \"Web Extraction Tool\",\n",
        "    description = (\n",
        "        \"Assistant should use this tool to extract content from web URLs based on user's input, \"\n",
        "        \"Web extraction is initially load into database and then return k: Number of results to retrieve\"\n",
        "    ),\n",
        "    args_schema = WebExtractionRequest,\n",
        "    return_direct = False, # Whether to return the tool’s output directly\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Ensemble Retrieval from General and In-Memory Vector Stores\n",
        "class EnsembleRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"Number of results.\")\n",
        "\n",
        "def ensemble_retriever(input: str, k: int = 5) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    general_retrieval = general_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    in_memory_retrieval = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "        retrievers=[general_retrieval, in_memory_retrieval],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "    docs = ensemble_retriever.invoke(input)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "ensemble_retriever_tool = StructuredTool.from_function(\n",
        "    func = ensemble_retriever,\n",
        "    name = \"Ensemble Retriever Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve information from reference database and\n",
        "    extraction of documents that Human has uploaded.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = EnsembleRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False\n",
        "    )\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# LLM Model Setup\n",
        "###############################################################################\n",
        "\n",
        "TEMPERATURE = 0.5\n",
        "model = ChatOpenAI(\n",
        "    model=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    temperature=TEMPERATURE,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    api_key=\"not_required\",\n",
        "    base_url=\"http://localhost:8000/v1\", # Use the VLLM instance URL\n",
        ")\n",
        "\n",
        "# model = ChatGroq(\n",
        "#     model_name=\"deepseek-r1-distill-llama-70b\",\n",
        "#     temperature=TEMPERATURE,\n",
        "#     api_key=GROQ_API_KEY,\n",
        "#     verbose=True\n",
        "# )\n",
        "\n",
        "###############################################################################\n",
        "# 1. Initialize memory + config\n",
        "###############################################################################\n",
        "in_memory_store = InMemoryStore(\n",
        "    index={\n",
        "        \"embed\": init_embeddings(\"huggingface:sentence-transformers/all-MiniLM-L6-v2\"),\n",
        "        \"dims\": 384,  # Embedding dimensions\n",
        "    }\n",
        ")\n",
        "\n",
        "# A memory saver to checkpoint conversation states\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "# Initialize config with user & thread info\n",
        "config = {}\n",
        "config[\"configurable\"] = {\n",
        "    \"user_id\": \"user_1\",\n",
        "    \"thread_id\": 0,\n",
        "}\n",
        "\n",
        "###############################################################################\n",
        "# 2. Define MessagesState\n",
        "###############################################################################\n",
        "class MessagesState(TypedDict):\n",
        "    \"\"\"The state of the agent.\n",
        "\n",
        "    The key 'messages' uses add_messages as a reducer,\n",
        "    so each time this state is updated, new messages are appended.\n",
        "    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 3. Memory Tools\n",
        "###############################################################################\n",
        "def save_memory(summary_text: str, *, config: RunnableConfig, store: BaseStore) -> str:\n",
        "    \"\"\"Save the given memory for the current user and return the key.\"\"\"\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    thread_id = config.get(\"configurable\", {}).get(\"thread_id\")\n",
        "    namespace = (user_id, \"memories\")\n",
        "    memory_id = thread_id\n",
        "    store.put(namespace, memory_id, {\"memory\": summary_text})\n",
        "    return f\"Saved to memory key: {memory_id}\"\n",
        "\n",
        "def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
        "    # Extract the messages list from the event, handling potential missing key\n",
        "    messages = state[\"messages\"]\n",
        "    # Convert LangChain messages to dictionaries before storing\n",
        "    messages_dict = [{\"role\": msg.type, \"content\": msg.content} for msg in messages]\n",
        "\n",
        "    # Get the user id from the config\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    thread_id = config.get(\"configurable\", {}).get(\"thread_id\")\n",
        "    # Namespace the memory\n",
        "    namespace = (user_id, \"memories\")\n",
        "    # Create a new memory ID\n",
        "    memory_id = f\"{thread_id}\"\n",
        "    store.put(namespace, memory_id, {\"memory\": messages_dict})\n",
        "    return f\"Saved to memory key: {memory_id}\"\n",
        "\n",
        "\n",
        "# Define a Pydantic schema for the save_memory tool (if needed elsewhere)\n",
        "# https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.InMemoryStore.html\n",
        "class RecallMemory(BaseModel):\n",
        "    query_text: str = Field(..., title=\"Search Text\", description=\"The text to search from memories for similar records.\")\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"Number of results to retrieve.\")\n",
        "\n",
        "def recall_memory(query_text: str, k: int = 5) -> str:\n",
        "    \"\"\"Retrieve user memories from in_memory_store.\"\"\"\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    memories = [\n",
        "        m.value[\"memory\"] for m in in_memory_store.search((user_id, \"memories\"), query=query_text, limit=k)\n",
        "        if \"memory\" in m.value\n",
        "    ]\n",
        "    return f\"User memories: {memories}\"\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "recall_memory_tool = StructuredTool.from_function(\n",
        "    func=recall_memory,\n",
        "    name=\"Recall Memory Tool\",\n",
        "    description=\"\"\"\n",
        "      Retrieve memories relevant to the user's query.\n",
        "      \"\"\",\n",
        "    args_schema=RecallMemory,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "###############################################################################\n",
        "# 4. Summarize Node (using StructuredTool)\n",
        "###############################################################################\n",
        "# Define a Pydantic schema for the Summary tool\n",
        "class SummariseConversation(BaseModel):\n",
        "    summary_text: str = Field(..., title=\"text\", description=\"Write a summary of entire conversation here\")\n",
        "\n",
        "def summarise_node(summary_text: str):\n",
        "    \"\"\"\n",
        "    Final node that summarizes the entire conversation for the current thread,\n",
        "    saves it in memory, increments the thread_id, and ends the conversation.\n",
        "    Returns a confirmation string.\n",
        "    \"\"\"\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "    current_thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "    new_thread_id = str(int(current_thread_id) + 1)\n",
        "\n",
        "    # Prepare configuration for saving memory with updated thread id\n",
        "    config_for_saving = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": user_id,\n",
        "            \"thread_id\": new_thread_id\n",
        "        }\n",
        "    }\n",
        "    key = save_memory(summary_text, config=config_for_saving, store=in_memory_store)\n",
        "    #return f\"Summary saved under key: {key}\"\n",
        "\n",
        "# Create a StructuredTool from the function (this wraps summarise_node)\n",
        "summarise_tool = StructuredTool.from_function(\n",
        "    func=summarise_node,\n",
        "    name=\"Summary Tool\",\n",
        "    description=\"\"\"\n",
        "      Summarize the current conversation in no more than\n",
        "      1000 words. Also retain any unanswered quiz questions along with\n",
        "      your internal answers so the next conversation thread can continue.\n",
        "      Do not reveal solutions to the user yet. Use this tool to save\n",
        "      the current conversation to memory and then end the conversation.\n",
        "      \"\"\",\n",
        "    args_schema=SummariseConversation,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "def call_summary(state: MessagesState, config: RunnableConfig):\n",
        "    # Convert message dicts to HumanMessage instances if needed.\n",
        "    system_message=\"\"\"\n",
        "      Summarize the current conversation in no more than\n",
        "      1000 words. Also retain any unanswered quiz questions along with\n",
        "      your internal answers.\n",
        "      \"\"\"\n",
        "    messages = []\n",
        "    for m in state[\"messages\"]:\n",
        "        if isinstance(m, dict):\n",
        "            # Use role from dict (defaulting to 'user' if missing)\n",
        "            messages.append(AIMessage(content=system_message, role=m.get(\"role\", \"assistant\")))\n",
        "        else:\n",
        "            messages.append(m)\n",
        "\n",
        "    summaries = llm_with_tools.invoke(messages)\n",
        "\n",
        "    summary_content = summaries.content\n",
        "\n",
        "    # Call Tool Manually\n",
        "    message_with_single_tool_call = AIMessage(\n",
        "        content=\"\",\n",
        "        tool_calls=[\n",
        "            {\n",
        "                \"name\": \"Summary Tool\",\n",
        "                \"args\": {\"summary_text\": summary_content},\n",
        "                \"id\": \"tool_call_id\",\n",
        "                \"type\": \"tool_call\",\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    tool_node.invoke({\"messages\": [message_with_single_tool_call]})\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 5. Build the Graph\n",
        "###############################################################################\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "\n",
        "# Use the built-in ToolNode from langgraph that calls any declared tools.\n",
        "tools = [\n",
        "    mcq_retriever_tool,\n",
        "    web_extraction_tool,\n",
        "    ensemble_retriever_tool,\n",
        "    general_retriever_tool,\n",
        "    in_memory_retriever_tool,\n",
        "    recall_memory_tool,\n",
        "    summarise_tool,\n",
        "]\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "#end_node = ToolNode(tools=[summarise_tool])\n",
        "\n",
        "# Wrap your model with tools\n",
        "llm_with_tools = model.bind_tools(tools)\n",
        "\n",
        "###############################################################################\n",
        "# 6. The agent's main node: call_model\n",
        "###############################################################################\n",
        "def call_model(state: MessagesState, config: RunnableConfig):\n",
        "    \"\"\"\n",
        "    The main agent node that calls the LLM with the user + system messages.\n",
        "    Since our vLLM chat wrapper expects a list of BaseMessage objects,\n",
        "    we convert any dict messages to HumanMessage objects.\n",
        "    If the LLM requests a tool call, we'll route to the 'tools' node next\n",
        "    (depending on the condition).\n",
        "    \"\"\"\n",
        "    # Convert message dicts to HumanMessage instances if needed.\n",
        "    messages = []\n",
        "    for m in state[\"messages\"]:\n",
        "        if isinstance(m, dict):\n",
        "            # Use role from dict (defaulting to 'user' if missing)\n",
        "            messages.append(HumanMessage(content=m.get(\"content\", \"\"), role=m.get(\"role\", \"user\")))\n",
        "        else:\n",
        "            messages.append(m)\n",
        "\n",
        "    # Invoke the LLM (with tools) using the converted messages.\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "\n",
        "def call_summary(state: MessagesState, config: RunnableConfig):\n",
        "    # Convert message dicts to HumanMessage instances if needed.\n",
        "    system_message=\"\"\"\n",
        "      Summarize the current conversation in no more than\n",
        "      1000 words. Also retain any unanswered quiz questions along with\n",
        "      your internal answers.\n",
        "      \"\"\"\n",
        "    messages = []\n",
        "    for m in state[\"messages\"]:\n",
        "        if isinstance(m, dict):\n",
        "            # Use role from dict (defaulting to 'user' if missing)\n",
        "            messages.append(AIMessage(content=system_message, role=m.get(\"role\", \"assistant\")))\n",
        "        else:\n",
        "            messages.append(m)\n",
        "\n",
        "    summaries = llm_with_tools.invoke(messages)\n",
        "\n",
        "    summary_content = summaries.content\n",
        "\n",
        "    # Call Tool Manually\n",
        "    message_with_single_tool_call = AIMessage(\n",
        "        content=\"\",\n",
        "        tool_calls=[\n",
        "            {\n",
        "                \"name\": \"Summary Tool\",\n",
        "                \"args\": {\"summary_text\": summary_content},\n",
        "                \"id\": \"tool_call_id\",\n",
        "                \"type\": \"tool_call\",\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    tool_node.invoke({\"messages\": [message_with_single_tool_call]})\n",
        "\n",
        "###############################################################################\n",
        "# 7. Add Nodes & Edges, Then Compile\n",
        "###############################################################################\n",
        "graph_builder.add_node(\"agent\", call_model)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "#graph_builder.add_node(\"summary\", call_summary)\n",
        "\n",
        "# Entry point\n",
        "graph_builder.set_entry_point(\"agent\")\n",
        "\n",
        "# def custom_tools_condition(llm_output: dict) -> str:\n",
        "#     \"\"\"Return which node to go to next based on the LLM output.\"\"\"\n",
        "\n",
        "#     # The LLM's JSON might have a field like {\"name\": \"Recall Memory Tool\", \"arguments\": {...}}.\n",
        "#     tool_name = llm_output.get(\"name\", None)\n",
        "\n",
        "#     # If the LLM calls \"Summary Tool\", jump directly to the 'summary' node\n",
        "#     if tool_name == \"Summary Tool\":\n",
        "#         return \"summary\"\n",
        "\n",
        "#     # If the LLM calls any other recognized tool, go to 'tools'\n",
        "#     valid_tool_names = [t.name for t in tools]  # all tools in the main tool_node\n",
        "#     if tool_name in valid_tool_names:\n",
        "#         return \"tools\"\n",
        "\n",
        "#     # If there's no recognized tool name, assume we're done => go to summary\n",
        "#     return \"__end__\"\n",
        "\n",
        "# graph_builder.add_conditional_edges(\n",
        "#     \"agent\",\n",
        "#     custom_tools_condition,\n",
        "#     {\n",
        "#         \"tools\": \"tools\",\n",
        "#         \"summary\": \"summary\",\n",
        "#         \"__end__\": \"summary\",\n",
        "#     }\n",
        "# )\n",
        "\n",
        "# If LLM requests a tool, go to \"tools\", otherwise go to \"summary\"\n",
        "graph_builder.add_conditional_edges(\"agent\", tools_condition)\n",
        "#graph_builder.add_conditional_edges(\"agent\", tools_condition, {\"tools\": \"tools\", \"__end__\": \"summary\"})\n",
        "#graph_builder.add_conditional_edges(\"agent\", lambda llm_output: \"tools\" if llm_output.get(\"name\", None) in [t.name for t in tools] else \"summary\", {\"tools\": \"tools\", \"__end__\": \"summary\"}\n",
        "\n",
        "# If we used a tool, return to the agent for final answer or more tools\n",
        "graph_builder.add_edge(\"tools\", \"agent\")\n",
        "#graph_builder.add_edge(\"agent\", \"summary\")\n",
        "#graph_builder.set_finish_point(\"summary\")\n",
        "\n",
        "# Compile the graph with checkpointing and persistent store\n",
        "graph = graph_builder.compile(checkpointer=checkpointer, store=in_memory_store)\n",
        "\n",
        "#from langgraph.prebuilt import create_react_agent\n",
        "#graph = create_react_agent(llm_with_tools, tools=tool_node, checkpointer=checkpointer, store=in_memory_store)\n",
        "\n",
        "#from IPython.display import Image, display\n",
        "#display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "\n",
        "\n",
        "########################################\n",
        "# Gradio Chatbot Application\n",
        "########################################\n",
        "\n",
        "import gradio as gr\n",
        "from gradio import ChatMessage\n",
        "\n",
        "system_prompt = \"You are a helpful Assistant. You will always use the tools available to you from {tools} to address user queries.\"\n",
        "\n",
        "########################################\n",
        "# Upload_documents\n",
        "########################################\n",
        "\n",
        "def upload_documents(file_list: List):\n",
        "    \"\"\"\n",
        "    Load documents into in-memory vector store.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "\n",
        "    for doc_path in file_list:\n",
        "        _documents.extend(load_file(doc_path))\n",
        "\n",
        "    # Split the documents into smaller chunks for better embedding coverage\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=300, # Split into chunks of 512 characters\n",
        "        chunk_overlap=50, # Overlap by 50 characters\n",
        "        add_start_index=True\n",
        "    )\n",
        "    chunked_texts = splitter.split_documents(_documents)\n",
        "    in_memory_vs.add_documents(chunked_texts)\n",
        "    return f\"Uploaded {len(file_list)} documents into in-memory vector store.\"\n",
        "\n",
        "\n",
        "########################################\n",
        "# Submit_queries (ChatInterface Function)\n",
        "########################################\n",
        "def submit_queries(message, _messages):\n",
        "    \"\"\"\n",
        "    - message: dict with {\"text\": ..., \"files\": [...]}\n",
        "    - history: list of ChatMessage\n",
        "    \"\"\"\n",
        "    _messages=[]\n",
        "    user_text = message.get(\"text\", \"\")\n",
        "    user_files = message.get(\"files\", [])\n",
        "\n",
        "    # Process user-uploaded files\n",
        "    if user_files:\n",
        "      for file_obj in user_files:\n",
        "          _messages.append(ChatMessage(role=\"user\", content=f\"Uploaded file: {file_obj}\"))\n",
        "      upload_response = upload_documents(user_files)\n",
        "      _messages.append(ChatMessage(role=\"assistant\", content=upload_response))\n",
        "      yield _messages\n",
        "      return # Exit early since we don't need to process text or call the LLM\n",
        "\n",
        "    # Append user text if present\n",
        "    if user_text:\n",
        "        events = graph.stream(\n",
        "      {\n",
        "          \"messages\": [\n",
        "              {\"role\": \"system\", \"content\": system_prompt},\n",
        "              {\"role\": \"user\",   \"content\": user_text},\n",
        "          ]\n",
        "        },\n",
        "        config,\n",
        "        stream_mode=\"values\"\n",
        "        )\n",
        "\n",
        "        for event in events:\n",
        "          response =  event[\"messages\"][-1]\n",
        "          if isinstance(response, AIMessage):\n",
        "            if \"tool_calls\" in response.additional_kwargs:\n",
        "              _messages.append(\n",
        "                  ChatMessage(role=\"assistant\",\n",
        "                              content=str(response.tool_calls[0][\"args\"]),\n",
        "                              metadata={\"title\": str(response.tool_calls[0][\"name\"]),\n",
        "                                        \"id\": config[\"configurable\"][\"thread_id\"]\n",
        "                                        }\n",
        "                              ))\n",
        "              yield _messages\n",
        "            else:\n",
        "              _messages.append(ChatMessage(role=\"assistant\",\n",
        "                                           content=response.content,\n",
        "                                           metadata={\"id\": config[\"configurable\"][\"thread_id\"]\n",
        "                                                     }\n",
        "                                           ))\n",
        "              yield _messages\n",
        "    return _messages\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "# 6) Main Gradio Interface\n",
        "########################################\n",
        "with gr.Blocks(theme=\"ocean\") as AI_Tutor:\n",
        "    gr.Markdown(\"# AI Tutor Chatbot (Gradio App)\")\n",
        "\n",
        "    # Primary Chat Interface\n",
        "    chat_interface = gr.ChatInterface(\n",
        "        fn=submit_queries,\n",
        "        type=\"messages\",\n",
        "        chatbot=gr.Chatbot(\n",
        "            label=\"Chat Window\",\n",
        "            height=500\n",
        "        ),\n",
        "        textbox=gr.MultimodalTextbox(\n",
        "            file_count=\"multiple\",\n",
        "            file_types=None,\n",
        "            sources=[\"upload\"],\n",
        "            label=\"Type your query here:\",\n",
        "            placeholder=\"Enter your question...\",\n",
        "        ),\n",
        "        title=\"AI Tutor Chatbot\",\n",
        "        description=\"Ask me anything about Artificial Intelligence!\",\n",
        "        multimodal=True,\n",
        "        save_history=True,\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    AI_Tutor.launch(inline=False, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "531a1613b7884b98a4f6ecb738407550",
            "89f28159d26b486eb75397e4ecbeb612",
            "bc0bf1f8b76043ac996e74c643013271",
            "773165eb96844842b2795ff7da4d762e",
            "7b78bea13f8d4e798e220ae1c02df8df",
            "4285bbc9e83449e58da3f37716aa1edd",
            "3da443126e4c434ca6c694c6d2d653eb",
            "394d406cbe5d49e6a630b97a85b3b0d2",
            "7a3226bafe1e452b830ddf7936b52b3e",
            "6c001473620b4b1aa097c5c8bec2f563",
            "aa1651313c76474d845be1e39b00936b",
            "dadf3cfdb3fe4fb7ae0e95bd5acdc8c3",
            "1a9bad03829542f78aabad029e0aa49c",
            "21329eaa3cba4a438bcd67297b588b8a",
            "03b1aa02da9a4996b042aa233611a713",
            "6d8f1b6222bb4e398bffd7950e471349",
            "5506bef9ca9940cdb5a4eb257a019d74",
            "a2ca9d561aae4bd0a83fd79a85a01412",
            "fca1515dce9c4a80a758d927692666c5",
            "3e19ad2d93bd4c9cbde705dc7ba7fc23",
            "bddede6d2ac24d208044a0595f1e45a3",
            "569db0b4eb7f442eb90f2cf4599ad7e9",
            "bb957dc078a44668bd7f89190d1d895a",
            "101ccc855a06494b866410875860e639",
            "c6e7d3e344124534a13a88cf10eaeb10",
            "5bbd4d0438934c69b73f1060acb166ae",
            "457a7fa9b29a40ad99c4424aa17b948d",
            "d8beebab95b043e2a386a08e3e75794f",
            "99a004a87396434f94a009ffc7130129",
            "bfc68326b37d449d8362738b9e14cbf2",
            "f47434872d6645f1a9ea6d4c85cd23f1",
            "1070868417ec48b68e782b65c58d2cce",
            "0041ed8cac1647eea8793fb9c575a9e1",
            "749a032695fe49439eee3a41bd55c36c",
            "21988dfce8a849c6ad9b06f57b9d1adb",
            "17494705d9be452280b8852c1fbe90aa",
            "e1019d54f44d45cf82247674009460bd",
            "50adf5bf509a486180041b40fd069a3f",
            "c0b20200816740a8a5d0dce540543874",
            "dddbfbdb39ec4ab79fc41d52f40e8c7b",
            "7cb117d9facc40468dfaa10dc0a62db5",
            "35d174d802264196b9da42f6b521d9b3",
            "270d4f24059740d98e6d9d59b20173c4",
            "00131d1ed8434d95897ca2e18cc5081d",
            "d49eb67d8ba643b88e44f23cacc86c93",
            "559389f7b09f4138b38d8b35b1584738",
            "c344468e154d4e91be66141d409f0b3b",
            "78273d214c034060a2ae3a00d1425715",
            "5c2c87195010404e9d94558329a2cf86",
            "ac606b1aa9a94c6d9e87dbbceb14c427",
            "e3a50b4a79b14d83948c2b4a2178863d",
            "a46b1a51bd3b44b6bc86f9987d56901e",
            "34f6eb2bfc7943c8a4542a0d229e25db",
            "f752284f6bc240fb9d0992d09aa3e75c",
            "0173607844b048fcbf7dfd00176818ad",
            "d5b5997ecb524b30a457d85b2e1a9efe",
            "c15d801015454884919c24965fe71542",
            "c707929302f148699bc200402fa78e8d",
            "66396ce11a10415c98d5ff5059d3daf0",
            "c45a860c65924eb4b48e966c56edda56",
            "f927d0f5718c4e869a1efc6a22bb0d12",
            "f9679bd7cdb84168bbfba6c7afb5cc54",
            "87d547c8d15d4d7198f353b0ff827456",
            "a0c7a1d43e994eaf8780742c63101054",
            "d17848c8cc1e43b7bea9f10584cde1d0",
            "8a65e8ac48ac43d79dc093ed1bde70d6",
            "0f159346c6c84876a3053575868bf9b6",
            "343d1e4d764d493c8803d0938553cf4c",
            "02ca03f0c572445a91a166d9cc12890a",
            "1c932836a78c41d8941269872d0b4650",
            "1f42e6294d144fed904c61001cd2d45a",
            "d8d1955f428b403f93a96a6943a6f94a",
            "c2a4d8be05ae43eca3a6ea6655f03582",
            "b3f4d9b4087e4898b7f61350f7a16e71",
            "72732fed485d42e7b49e8eb1b6a800dd",
            "a1170bd0bfe940e6a783de1a2ab174b8",
            "cb36b2059ab44b1abc53ec9186df6814",
            "ff0d1857feff43a2b88211301c4195bb",
            "f29e325edde64e2a8460d85c7d4a31be",
            "543633e9d7e64d0c8ade5e7cb4e24e34",
            "180f306246e24d01bb3a0883b2c688aa",
            "c14e22e125fd4fdfb985628851beec6b",
            "10e6f2f694ac4e5a8672a74279754701",
            "74beb33967934d5ba7ab6e67f53093ec",
            "58b7a171059c4e83bb54dc358a8c651d",
            "4a98f1b2de82430c8fbadbc33fa9cfa0",
            "8f96d05cc3aa46d78b5473a848e1b169",
            "2228c2582d7e479db4967bd0db8c13e9",
            "ede9260f6249448694e8732fc7d0d905",
            "6da6040066254e459681f66c3fd97ed0",
            "bbfcc6a38c5242cea642bba08d25b8a2",
            "1ea39038072d4a878b0f5f8d3f28d63d",
            "889f8e568fbf450abb49738545395ce0",
            "afd0fa15b6cd48389c6c824dc3eabf4f",
            "69da5725393c46b7b933550c6e907ce9",
            "76bfc330e55b47c2b9d4ccf8a90b4265",
            "ac84606f330140259bde954e459a61ee",
            "a68e774302e24a3f8ab8358ce07b883e",
            "628ff5609f184a46b33b8f4a83fafcdc",
            "4527f02956614ed7ba6633ae130d795a",
            "47735b4899594106bfc53c203f67b5bb",
            "8197cc2063694f4491d4bd4f37197f8c",
            "a72af582bd3b4e0fbdb3f45d54649392",
            "350a02d010bc4d50afddd2984aa7ee0b",
            "7d2b5232d2ce4afa8879aac76c037cd6",
            "53f99850e67e4b398042c025ba5add2d",
            "f7b7044cabed4220a8ef96b932c9049a",
            "75144eebbbf1420785ea0c6928b787fd",
            "6164ff8fe85e4bd3872778ee043c57e5",
            "b53dbcc2222f4283aa366c11d1bd58e0",
            "456670d20ca348c986d78b61f479cfa4",
            "e8d958b1081541f6a8f96fe45164c46b",
            "4aa32b85c71d4d218c51707225007d96",
            "36d334a121d54588bf9890e30092a674",
            "6e402d423f65478dbcb681b8396df6e8",
            "c7c1d404f22b4ac697e1b90fe2025e4a",
            "677f92b3bf164a1fb28683f4ab974a3c",
            "aac4fbbe7a904ec3b3404f84a972bb18",
            "011f7121a5d2485284049740e82765e6",
            "8773ab8f564644b394523bc9195dd98e",
            "ab6b0e3935eb45c9963df8975dbce117"
          ]
        },
        "id": "PVFoymfXkYez",
        "outputId": "ef6d9d37-266a-4f65-b27d-44300e625235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "531a1613b7884b98a4f6ecb738407550"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dadf3cfdb3fe4fb7ae0e95bd5acdc8c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb957dc078a44668bd7f89190d1d895a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "749a032695fe49439eee3a41bd55c36c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d49eb67d8ba643b88e44f23cacc86c93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5b5997ecb524b30a457d85b2e1a9efe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f159346c6c84876a3053575868bf9b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff0d1857feff43a2b88211301c4195bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ede9260f6249448694e8732fc7d0d905"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4527f02956614ed7ba6633ae130d795a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "456670d20ca348c986d78b61f479cfa4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-d5189d42cf58>:566: LangChainBetaWarning: The function `init_embeddings` is in beta. It is actively being worked on, so the API may change.\n",
            "  \"embed\": init_embeddings(\"huggingface:sentence-transformers/all-MiniLM-L6-v2\"),\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:285: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:310: UserWarning: The type of the gr.Chatbot does not match the type of the gr.ChatInterface.The type of the gr.ChatInterface, 'messages', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://495da550e12dea8814.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/routes.py\", line 1023, in predict\n",
            "    output = await route_utils.call_process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2096, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1643, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py\", line 801, in _append_message_to_history\n",
            "    message_dicts = self._message_as_message_dict(message, role)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py\", line 839, in _message_as_message_dict\n",
            "    for x in msg.get(\"files\", []):\n",
            "             ^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'get'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2096, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1655, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 735, in async_iteration\n",
            "    return await anext(iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 840, in asyncgen_wrapper\n",
            "    response = await iterator.__anext__()\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py\", line 896, in _stream_fn\n",
            "    history = self._append_message_to_history(message, history, \"user\")\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py\", line 801, in _append_message_to_history\n",
            "    message_dicts = self._message_as_message_dict(message, role)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py\", line 839, in _message_as_message_dict\n",
            "    for x in msg.get(\"files\", []):\n",
            "             ^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'get'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23Gn89-wNN4b",
        "outputId": "fb08174c-36c4-4ca6-c332-ccec3918ff10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Why did the neural network go to therapy?\\n\\nBecause it had a lot of \"hidden layers\" and was struggling to \"backpropagate\" its emotions!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 52, 'total_tokens': 84, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'unsloth/llama-3-8b-Instruct-bnb-4bit', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-31488c76-daeb-4a16-a5fb-e222675c6d39-0', usage_metadata={'input_tokens': 52, 'output_tokens': 32, 'total_tokens': 84, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(\n",
        "    model=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    temperature=0.5,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    api_key=\"not_required\",\n",
        "    base_url=\"http://localhost:8000/v1\",\n",
        "    # organization=\"...\",\n",
        "    # other params...\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant and study companion.\",\n",
        "    ),\n",
        "    (\"human\", \"Tell me a joke about Deep Learning.\"),\n",
        "]\n",
        "ai_msg = model.invoke(messages)\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7soxxBDs_qr"
      },
      "source": [
        "### GROQ Serving (For Test References)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyup3FLGDE34",
        "outputId": "dfe183e0-1458-4049-d1a6-e654b13a4ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"<think>\\n\\n</think>\\n\\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 7, 'total_tokens': 51, 'completion_time': 0.16, 'prompt_time': 0.00349463, 'queue_time': 0.234613626, 'total_time': 0.16349463}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_492bd52206', 'finish_reason': 'stop', 'logprobs': None} id='run-e002d566-6ff3-4a56-b48e-8029a69bfad1-0' usage_metadata={'input_tokens': 7, 'output_tokens': 44, 'total_tokens': 51}\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Initialize Groq LLM\n",
        "model = ChatGroq(\n",
        "    model_name=\"deepseek-r1-distill-llama-70b\",   #\"llama-3.2-3b-preview\", \"deepseek-r1-distill-llama-70b\"\n",
        "    temperature=0.6,\n",
        "    api_key=GROQ_API_KEY,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(model.invoke(\"Who are you?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4cHTN0bHivs"
      },
      "source": [
        "### LangGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "SXMyAFM6bSiz",
        "outputId": "c21264b5-e87c-493a-daaa-39918c529888"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "from typing import (\n",
        "    Annotated,\n",
        "    Sequence,\n",
        "    TypedDict,\n",
        "    List,\n",
        ")\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# LangChain / LangGraph imports\n",
        "from langchain_core.messages import (\n",
        "    SystemMessage,\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    BaseMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "from langchain_core.tools import StructuredTool\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "from langgraph.prebuilt import InjectedStore\n",
        "from langgraph.store.base import BaseStore\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain.embeddings import init_embeddings\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "###############################################################################\n",
        "# 1. Initialize memory + config\n",
        "###############################################################################\n",
        "in_memory_store = InMemoryStore(\n",
        "    index={\n",
        "        \"embed\": init_embeddings(\"huggingface:sentence-transformers/all-MiniLM-L6-v2\"),\n",
        "        \"dims\": 384,  # Embedding dimensions\n",
        "    }\n",
        ")\n",
        "\n",
        "# A memory saver to checkpoint conversation states\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "# Initialize config with user & thread info\n",
        "config = {}\n",
        "config[\"configurable\"] = {\n",
        "    \"user_id\": \"user_1\",\n",
        "    \"thread_id\": 0,\n",
        "}\n",
        "\n",
        "###############################################################################\n",
        "# 2. Define MessagesState\n",
        "###############################################################################\n",
        "class MessagesState(TypedDict):\n",
        "    \"\"\"The state of the agent.\n",
        "\n",
        "    The key 'messages' uses add_messages as a reducer,\n",
        "    so each time this state is updated, new messages are appended.\n",
        "    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 3. Memory Tools\n",
        "###############################################################################\n",
        "def save_memory(summary_text: str, *, config: RunnableConfig, store: BaseStore) -> str:\n",
        "    \"\"\"Save the given memory for the current user and return the key.\"\"\"\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    thread_id = config.get(\"configurable\", {}).get(\"thread_id\")\n",
        "    namespace = (user_id, \"memories\")\n",
        "    memory_id = thread_id\n",
        "    store.put(namespace, memory_id, {\"memory\": summary_text})\n",
        "    return f\"Saved to memory key: {memory_id}\"\n",
        "\n",
        "def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
        "    # Extract the messages list from the event, handling potential missing key\n",
        "    messages = state[\"messages\"]\n",
        "    # Convert LangChain messages to dictionaries before storing\n",
        "    messages_dict = [{\"role\": msg.type, \"content\": msg.content} for msg in messages]\n",
        "\n",
        "    # Get the user id from the config\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    thread_id = config.get(\"configurable\", {}).get(\"thread_id\")\n",
        "    # Namespace the memory\n",
        "    namespace = (user_id, \"memories\")\n",
        "    # Create a new memory ID\n",
        "    memory_id = f\"{thread_id}\"\n",
        "    store.put(namespace, memory_id, {\"memory\": messages_dict})\n",
        "    return f\"Saved to memory key: {memory_id}\"\n",
        "\n",
        "\n",
        "# Define a Pydantic schema for the save_memory tool (if needed elsewhere)\n",
        "# https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.InMemoryStore.html\n",
        "class RecallMemory(BaseModel):\n",
        "    query_text: str = Field(..., title=\"Search Text\", description=\"The text to search from memories for similar records.\")\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"Number of results to retrieve.\")\n",
        "\n",
        "def recall_memory(query_text: str, k: int = 5) -> str:\n",
        "    \"\"\"Retrieve user memories from in_memory_store.\"\"\"\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    memories = [\n",
        "        m.value[\"memory\"] for m in in_memory_store.search((user_id, \"memories\"), query=query_text, limit=k)\n",
        "        if \"memory\" in m.value\n",
        "    ]\n",
        "    return f\"User memories: {memories}\"\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "recall_memory_tool = StructuredTool.from_function(\n",
        "    func=recall_memory,\n",
        "    name=\"Recall Memory Tool\",\n",
        "    description=\"\"\"\n",
        "      Retrieve memories relevant to the user's query.\n",
        "      \"\"\",\n",
        "    args_schema=RecallMemory,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "###############################################################################\n",
        "# 4. Summarize Node (using StructuredTool)\n",
        "###############################################################################\n",
        "# Define a Pydantic schema for the Summary tool\n",
        "class SummariseConversation(BaseModel):\n",
        "    summary_text: str = Field(..., title=\"text\", description=\"Write a summary of entire conversation here\")\n",
        "\n",
        "def summarise_node(summary_text: str):\n",
        "    \"\"\"\n",
        "    Final node that summarizes the entire conversation for the current thread,\n",
        "    saves it in memory, increments the thread_id, and ends the conversation.\n",
        "    Returns a confirmation string.\n",
        "    \"\"\"\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "    current_thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "    new_thread_id = str(int(current_thread_id) + 1)\n",
        "\n",
        "    # Prepare configuration for saving memory with updated thread id\n",
        "    config_for_saving = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": user_id,\n",
        "            \"thread_id\": new_thread_id\n",
        "        }\n",
        "    }\n",
        "    key = save_memory(summary_text, config=config_for_saving, store=in_memory_store)\n",
        "    #return f\"Summary saved under key: {key}\"\n",
        "\n",
        "# Create a StructuredTool from the function (this wraps summarise_node)\n",
        "summarise_tool = StructuredTool.from_function(\n",
        "    func=summarise_node,\n",
        "    name=\"Summary Tool\",\n",
        "    description=\"\"\"\n",
        "      Summarize the current conversation in no more than\n",
        "      1000 words. Also retain any unanswered quiz questions along with\n",
        "      your internal answers so the next conversation thread can continue.\n",
        "      Do not reveal solutions to the user yet. Use this tool to save\n",
        "      the current conversation to memory and then end the conversation.\n",
        "      \"\"\",\n",
        "    args_schema=SummariseConversation,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "def call_summary(state: MessagesState, config: RunnableConfig):\n",
        "    # Convert message dicts to HumanMessage instances if needed.\n",
        "    system_message=\"\"\"\n",
        "      Summarize the current conversation in no more than\n",
        "      1000 words. Also retain any unanswered quiz questions along with\n",
        "      your internal answers.\n",
        "      \"\"\"\n",
        "    messages = []\n",
        "    for m in state[\"messages\"]:\n",
        "        if isinstance(m, dict):\n",
        "            # Use role from dict (defaulting to 'user' if missing)\n",
        "            messages.append(AIMessage(content=system_message, role=m.get(\"role\", \"assistant\")))\n",
        "        else:\n",
        "            messages.append(m)\n",
        "\n",
        "    summaries = llm_with_tools.invoke(messages)\n",
        "\n",
        "    summary_content = summaries.content\n",
        "\n",
        "    # Call Tool Manually\n",
        "    message_with_single_tool_call = AIMessage(\n",
        "        content=\"\",\n",
        "        tool_calls=[\n",
        "            {\n",
        "                \"name\": \"Summary Tool\",\n",
        "                \"args\": {\"summary_text\": summary_content},\n",
        "                \"id\": \"tool_call_id\",\n",
        "                \"type\": \"tool_call\",\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    tool_node.invoke({\"messages\": [message_with_single_tool_call]})\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 5. Build the Graph\n",
        "###############################################################################\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "\n",
        "# Use the built-in ToolNode from langgraph that calls any declared tools.\n",
        "tools = [\n",
        "    mcq_retriever_tool,\n",
        "    web_extraction_tool,\n",
        "    ensemble_retriever_tool,\n",
        "    general_retriever_tool,\n",
        "    in_memory_retriever_tool,\n",
        "    recall_memory_tool,\n",
        "    summarise_tool,\n",
        "]\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "#end_node = ToolNode(tools=[summarise_tool])\n",
        "\n",
        "# Wrap your model with tools\n",
        "llm_with_tools = model.bind_tools(tools)\n",
        "\n",
        "###############################################################################\n",
        "# 6. The agent's main node: call_model\n",
        "###############################################################################\n",
        "def call_model(state: MessagesState, config: RunnableConfig):\n",
        "    \"\"\"\n",
        "    The main agent node that calls the LLM with the user + system messages.\n",
        "    Since our vLLM chat wrapper expects a list of BaseMessage objects,\n",
        "    we convert any dict messages to HumanMessage objects.\n",
        "    If the LLM requests a tool call, we'll route to the 'tools' node next\n",
        "    (depending on the condition).\n",
        "    \"\"\"\n",
        "    # Convert message dicts to HumanMessage instances if needed.\n",
        "    messages = []\n",
        "    for m in state[\"messages\"]:\n",
        "        if isinstance(m, dict):\n",
        "            # Use role from dict (defaulting to 'user' if missing)\n",
        "            messages.append(HumanMessage(content=m.get(\"content\", \"\"), role=m.get(\"role\", \"user\")))\n",
        "        else:\n",
        "            messages.append(m)\n",
        "\n",
        "    # Invoke the LLM (with tools) using the converted messages.\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "\n",
        "def call_summary(state: MessagesState, config: RunnableConfig):\n",
        "    # Convert message dicts to HumanMessage instances if needed.\n",
        "    system_message=\"\"\"\n",
        "      Summarize the current conversation in no more than\n",
        "      1000 words. Also retain any unanswered quiz questions along with\n",
        "      your internal answers.\n",
        "      \"\"\"\n",
        "    messages = []\n",
        "    for m in state[\"messages\"]:\n",
        "        if isinstance(m, dict):\n",
        "            # Use role from dict (defaulting to 'user' if missing)\n",
        "            messages.append(AIMessage(content=system_message, role=m.get(\"role\", \"assistant\")))\n",
        "        else:\n",
        "            messages.append(m)\n",
        "\n",
        "    summaries = llm_with_tools.invoke(messages)\n",
        "\n",
        "    summary_content = summaries.content\n",
        "\n",
        "    # Call Tool Manually\n",
        "    message_with_single_tool_call = AIMessage(\n",
        "        content=\"\",\n",
        "        tool_calls=[\n",
        "            {\n",
        "                \"name\": \"Summary Tool\",\n",
        "                \"args\": {\"summary_text\": summary_content},\n",
        "                \"id\": \"tool_call_id\",\n",
        "                \"type\": \"tool_call\",\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    tool_node.invoke({\"messages\": [message_with_single_tool_call]})\n",
        "\n",
        "###############################################################################\n",
        "# 7. Add Nodes & Edges, Then Compile\n",
        "###############################################################################\n",
        "graph_builder.add_node(\"agent\", call_model)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "#graph_builder.add_node(\"summary\", call_summary)\n",
        "\n",
        "# Entry point\n",
        "graph_builder.set_entry_point(\"agent\")\n",
        "\n",
        "# def custom_tools_condition(llm_output: dict) -> str:\n",
        "#     \"\"\"Return which node to go to next based on the LLM output.\"\"\"\n",
        "\n",
        "#     # The LLM's JSON might have a field like {\"name\": \"Recall Memory Tool\", \"arguments\": {...}}.\n",
        "#     tool_name = llm_output.get(\"name\", None)\n",
        "\n",
        "#     # If the LLM calls \"Summary Tool\", jump directly to the 'summary' node\n",
        "#     if tool_name == \"Summary Tool\":\n",
        "#         return \"summary\"\n",
        "\n",
        "#     # If the LLM calls any other recognized tool, go to 'tools'\n",
        "#     valid_tool_names = [t.name for t in tools]  # all tools in the main tool_node\n",
        "#     if tool_name in valid_tool_names:\n",
        "#         return \"tools\"\n",
        "\n",
        "#     # If there's no recognized tool name, assume we're done => go to summary\n",
        "#     return \"__end__\"\n",
        "\n",
        "# graph_builder.add_conditional_edges(\n",
        "#     \"agent\",\n",
        "#     custom_tools_condition,\n",
        "#     {\n",
        "#         \"tools\": \"tools\",\n",
        "#         \"summary\": \"summary\",\n",
        "#         \"__end__\": \"summary\",\n",
        "#     }\n",
        "# )\n",
        "\n",
        "# If LLM requests a tool, go to \"tools\", otherwise go to \"summary\"\n",
        "graph_builder.add_conditional_edges(\"agent\", tools_condition)\n",
        "#graph_builder.add_conditional_edges(\"agent\", tools_condition, {\"tools\": \"tools\", \"__end__\": \"summary\"})\n",
        "#graph_builder.add_conditional_edges(\"agent\", lambda llm_output: \"tools\" if llm_output.get(\"name\", None) in [t.name for t in tools] else \"summary\", {\"tools\": \"tools\", \"__end__\": \"summary\"}\n",
        "\n",
        "# If we used a tool, return to the agent for final answer or more tools\n",
        "graph_builder.add_edge(\"tools\", \"agent\")\n",
        "#graph_builder.add_edge(\"agent\", \"summary\")\n",
        "#graph_builder.set_finish_point(\"summary\")\n",
        "\n",
        "# Compile the graph with checkpointing and persistent store\n",
        "graph = graph_builder.compile(checkpointer=checkpointer, store=in_memory_store)\n",
        "\n",
        "#from langgraph.prebuilt import create_react_agent\n",
        "#graph = create_react_agent(llm_with_tools, tools=tool_node, checkpointer=checkpointer, store=in_memory_store)\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F17kNlhSHivw"
      },
      "source": [
        "### Testing with Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "5K-VIAskyrtM",
        "outputId": "03cfb75e-85f4-4acb-ba36-ce0a42a8eb57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversation Thread ID: 0 -> 1\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Find out about Deep Learning from databases. Using Web Extraction Tool, search from website: https://www.ibm.com/think/topics/artificial-intelligence and https://www.ibm.com/think/topics/machine-learning to provide the context\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Web Extraction Tool (call_8ybd)\n",
            " Call ID: call_8ybd\n",
            "  Args:\n",
            "    input: Deep Learning\n",
            "    k: 5\n",
            "    url: https://www.ibm.com/think/topics/artificial-intelligence, https://www.ibm.com/think/topics/machine-learning\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: Web Extraction Tool\n",
            "\n",
            "[\"like humans, explore its history and the types of AI, and uncover the ways it impacts our lives.     What is machine learning?   Gain insight into how ML leverages data and algorithms, its use cases and associated concerns to empower responsible and innovative use of the technology.    What is deep learning?   Discover how deep learning simulates our brain, helping systems learn to identify and perform complex tasks with increasing accuracy unsupervised.    What is the k-nearest neighbors algorithm?   Learn about the k-nearest neighbors algorithm, one of the popular and simplest classification and regression classifiers used in machine learning today.     What is natural language processing (NLP)?   Dive into how NLP enables machines to understand and respond to text or voice data and learn about various NLP tasks to obtain optimal results.    What are neural networks?   Learn how neural networks allow programs to recognize patterns and solve common problems in artificial\", \"news     Weekly insights, research and expert views on AI, security, cloud and more in the Think Newsletter  Sign up now         Videos     Elevate your understanding with our expert-led educational videos and YouTube playlists on the biggest topics and trends in tech. Master the basics, enhance your skill set or acquire real-world strategies for leveraging technology.  Watch the videos         Podcasts     Explore our diverse podcast series, featuring expert discussions on top tech topics, real-world application of our products and a look at our culture of diversity, learning and agility.  Tune in to our podcasts\", \"era of quantum computing with quantum-safe cryptography.     What is a qubit?   Encode data with a qubit, the basic unit of information in quantum computing, which serves as the quantum equivalent of the traditional bit in classical computers.    What is quantum-centric supercomputing?   Leverage quantum-centric supercomputing to combine quantum and traditional high-performance computing (HPC), enabling solutions for complex real-world problems.    What is neuromorphic computing?  Understand how neuromorphic computing, also known as neuromorphic engineering, mimics the way the human brain works.     What is supercomputing?   Explore how supercomputing uses high-performance computing systems to determine or calculate, reducing the overall time to a solution.           Security and identity        View all security topics     What is cybersecurity?   Stay informed about cybersecurity technology, types of threats and best practices to protect your critical systems and sensitive\", \"regression method and by knowing which type of logistic regression to use.    What is Monte Carlo simulation?   Learn how to run a Monte Carlo Simulation, a computational algorithm, by using repeated random sampling to estimate the possible outcomes of an uncertain event.    What is exploratory data analysis?   Know about exploratory data analysis to effectively analyze and summarize data sets, helping one to discover patterns and anomalies, and generate a hypothesis.    What is data science?   Leverage data science to unlock business insights from an increasing amount of data, accelerate digital transformation and data-driven decision making.    Asset management View all asset management topics     What is visual inspection?   Master visual inspection of equipments to detect defects both in person and remotely using digital images to maintain quality and safety standards.     What is preventive maintenance?   Implement preventive maintenance by combining regular maintenance tasks\", \"Explore how energy management helps businesses proactively monitor, control and optimize energy consumption to converse use and reduce energy costs.     What is electronic data interchange (EDI)?   Learn how electronic data interchange (EDI) is used to exchange business information, saving time and eliminating costly errors caused by manual processing.    What is managed file transfer?   Facilitate secure, reliable and automated exchange of file-based data over the internet with managed file transfer (MFT) technology to meet compliance needs.    What is the TCFD?   Find out how the Task Force on Climate-related Financial Disclosures (TCFD) seeks to keep investors better-informed about companies' climate-related risks.     Explore more ways to learn      Stay ahead with the latest tech news     Weekly insights, research and expert views on AI, security, cloud and more in the Think Newsletter  Sign up now         Videos     Elevate your understanding with our expert-led educational\"]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Web Extraction Tool (call_awfz)\n",
            " Call ID: call_awfz\n",
            "  Args:\n",
            "    input: Deep Learning\n",
            "    k: 5\n",
            "    url: https://www.ibm.com/think/topics/artificial-intelligence, https://www.ibm.com/think/topics/machine-learning\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: Web Extraction Tool\n",
            "\n",
            "[\"like humans, explore its history and the types of AI, and uncover the ways it impacts our lives.     What is machine learning?   Gain insight into how ML leverages data and algorithms, its use cases and associated concerns to empower responsible and innovative use of the technology.    What is deep learning?   Discover how deep learning simulates our brain, helping systems learn to identify and perform complex tasks with increasing accuracy unsupervised.    What is the k-nearest neighbors algorithm?   Learn about the k-nearest neighbors algorithm, one of the popular and simplest classification and regression classifiers used in machine learning today.     What is natural language processing (NLP)?   Dive into how NLP enables machines to understand and respond to text or voice data and learn about various NLP tasks to obtain optimal results.    What are neural networks?   Learn how neural networks allow programs to recognize patterns and solve common problems in artificial\", \"Think Topics | IBM             Home Think  Topics      Explainers            Become an expert on emerging and fundamental tech topics            Featured topics  Demystify transformative technologies. Decode tech topics with content crafted by IBM experts.        What is data mining?  Learn how data mining combines statistics and artificial intelligence to analyze large data sets to discover meaningful insights and useful information.        What is a vector database?  Use a vector database for storing, managing and indexing huge quantities of high-dimensional vector data efficiently for generative AI use cases and applications.        What is a chatbot?  Explore chatbot technology to understand how chatbots simulate human conversation, often using NLP to parse inputs and generative AI to automate responses.        What is a DDoS (distributed denial of service) attack?  Find out how a DDoS attack floods websites and other network resources with malicious traffic, disrupting normal\", \"news     Weekly insights, research and expert views on AI, security, cloud and more in the Think Newsletter  Sign up now         Videos     Elevate your understanding with our expert-led educational videos and YouTube playlists on the biggest topics and trends in tech. Master the basics, enhance your skill set or acquire real-world strategies for leveraging technology.  Watch the videos         Podcasts     Explore our diverse podcast series, featuring expert discussions on top tech topics, real-world application of our products and a look at our culture of diversity, learning and agility.  Tune in to our podcasts\", \"era of quantum computing with quantum-safe cryptography.     What is a qubit?   Encode data with a qubit, the basic unit of information in quantum computing, which serves as the quantum equivalent of the traditional bit in classical computers.    What is quantum-centric supercomputing?   Leverage quantum-centric supercomputing to combine quantum and traditional high-performance computing (HPC), enabling solutions for complex real-world problems.    What is neuromorphic computing?  Understand how neuromorphic computing, also known as neuromorphic engineering, mimics the way the human brain works.     What is supercomputing?   Explore how supercomputing uses high-performance computing systems to determine or calculate, reducing the overall time to a solution.           Security and identity        View all security topics     What is cybersecurity?   Stay informed about cybersecurity technology, types of threats and best practices to protect your critical systems and sensitive\", \"in person and remotely using digital images to maintain quality and safety standards.     What is preventive maintenance?   Implement preventive maintenance by combining regular maintenance tasks with ML, data analytics and predictive asset health monitoring to prevent downtime.    What is a CMMS?   Enhance maintenance efforts with a computerized maintenance management system (CMMS) for efficient scheduling, managing, and tracking of maintenance tasks.    What is a digital twin?   Explore how digital twins create a virtual copy of an object or system by integrating real-time data, simulation, ML and reasoning to support decision-making.    What is field service management?   Learn field service management to effectively coordinate resources, employees and equipment, in work activities and operations off company property.           Business operations        View all business automation topics     What is business automation?   Explore what business automation is, why it matters and\"]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "<tool_call>{\"name\":\"Assistant References Retrieval Tool\",\"arguments\":{\"input\":\"What is deep learning?\",\"k\":5}}<｜tool▁calls▁end｜>\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Saved to memory key: 1'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# URL\n",
        "url1 = \"https://www.ibm.com/think/topics/artificial-intelligence\"\n",
        "url2 = \"https://www.ibm.com/think/topics/machine-learning\"\n",
        "\n",
        "question_1 = (f\"Find out about Deep Learning from databases. Using Web Extraction Tool, search from website: {url1} and {url2} to provide the context\")\n",
        "question_2 = \"Provide 5 MCQ questions on Artificial Intelligence to help me with practice.\"\n",
        "question_3 = \"Here are my answers: 1. A, 2. B, 3. C, 4. D, 5. E. Please check mu answer, provide reasons for my wrong answers and provide the correct answers with explanations.\"\n",
        "question_4 = \"Provide another 5 MCQ questions on Artificial Intelligence to help me with practice.\"\n",
        "question_5 = \"Here are my answers: 1. A, 2. B, 3. C, 4. D, 5. E. Please check mu answer, provide reasons for my wrong answers and provide the correct answers with explanations.\"\n",
        "question_6 = \"Provide a study quide to help me learn for my wrong answers for the MCQ questions.\"\n",
        "question_7 = \"Based on your reference databases only, provide a study quide on Deep Learning.\"\n",
        "question_8 = \"Based on your memory, provide a summary of our conversation.\"\n",
        "question_9 = \"Summarise the conversation so far.\"\n",
        "\n",
        "\n",
        "# Grab the current user_id and thread_id from config\n",
        "user_id = \"user_1\"\n",
        "\n",
        "# Get last thread_id\n",
        "last_thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "thread_id = str(int(last_thread_id) + 1)\n",
        "\n",
        "# Print Config\n",
        "print(f\"Conversation Thread ID: {last_thread_id} -> {thread_id}\")\n",
        "\n",
        "# Update the config with the new thread_id\n",
        "config = {\"configurable\": {\"thread_id\": thread_id, \"user_id\": user_id}}\n",
        "\n",
        "# Create a system prompt (your overall instructions to the AI)\n",
        "system_prompt = (f\"\"\"\n",
        "You are a helpful AI Tutor assistant.\n",
        "\"\"\")\n",
        "\n",
        "#The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [\n",
        "        #{\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question_1}]},\n",
        "    config, # Pass the thread-level persistence to the graph\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    response = event[\"messages\"][-1]\n",
        "    response.pretty_print()\n",
        "\n",
        "update_memory(event, config, store=in_memory_store) # Update Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rZtd4vjRJYG"
      },
      "outputs": [],
      "source": [
        "# Function for user to input their query and system prompt to the agent\n",
        "def submit_query(user_query, temperature, config: RunnableConfig):\n",
        "    # 'user_1' from the config in utils.\n",
        "    USER_ID = config[\"configurable\"][\"user_id\"]\n",
        "    # Get last thread_id\n",
        "    last_thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "    thread_id = str(int(last_thread_id) + 1)\n",
        "\n",
        "    # Update the config with the new thread_id\n",
        "    config = {\"configurable\": {\"thread_id\": thread_id, \"user_id\": USER_ID}}\n",
        "\n",
        "    # 1) Call query_agent as before to get the streaming generator.\n",
        "    events = graph.stream(\n",
        "        {\n",
        "            \"messages\": [\n",
        "                #{\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_query}\n",
        "            ]\n",
        "        },\n",
        "        config,  # user_id, thread_id, etc.\n",
        "        stream_mode=\"values\",\n",
        "    )\n",
        "\n",
        "    # 2) We'll capture each message's pretty_print() into a list for the console.\n",
        "    from io import StringIO\n",
        "    import sys\n",
        "\n",
        "    message_stream = []\n",
        "    final_response = \"\"\n",
        "    for event in events:\n",
        "        # Each event is a dict with \"messages\" as a list of BaseMessage objects.\n",
        "        for msg in event[\"messages\"]:\n",
        "            # If for some reason the msg doesn't support pretty_print, we fallback to .content\n",
        "            captured_output = StringIO()\n",
        "            original_stdout = sys.stdout\n",
        "            try:\n",
        "                sys.stdout = captured_output\n",
        "                # If it's an AI/Tool message with pretty_print(), call it:\n",
        "                if hasattr(msg, \"pretty_print\"):\n",
        "                    msg.pretty_print()\n",
        "                else:\n",
        "                    # Otherwise, just print its content\n",
        "                    print(msg.content)\n",
        "            finally:\n",
        "                sys.stdout = original_stdout\n",
        "\n",
        "            # Store the captured text into our message_stream\n",
        "            pretty_text = captured_output.getvalue()\n",
        "            message_stream.append(pretty_text.strip())\n",
        "\n",
        "        # If this event had at least one message, the last is presumably the LLM's response\n",
        "        if event[\"messages\"]:\n",
        "            final_response = event[\"messages\"][-1].content\n",
        "\n",
        "    update_memory(event, config, store=in_memory_store) # Update Memory\n",
        "\n",
        "    # 3) Return final response for the chatbot's main window\n",
        "    #    and the entire stream for the console output.\n",
        "    return final_response, \"\\n\".join(message_stream), config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzcOcBWOmWiP"
      },
      "source": [
        "#### Accessing and Testing LLM Chat Model Memory Persistence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtldTWDfRJYG",
        "outputId": "af3065b8-8fb5-4297-a1df-240b669fbb85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversation History:\n",
            "[\n",
            "  [\n",
            "    {\n",
            "      \"role\": \"human\",\n",
            "      \"content\": \"Find out about Deep Learning from databases. Using Web Extraction Tool, search from website: https://www.ibm.com/think/topics/artificial-intelligence and https://www.ibm.com/think/topics/machine-learning to provide the context\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"ai\",\n",
            "      \"content\": \"\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"tool\",\n",
            "      \"content\": \"[\\\"like humans, explore its history and the types of AI, and uncover the ways it impacts our lives.     What is machine learning?   Gain insight into how ML leverages data and algorithms, its use cases and associated concerns to empower responsible and innovative use of the technology.    What is deep learning?   Discover how deep learning simulates our brain, helping systems learn to identify and perform complex tasks with increasing accuracy unsupervised.    What is the k-nearest neighbors algorithm?   Learn about the k-nearest neighbors algorithm, one of the popular and simplest classification and regression classifiers used in machine learning today.     What is natural language processing (NLP)?   Dive into how NLP enables machines to understand and respond to text or voice data and learn about various NLP tasks to obtain optimal results.    What are neural networks?   Learn how neural networks allow programs to recognize patterns and solve common problems in artificial\\\", \\\"era of quantum computing with quantum-safe cryptography.     What is a qubit?   Encode data with a qubit, the basic unit of information in quantum computing, which serves as the quantum equivalent of the traditional bit in classical computers.    What is quantum-centric supercomputing?   Leverage quantum-centric supercomputing to combine quantum and traditional high-performance computing (HPC), enabling solutions for complex real-world problems.    What is neuromorphic computing?  Understand how neuromorphic computing, also known as neuromorphic engineering, mimics the way the human brain works.     What is supercomputing?   Explore how supercomputing uses high-performance computing systems to determine or calculate, reducing the overall time to a solution.           Security and identity        View all security topics     What is cybersecurity?   Stay informed about cybersecurity technology, types of threats and best practices to protect your critical systems and sensitive\\\", \\\"Explore how energy management helps businesses proactively monitor, control and optimize energy consumption to converse use and reduce energy costs.     What is electronic data interchange (EDI)?   Learn how electronic data interchange (EDI) is used to exchange business information, saving time and eliminating costly errors caused by manual processing.    What is managed file transfer?   Facilitate secure, reliable and automated exchange of file-based data over the internet with managed file transfer (MFT) technology to meet compliance needs.    What is the TCFD?   Find out how the Task Force on Climate-related Financial Disclosures (TCFD) seeks to keep investors better-informed about companies' climate-related risks.     Explore more ways to learn      Stay ahead with the latest tech news     Weekly insights, research and expert views on AI, security, cloud and more in the Think Newsletter  Sign up now         Videos     Elevate your understanding with our expert-led educational\\\", \\\"down a monolithic application into smaller deployable services, enabling agility and scalability.    What is mobile application development?   Gain an introductory understanding of mobile application development, which involves making software for smartphones, tablets and digital assistants.    What is Java Spring Boot?   Learn how Java Spring Boot simplifies development of web applications and microservices with an open-source framework referred to as the Java Spring Framework.     What is three-tier architecture?   Separate applications into three logical and physical computing tiers, making it easy to maintain and scale each tier as needed without impacting the other tiers.           Network        View all network topics     What is network security?   Use network security solutions to protect network infrastructure, resources and traffic from internal and external security threats and cyberattacks.     What is load balancing?   Understand how to distribute network traffic\\\", \\\"backbone for Kubernetes and other distributed platform.    What is a relational database?   Use a relational database which provides a structured way to organize data in rows and columns that form a table, maintaining data consistency and integrity.           DevOps        View all DevOps topics     What is DevOps?   Learn how DevOps combines and automates software tasks to speed the delivery of high-quality software, and its synergy with site reliability engineering (SRE).    What is DevSecOps?   Automate the integration of security at every phase of the software development lifecycle, with DevSecOps, short for development, security, and operations.     What is software development?   See how software is developed and how it can help your business compete. Discover software development essentials, innovations and technologies.     What is software testing?   Find software errors and verify that an application or system is fit for use, preventing bugs, reducing development costs and\\\"]\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"ai\",\n",
            "      \"content\": \"\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"tool\",\n",
            "      \"content\": \"[\\\"like humans, explore its history and the types of AI, and uncover the ways it impacts our lives.     What is machine learning?   Gain insight into how ML leverages data and algorithms, its use cases and associated concerns to empower responsible and innovative use of the technology.    What is deep learning?   Discover how deep learning simulates our brain, helping systems learn to identify and perform complex tasks with increasing accuracy unsupervised.    What is the k-nearest neighbors algorithm?   Learn about the k-nearest neighbors algorithm, one of the popular and simplest classification and regression classifiers used in machine learning today.     What is natural language processing (NLP)?   Dive into how NLP enables machines to understand and respond to text or voice data and learn about various NLP tasks to obtain optimal results.    What are neural networks?   Learn how neural networks allow programs to recognize patterns and solve common problems in artificial\\\", \\\"news     Weekly insights, research and expert views on AI, security, cloud and more in the Think Newsletter  Sign up now         Videos     Elevate your understanding with our expert-led educational videos and YouTube playlists on the biggest topics and trends in tech. Master the basics, enhance your skill set or acquire real-world strategies for leveraging technology.  Watch the videos         Podcasts     Explore our diverse podcast series, featuring expert discussions on top tech topics, real-world application of our products and a look at our culture of diversity, learning and agility.  Tune in to our podcasts\\\", \\\"era of quantum computing with quantum-safe cryptography.     What is a qubit?   Encode data with a qubit, the basic unit of information in quantum computing, which serves as the quantum equivalent of the traditional bit in classical computers.    What is quantum-centric supercomputing?   Leverage quantum-centric supercomputing to combine quantum and traditional high-performance computing (HPC), enabling solutions for complex real-world problems.    What is neuromorphic computing?  Understand how neuromorphic computing, also known as neuromorphic engineering, mimics the way the human brain works.     What is supercomputing?   Explore how supercomputing uses high-performance computing systems to determine or calculate, reducing the overall time to a solution.           Security and identity        View all security topics     What is cybersecurity?   Stay informed about cybersecurity technology, types of threats and best practices to protect your critical systems and sensitive\\\", \\\"regression method and by knowing which type of logistic regression to use.    What is Monte Carlo simulation?   Learn how to run a Monte Carlo Simulation, a computational algorithm, by using repeated random sampling to estimate the possible outcomes of an uncertain event.    What is exploratory data analysis?   Know about exploratory data analysis to effectively analyze and summarize data sets, helping one to discover patterns and anomalies, and generate a hypothesis.    What is data science?   Leverage data science to unlock business insights from an increasing amount of data, accelerate digital transformation and data-driven decision making.    Asset management View all asset management topics     What is visual inspection?   Master visual inspection of equipments to detect defects both in person and remotely using digital images to maintain quality and safety standards.     What is preventive maintenance?   Implement preventive maintenance by combining regular maintenance tasks\\\", \\\"Explore how energy management helps businesses proactively monitor, control and optimize energy consumption to converse use and reduce energy costs.     What is electronic data interchange (EDI)?   Learn how electronic data interchange (EDI) is used to exchange business information, saving time and eliminating costly errors caused by manual processing.    What is managed file transfer?   Facilitate secure, reliable and automated exchange of file-based data over the internet with managed file transfer (MFT) technology to meet compliance needs.    What is the TCFD?   Find out how the Task Force on Climate-related Financial Disclosures (TCFD) seeks to keep investors better-informed about companies' climate-related risks.     Explore more ways to learn      Stay ahead with the latest tech news     Weekly insights, research and expert views on AI, security, cloud and more in the Think Newsletter  Sign up now         Videos     Elevate your understanding with our expert-led educational\\\"]\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"ai\",\n",
            "      \"content\": \"\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"tool\",\n",
            "      \"content\": \"[\\\"like humans, explore its history and the types of AI, and uncover the ways it impacts our lives.     What is machine learning?   Gain insight into how ML leverages data and algorithms, its use cases and associated concerns to empower responsible and innovative use of the technology.    What is deep learning?   Discover how deep learning simulates our brain, helping systems learn to identify and perform complex tasks with increasing accuracy unsupervised.    What is the k-nearest neighbors algorithm?   Learn about the k-nearest neighbors algorithm, one of the popular and simplest classification and regression classifiers used in machine learning today.     What is natural language processing (NLP)?   Dive into how NLP enables machines to understand and respond to text or voice data and learn about various NLP tasks to obtain optimal results.    What are neural networks?   Learn how neural networks allow programs to recognize patterns and solve common problems in artificial\\\", \\\"Think Topics | IBM             Home Think  Topics      Explainers            Become an expert on emerging and fundamental tech topics            Featured topics  Demystify transformative technologies. Decode tech topics with content crafted by IBM experts.        What is data mining?  Learn how data mining combines statistics and artificial intelligence to analyze large data sets to discover meaningful insights and useful information.        What is a vector database?  Use a vector database for storing, managing and indexing huge quantities of high-dimensional vector data efficiently for generative AI use cases and applications.        What is a chatbot?  Explore chatbot technology to understand how chatbots simulate human conversation, often using NLP to parse inputs and generative AI to automate responses.        What is a DDoS (distributed denial of service) attack?  Find out how a DDoS attack floods websites and other network resources with malicious traffic, disrupting normal\\\", \\\"news     Weekly insights, research and expert views on AI, security, cloud and more in the Think Newsletter  Sign up now         Videos     Elevate your understanding with our expert-led educational videos and YouTube playlists on the biggest topics and trends in tech. Master the basics, enhance your skill set or acquire real-world strategies for leveraging technology.  Watch the videos         Podcasts     Explore our diverse podcast series, featuring expert discussions on top tech topics, real-world application of our products and a look at our culture of diversity, learning and agility.  Tune in to our podcasts\\\", \\\"era of quantum computing with quantum-safe cryptography.     What is a qubit?   Encode data with a qubit, the basic unit of information in quantum computing, which serves as the quantum equivalent of the traditional bit in classical computers.    What is quantum-centric supercomputing?   Leverage quantum-centric supercomputing to combine quantum and traditional high-performance computing (HPC), enabling solutions for complex real-world problems.    What is neuromorphic computing?  Understand how neuromorphic computing, also known as neuromorphic engineering, mimics the way the human brain works.     What is supercomputing?   Explore how supercomputing uses high-performance computing systems to determine or calculate, reducing the overall time to a solution.           Security and identity        View all security topics     What is cybersecurity?   Stay informed about cybersecurity technology, types of threats and best practices to protect your critical systems and sensitive\\\", \\\"in person and remotely using digital images to maintain quality and safety standards.     What is preventive maintenance?   Implement preventive maintenance by combining regular maintenance tasks with ML, data analytics and predictive asset health monitoring to prevent downtime.    What is a CMMS?   Enhance maintenance efforts with a computerized maintenance management system (CMMS) for efficient scheduling, managing, and tracking of maintenance tasks.    What is a digital twin?   Explore how digital twins create a virtual copy of an object or system by integrating real-time data, simulation, ML and reasoning to support decision-making.    What is field service management?   Learn field service management to effectively coordinate resources, employees and equipment, in work activities and operations off company property.           Business operations        View all business automation topics     What is business automation?   Explore what business automation is, why it matters and\\\"]\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"ai\",\n",
            "      \"content\": \"Deep Learning is a subset of machine learning that utilizes neural networks with multiple layers to learn complex patterns in data. These networks, inspired by the human brain, consist of an input layer, several hidden layers, and an output layer, allowing them to handle unstructured data like images, speech, and text more effectively than traditional machine learning methods.\\n\\n**Key Points:**\\n\\n1. **Structure and Function:**\\n   - Deep Learning uses deep neural networks with many layers, enabling the learning of intricate data patterns.\\n   - Techniques include convolutional neural networks (CNNs) for image recognition and recurrent neural networks (RNNs) for sequence data.\\n\\n2. **Applications:**\\n   - Used in self-driving cars for object recognition, virtual assistants for speech understanding, and recommendation systems for personalized suggestions.\\n\\n3. **Challenges:**\\n   - Requires large datasets and significant computational resources for training.\\n   - Issues include data bias, resource intensity, and the difficulty of interpreting model decisions.\\n\\n4. **Frameworks and Tools:**\\n   - Utilizes frameworks like TensorFlow and PyTorch for building and training models.\\n\\n5. **Future Directions:**\\n   - Potential advancements with quantum computing and edge computing to enhance efficiency and accessibility.\\n\\n6. **Ethical Considerations:**\\n   - Concerns about privacy, especially in facial recognition, and the need for careful data curation to avoid bias.\\n\\nIn summary, Deep Learning is a powerful tool with diverse applications, offering capabilities beyond traditional machine learning but requiring careful consideration of its challenges and ethical implications.\"\n",
            "    }\n",
            "  ]\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Define function to download the entire conversation history from the memory store and save it to a file\n",
        "def download_conversation_history(user_id: str, store: BaseStore = in_memory_store) -> str:\n",
        "    \"\"\"\n",
        "    Download the entire conversation history from memory store and save it to a JSON file.\n",
        "    \"\"\"\n",
        "    memories = store.search((user_id, \"memories\"))\n",
        "    conversation_history = [m.value.get(\"memory\") for m in memories if \"memory\" in m.value]\n",
        "    filename = f\"{user_id}_conversation_history.json\"\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(conversation_history, f, indent=2)\n",
        "    return f\"Conversation history saved to {filename}\"\n",
        "\n",
        "# Download the conversation history\n",
        "download_conversation_history(user_id, in_memory_store)\n",
        "\n",
        "# Print the conversation history\n",
        "with open(f\"{user_id}_conversation_history.json\", \"r\") as f:\n",
        "    conversation_history = json.load(f)\n",
        "    print(\"Conversation History:\")\n",
        "    print(json.dumps(conversation_history, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xgiL2CJRJYJ"
      },
      "source": [
        "## App Development (Huggingface)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdbmDpC7k6vp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "from huggingface_hub import login\n",
        "login(token=HF_TOKEN)\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# HF_TOKEN = os.getenv(\"HF_TOKEN\")  # Read from environment variable\n",
        "# if HF_TOKEN:\n",
        "#     login(token=HF_TOKEN)  # Log in to Hugging Face Hub\n",
        "# else:\n",
        "#     print(\"Warning: HF_TOKEN not found in environment variables.\")\n",
        "\n",
        "# GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")  # Read from environment variable\n",
        "\n",
        "\n",
        "# Download required files from Github repo\n",
        "!wget -q https://github.com/sumkh/NYP_Dataset/raw/refs/heads/main/Documents.zip\n",
        "!unzip -o -qq /content/Documents.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTL9JhvhRJYJ"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "AqPpud8wRJYJ",
        "outputId": "80e05ddb-77a3-4fe7-cb5b-af60501d9f2e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from io import StringIO\n",
        "import sys\n",
        "\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "import gradio as gr\n",
        "import json\n",
        "import csv\n",
        "import hashlib\n",
        "import uuid\n",
        "import logging\n",
        "from typing import Annotated, List, Dict, Sequence, TypedDict\n",
        "\n",
        "# LangChain & related imports\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langchain_core.tools import tool, StructuredTool\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "# Extraction for Documents\n",
        "from langchain_docling.loader import ExportType\n",
        "from langchain_docling import DoclingLoader\n",
        "from docling.chunking import HybridChunker\n",
        "\n",
        "# Extraction for HTML\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import InjectedStore\n",
        "from langgraph.store.base import BaseStore\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain.embeddings import init_embeddings\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langchain_core.messages import (\n",
        "    SystemMessage,\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    BaseMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress all library logs at or below WARNING for user experience:\n",
        "logging.disable(logging.WARNING)\n",
        "\n",
        "\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# =============================================================================\n",
        "#                         Document Extraction Functions\n",
        "# =============================================================================\n",
        "\n",
        "def extract_documents(doc_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Recursively collects all file paths from folder 'doc_path'.\n",
        "    Used by ExtractDocument.load_files() to find documents to parse.\n",
        "    \"\"\"\n",
        "    extracted_docs = []\n",
        "\n",
        "    for root, _, files in os.walk(doc_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            extracted_docs.append(file_path)\n",
        "    return extracted_docs\n",
        "\n",
        "\n",
        "def _generate_uuid(page_content: str) -> str:\n",
        "    \"\"\"Generate a UUID for a chunk of text using MD5 hashing.\"\"\"\n",
        "    md5_hash = hashlib.md5(page_content.encode()).hexdigest()\n",
        "    return str(uuid.UUID(md5_hash[0:32]))\n",
        "\n",
        "\n",
        "def load_file(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load a file from the given path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "\n",
        "    # Load the file and extract the text chunks\n",
        "    try:\n",
        "        loader = DoclingLoader(\n",
        "            file_path = file_path,\n",
        "            export_type = ExportType.DOC_CHUNKS,\n",
        "            chunker = HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
        "        )\n",
        "        docs = loader.load()\n",
        "        logger.info(f\"Total parsed doc-chunks: {len(docs)} from Source: {file_path}\")\n",
        "\n",
        "        for d in docs:\n",
        "            # Tag each document's chunk with the source file and a unique ID\n",
        "            doc = Document(\n",
        "                page_content=d.page_content,\n",
        "                metadata={\n",
        "                    \"source\": file_path,\n",
        "                    \"doc_id\": _generate_uuid(d.page_content),\n",
        "                    \"source_type\": \"file\",\n",
        "                }\n",
        "            )\n",
        "            _documents.append(doc)\n",
        "        logger.info(f\"Total generated LangChain document chunks: {len(_documents)}\\n.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {file_path}. Exception: {e}\\n.\")\n",
        "\n",
        "    return _documents\n",
        "\n",
        "\n",
        "# Define function to load documents from a folder\n",
        "def load_files_from_folder(doc_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load documents from the given folder path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "    # Extract all files path from the given folder\n",
        "    extracted_docs = extract_documents(doc_path)\n",
        "\n",
        "    # Iterate through each document and extract the text chunks\n",
        "    for file_path in extracted_docs:\n",
        "        _documents.extend(load_file(file_path))\n",
        "\n",
        "    return _documents\n",
        "\n",
        "# =============================================================================\n",
        "# Load structured data in csv file to LangChain Document format\n",
        "def load_mcq_csvfiles(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load structured data in mcq csv file from the given file path and return a list of Document object.\n",
        "    Expected format: each row of csv is comma separated into \"mcq_number\", \"mcq_type\", \"text_content\"\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "\n",
        "    # iterate through each csv file and load each row into _dict_per_question format\n",
        "    # Ensure we process only CSV files\n",
        "    if not file_path.endswith(\".csv\"):\n",
        "        return _documents  # Skip non-CSV files\n",
        "    try:\n",
        "        # Open and read the CSV file\n",
        "        with open(file_path, mode='r', encoding='utf-8') as file:\n",
        "            reader = csv.DictReader(file)\n",
        "            for row in reader:\n",
        "                # Ensure required columns exist in the row\n",
        "                if not all(k in row for k in [\"mcq_number\", \"mcq_type\", \"text_content\"]): # Ensure required columns exist and exclude header\n",
        "                    logger.error(f\"Skipping row due to missing fields: {row}\")\n",
        "                    continue\n",
        "                # Tag each row of csv is comma separated into \"mcq_number\", \"mcq_type\", \"text_content\"\n",
        "                doc = Document(\n",
        "                    page_content = row[\"text_content\"], # text_content segment is separated by \"|\"\n",
        "                    metadata={\n",
        "                        \"source\": f\"{file_path}_{row['mcq_number']}\",  # file_path + mcq_number\n",
        "                        \"doc_id\": _generate_uuid(f\"{file_path}_{row['mcq_number']}\"),  # Unique ID\n",
        "                        \"source_type\": row[\"mcq_type\"],  # MCQ type\n",
        "                    }\n",
        "                )\n",
        "                _documents.append(doc)\n",
        "            logger.info(f\"Successfully loaded {len(_documents)} LangChain document chunks from {file_path}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {file_path}. Exception: {e}\\n.\")\n",
        "\n",
        "    return _documents\n",
        "\n",
        "# Define function to load documents from a folder for structured data in csv file\n",
        "def load_files_from_folder_mcq(doc_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load mcq csv file from the given folder path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "    # Extract all files path from the given folder\n",
        "    extracted_docs = [\n",
        "        os.path.join(doc_path, file) for file in os.listdir(doc_path)\n",
        "        if file.endswith(\".csv\")  # Process only CSV files\n",
        "    ]\n",
        "\n",
        "    # Iterate through each document and extract the text chunks\n",
        "    for file_path in extracted_docs:\n",
        "        _documents.extend(load_mcq_csvfiles(file_path))\n",
        "\n",
        "    return _documents\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                         Website Extraction Functions\n",
        "# =============================================================================\n",
        "def _generate_uuid(page_content: str) -> str:\n",
        "    \"\"\"Generate a UUID for a chunk of text using MD5 hashing.\"\"\"\n",
        "    md5_hash = hashlib.md5(page_content.encode()).hexdigest()\n",
        "    return str(uuid.UUID(md5_hash[0:32]))\n",
        "\n",
        "def ensure_scheme(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    if not parsed_url.scheme:\n",
        "        return 'http://' + url  # Default to http, or use 'https://' if preferred\n",
        "    return url\n",
        "\n",
        "def extract_html(url: List[str]) -> List[Document]:\n",
        "    if isinstance(url, str):\n",
        "        url = [url]\n",
        "    \"\"\"\n",
        "    Extracts text from the HTML content of web pages listed in 'web_path'.\n",
        "    Returns a list of LangChain 'Document' objects.\n",
        "    \"\"\"\n",
        "    # Ensure all URLs have a scheme\n",
        "    web_paths = [ensure_scheme(u) for u in url]\n",
        "\n",
        "    loader = WebBaseLoader(web_paths)\n",
        "    loader.requests_per_second = 1\n",
        "    docs = loader.load()\n",
        "\n",
        "    # Iterate through each document, clean the content, removing excessive line return and store it in a LangChain Document\n",
        "    _documents = []\n",
        "    for doc in docs:\n",
        "        # Clean the concent\n",
        "        doc.page_content = doc.page_content.strip()\n",
        "        doc.page_content = doc.page_content.replace(\"\\n\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"\\r\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"\\t\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"  \", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"   \", \" \")\n",
        "\n",
        "        # Store it in a LangChain Document\n",
        "        web_doc = Document(\n",
        "            page_content=doc.page_content,\n",
        "            metadata={\n",
        "                \"source\": doc.metadata.get(\"source\"),\n",
        "                \"doc_id\": _generate_uuid(doc.page_content),\n",
        "                \"source_type\": \"web\"\n",
        "            }\n",
        "        )\n",
        "        _documents.append(web_doc)\n",
        "    return _documents\n",
        "\n",
        "# =============================================================================\n",
        "#                         Vector Store Initialisation\n",
        "# =============================================================================\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
        "\n",
        "# Initialise vector stores\n",
        "general_vs = Chroma(\n",
        "    collection_name=\"general_vstore\",\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"./general_db\"\n",
        ")\n",
        "\n",
        "mcq_vs = Chroma(\n",
        "    collection_name=\"mcq_vstore\",\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"./mcq_db\"\n",
        ")\n",
        "\n",
        "in_memory_vs = Chroma(\n",
        "    collection_name=\"in_memory_vstore\",\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "# Split the documents into smaller chunks for better embedding coverage\n",
        "def split_text_into_chunks(docs: List[Document]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Splits a list of Documents into smaller text chunks using\n",
        "    RecursiveCharacterTextSplitter while preserving metadata.\n",
        "    Returns a list of Document objects.\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000, # Split into chunks of 1000 characters\n",
        "        chunk_overlap=200, # Overlap by 200 characters\n",
        "        add_start_index=True\n",
        "    )\n",
        "    chunked_docs = splitter.split_documents(docs)\n",
        "    return chunked_docs # List of Document objects\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                         Retrieval Tools\n",
        "# =============================================================================\n",
        "\n",
        "# Define a simple similarity search retrieval tool on msq_vs\n",
        "class MCQRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"Search topic.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def mcq_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Retrieve the top k most similar mcq question documents from the vector store\n",
        "    docs_func = mcq_vs.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\n",
        "        'k': k,\n",
        "        'filter':{\"source_type\": \"mcq_question\"}\n",
        "    },\n",
        "    )\n",
        "    docs_qns = docs_func.invoke(input, k=k)\n",
        "\n",
        "    # Extract the document IDs from the retrieved documents\n",
        "    doc_ids = [d.metadata.get(\"doc_id\") for d in docs_qns if \"doc_id\" in d.metadata]\n",
        "\n",
        "    # Retrieve full documents based on the doc_ids\n",
        "    docs = mcq_vs.get(where = {'doc_id': {\"$in\":doc_ids}})\n",
        "\n",
        "    qns_list = {}\n",
        "    for i, d in enumerate(docs['metadatas']):\n",
        "        qns_list[d['source'] + \" \" + d['source_type']] = docs['documents'][i]\n",
        "\n",
        "    return qns_list\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "mcq_retriever_tool = StructuredTool.from_function(\n",
        "    func = mcq_retriever,\n",
        "    name = \"MCQ Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve MCQ questions set when Human asks to generate a quiz related to a topic.\n",
        "    DO NOT GIVE THE ANSWERS to Human before Human has answered all the questions.\n",
        "\n",
        "    If Human give answers for questions you do not know, SAY you do not have the questions for the answer\n",
        "    and ASK if the Human want you to generate a new quiz and then SAVE THE QUIZ with Summary Tool before ending the conversation.\n",
        "\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The search topic to retrieve MCQ questions set related to the topic.\n",
        "        - k (int): Number of question set to retrieve.\n",
        "        Example usage: input='What is AI?', k=5\n",
        "\n",
        "    Returns:\n",
        "    - A dict of MCQ questions:\n",
        "    Key: 'metadata of question' e.g. './Documents/mcq/mcq.csv_Qn31 mcq_question' with suffix ['question', 'answer', 'answer_reason', 'options', 'wrong_options_reason']\n",
        "    Value: Text Content\n",
        "\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = MCQRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Return the response as a list of strings\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Retrieve more documents with higher diversity using MMR (Maximal Marginal Relevance) from the general vector store\n",
        "# Useful if the dataset has many similar documents\n",
        "class GenRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def gen_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = general_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "general_retriever_tool = StructuredTool.from_function(\n",
        "    func = gen_retriever,\n",
        "    name = \"Assistant References Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve reference information from Assistant reference database for Human queries related to a topic or\n",
        "    and when Human asked to generate guides to learn or study about a topic.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "        Example usage: input='What is AI?', k=5\n",
        "    Returns:\n",
        "    - A list of retrieved document's content string.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = GenRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Return the content of the documents\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Retrieve more documents with higher diversity using MMR (Maximal Marginal Relevance) from the in-memory vector store\n",
        "# Query in-memory vector store only\n",
        "class InMemoryRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def in_memory_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "in_memory_retriever_tool = StructuredTool.from_function(\n",
        "    func = in_memory_retriever,\n",
        "    name = \"In-Memory Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool when Human ask Assistant to retrieve information from documents that Human has uploaded.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = InMemoryRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Whether to return the tool’s output directly\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Web Extraction Tool\n",
        "class WebExtractionRequest(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"Search text.\")\n",
        "    url: str = Field(\n",
        "        ...,\n",
        "        title=\"url\",\n",
        "        description=\"Web URL(s) to extract content from. If multiple URLs, separate them with a comma.\"\n",
        "    )\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "# Extract content from a web URL, load into in_memory_vstore\n",
        "def extract_web_path_tool(input: str, url: str, k: int = 5) -> List[str]:\n",
        "    if isinstance(url, str):\n",
        "        url = [url]\n",
        "    \"\"\"\n",
        "    Extract content from the web URLs based on user's input.\n",
        "    Args:\n",
        "    - input: The input text to search for.\n",
        "    - url: URLs to extract content from.\n",
        "    - k: Number of results to retrieve.\n",
        "    Returns:\n",
        "     - A list of retrieved document's content string.\n",
        "    \"\"\"\n",
        "    # Extract content from the web\n",
        "    html_docs = extract_html(url)\n",
        "    if not html_docs:\n",
        "        return f\"No content extracted from {url}.\"\n",
        "\n",
        "    # Split the documents into smaller chunks for better embedding coverage\n",
        "    chunked_texts = split_text_into_chunks(html_docs)\n",
        "    in_memory_vs.add_documents(chunked_texts) # Add the chunked texts to the in-memory vector store\n",
        "\n",
        "    # Extract content from the in-memory vector store\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\n",
        "        'k': k,\n",
        "        'lambda_mult': 0.25,\n",
        "        'filter':{\"source\": {\"$in\": url}}\n",
        "    },\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "web_extraction_tool = StructuredTool.from_function(\n",
        "    func = extract_web_path_tool,\n",
        "    name = \"Web Extraction Tool\",\n",
        "    description = (\n",
        "        \"Assistant should use this tool to extract content from web URLs based on user's input, \"\n",
        "        \"Web extraction is initially load into database and then return k: Number of results to retrieve\"\n",
        "    ),\n",
        "    args_schema = WebExtractionRequest,\n",
        "    return_direct = False, # Whether to return the tool’s output directly\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Ensemble Retrieval from General and In-Memory Vector Stores\n",
        "class EnsembleRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"Number of results.\")\n",
        "\n",
        "def ensemble_retriever(input: str, k: int = 5) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    general_retrieval = general_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    in_memory_retrieval = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "        retrievers=[general_retrieval, in_memory_retrieval],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "    docs = ensemble_retriever.invoke(input)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "ensemble_retriever_tool = StructuredTool.from_function(\n",
        "    func = ensemble_retriever,\n",
        "    name = \"Ensemble Retriever Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve information from reference database and\n",
        "    extraction of documents that Human has uploaded.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = EnsembleRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False\n",
        "    )\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# LLM Model Setup\n",
        "###############################################################################\n",
        "\n",
        "TEMPERATURE = 0.5\n",
        "# model = ChatOpenAI(\n",
        "#     model=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "#     temperature=TEMPERATURE,\n",
        "#     timeout=None,\n",
        "#     max_retries=2,\n",
        "#     api_key=\"not_required\",\n",
        "#     base_url=\"http://localhost:8000/v1\",\n",
        "# )\n",
        "\n",
        "model = ChatGroq(\n",
        "    model_name=\"deepseek-r1-distill-llama-70b\",\n",
        "    temperature=TEMPERATURE,\n",
        "    api_key=GROQ_API_KEY,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "###############################################################################\n",
        "# 1. Initialize memory + config\n",
        "###############################################################################\n",
        "in_memory_store = InMemoryStore(\n",
        "    index={\n",
        "        \"embed\": init_embeddings(\"huggingface:sentence-transformers/all-MiniLM-L6-v2\"),\n",
        "        \"dims\": 384,  # Embedding dimensions\n",
        "    }\n",
        ")\n",
        "\n",
        "# A memory saver to checkpoint conversation states\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "# Initialize config with user & thread info\n",
        "config = {}\n",
        "config[\"configurable\"] = {\n",
        "    \"user_id\": \"user_1\",\n",
        "    \"thread_id\": 0,\n",
        "}\n",
        "\n",
        "###############################################################################\n",
        "# 2. Define MessagesState\n",
        "###############################################################################\n",
        "class MessagesState(TypedDict):\n",
        "    \"\"\"The state of the agent.\n",
        "\n",
        "    The key 'messages' uses add_messages as a reducer,\n",
        "    so each time this state is updated, new messages are appended.\n",
        "    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 3. Memory Tools\n",
        "###############################################################################\n",
        "def save_memory(summary_text: str, *, config: RunnableConfig, store: BaseStore) -> str:\n",
        "    \"\"\"Save the given memory for the current user and return the key.\"\"\"\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    thread_id = config.get(\"configurable\", {}).get(\"thread_id\")\n",
        "    namespace = (user_id, \"memories\")\n",
        "    memory_id = thread_id\n",
        "    store.put(namespace, memory_id, {\"memory\": summary_text})\n",
        "    return f\"Saved to memory key: {memory_id}\"\n",
        "\n",
        "def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
        "    # Extract the messages list from the event, handling potential missing key\n",
        "    messages = state[\"messages\"]\n",
        "    # Convert LangChain messages to dictionaries before storing\n",
        "    messages_dict = [{\"role\": msg.type, \"content\": msg.content} for msg in messages]\n",
        "\n",
        "    # Get the user id from the config\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    thread_id = config.get(\"configurable\", {}).get(\"thread_id\")\n",
        "    # Namespace the memory\n",
        "    namespace = (user_id, \"memories\")\n",
        "    # Create a new memory ID\n",
        "    memory_id = f\"{thread_id}\"\n",
        "    store.put(namespace, memory_id, {\"memory\": messages_dict})\n",
        "    return f\"Saved to memory key: {memory_id}\"\n",
        "\n",
        "\n",
        "# Define a Pydantic schema for the save_memory tool (if needed elsewhere)\n",
        "# https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.InMemoryStore.html\n",
        "class RecallMemory(BaseModel):\n",
        "    query_text: str = Field(..., title=\"Search Text\", description=\"The text to search from memories for similar records.\")\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"Number of results to retrieve.\")\n",
        "\n",
        "def recall_memory(query_text: str, k: int = 5) -> str:\n",
        "    \"\"\"Retrieve user memories from in_memory_store.\"\"\"\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    memories = [\n",
        "        m.value[\"memory\"] for m in in_memory_store.search((user_id, \"memories\"), query=query_text, limit=k)\n",
        "        if \"memory\" in m.value\n",
        "    ]\n",
        "    return f\"User memories: {memories}\"\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "recall_memory_tool = StructuredTool.from_function(\n",
        "    func=recall_memory,\n",
        "    name=\"Recall Memory Tool\",\n",
        "    description=\"\"\"\n",
        "      Retrieve memories relevant to the user's query.\n",
        "      \"\"\",\n",
        "    args_schema=RecallMemory,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "###############################################################################\n",
        "# 4. Summarize Node (using StructuredTool)\n",
        "###############################################################################\n",
        "# Define a Pydantic schema for the Summary tool\n",
        "class SummariseConversation(BaseModel):\n",
        "    summary_text: str = Field(..., title=\"text\", description=\"Write a summary of entire conversation here\")\n",
        "\n",
        "def summarise_node(summary_text: str):\n",
        "    \"\"\"\n",
        "    Final node that summarizes the entire conversation for the current thread,\n",
        "    saves it in memory, increments the thread_id, and ends the conversation.\n",
        "    Returns a confirmation string.\n",
        "    \"\"\"\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "    current_thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "    new_thread_id = str(int(current_thread_id) + 1)\n",
        "\n",
        "    # Prepare configuration for saving memory with updated thread id\n",
        "    config_for_saving = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": user_id,\n",
        "            \"thread_id\": new_thread_id\n",
        "        }\n",
        "    }\n",
        "    key = save_memory(summary_text, config=config_for_saving, store=in_memory_store)\n",
        "    #return f\"Summary saved under key: {key}\"\n",
        "\n",
        "# Create a StructuredTool from the function (this wraps summarise_node)\n",
        "summarise_tool = StructuredTool.from_function(\n",
        "    func=summarise_node,\n",
        "    name=\"Summary Tool\",\n",
        "    description=\"\"\"\n",
        "      Summarize the current conversation in no more than\n",
        "      1000 words. Also retain any unanswered quiz questions along with\n",
        "      your internal answers so the next conversation thread can continue.\n",
        "      Do not reveal solutions to the user yet. Use this tool to save\n",
        "      the current conversation to memory and then end the conversation.\n",
        "      \"\"\",\n",
        "    args_schema=SummariseConversation,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "def call_summary(state: MessagesState, config: RunnableConfig):\n",
        "    # Convert message dicts to HumanMessage instances if needed.\n",
        "    system_message=\"\"\"\n",
        "      Summarize the current conversation in no more than\n",
        "      1000 words. Also retain any unanswered quiz questions along with\n",
        "      your internal answers.\n",
        "      \"\"\"\n",
        "    messages = []\n",
        "    for m in state[\"messages\"]:\n",
        "        if isinstance(m, dict):\n",
        "            # Use role from dict (defaulting to 'user' if missing)\n",
        "            messages.append(AIMessage(content=system_message, role=m.get(\"role\", \"assistant\")))\n",
        "        else:\n",
        "            messages.append(m)\n",
        "\n",
        "    summaries = llm_with_tools.invoke(messages)\n",
        "\n",
        "    summary_content = summaries.content\n",
        "\n",
        "    # Call Tool Manually\n",
        "    message_with_single_tool_call = AIMessage(\n",
        "        content=\"\",\n",
        "        tool_calls=[\n",
        "            {\n",
        "                \"name\": \"Summary Tool\",\n",
        "                \"args\": {\"summary_text\": summary_content},\n",
        "                \"id\": \"tool_call_id\",\n",
        "                \"type\": \"tool_call\",\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    tool_node.invoke({\"messages\": [message_with_single_tool_call]})\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 5. Build the Graph\n",
        "###############################################################################\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "\n",
        "# Use the built-in ToolNode from langgraph that calls any declared tools.\n",
        "tools = [\n",
        "    mcq_retriever_tool,\n",
        "    web_extraction_tool,\n",
        "    ensemble_retriever_tool,\n",
        "    general_retriever_tool,\n",
        "    in_memory_retriever_tool,\n",
        "    recall_memory_tool,\n",
        "    summarise_tool,\n",
        "]\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "#end_node = ToolNode(tools=[summarise_tool])\n",
        "\n",
        "# Wrap your model with tools\n",
        "llm_with_tools = model.bind_tools(tools)\n",
        "\n",
        "###############################################################################\n",
        "# 6. The agent's main node: call_model\n",
        "###############################################################################\n",
        "def call_model(state: MessagesState, config: RunnableConfig):\n",
        "    \"\"\"\n",
        "    The main agent node that calls the LLM with the user + system messages.\n",
        "    Since our vLLM chat wrapper expects a list of BaseMessage objects,\n",
        "    we convert any dict messages to HumanMessage objects.\n",
        "    If the LLM requests a tool call, we'll route to the 'tools' node next\n",
        "    (depending on the condition).\n",
        "    \"\"\"\n",
        "    # Convert message dicts to HumanMessage instances if needed.\n",
        "    messages = []\n",
        "    for m in state[\"messages\"]:\n",
        "        if isinstance(m, dict):\n",
        "            # Use role from dict (defaulting to 'user' if missing)\n",
        "            messages.append(HumanMessage(content=m.get(\"content\", \"\"), role=m.get(\"role\", \"user\")))\n",
        "        else:\n",
        "            messages.append(m)\n",
        "\n",
        "    # Invoke the LLM (with tools) using the converted messages.\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "\n",
        "def call_summary(state: MessagesState, config: RunnableConfig):\n",
        "    # Convert message dicts to HumanMessage instances if needed.\n",
        "    system_message=\"\"\"\n",
        "      Summarize the current conversation in no more than\n",
        "      1000 words. Also retain any unanswered quiz questions along with\n",
        "      your internal answers.\n",
        "      \"\"\"\n",
        "    messages = []\n",
        "    for m in state[\"messages\"]:\n",
        "        if isinstance(m, dict):\n",
        "            # Use role from dict (defaulting to 'user' if missing)\n",
        "            messages.append(AIMessage(content=system_message, role=m.get(\"role\", \"assistant\")))\n",
        "        else:\n",
        "            messages.append(m)\n",
        "\n",
        "    summaries = llm_with_tools.invoke(messages)\n",
        "\n",
        "    summary_content = summaries.content\n",
        "\n",
        "    # Call Tool Manually\n",
        "    message_with_single_tool_call = AIMessage(\n",
        "        content=\"\",\n",
        "        tool_calls=[\n",
        "            {\n",
        "                \"name\": \"Summary Tool\",\n",
        "                \"args\": {\"summary_text\": summary_content},\n",
        "                \"id\": \"tool_call_id\",\n",
        "                \"type\": \"tool_call\",\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    tool_node.invoke({\"messages\": [message_with_single_tool_call]})\n",
        "\n",
        "###############################################################################\n",
        "# 7. Add Nodes & Edges, Then Compile\n",
        "###############################################################################\n",
        "graph_builder.add_node(\"agent\", call_model)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "#graph_builder.add_node(\"summary\", call_summary)\n",
        "\n",
        "# Entry point\n",
        "graph_builder.set_entry_point(\"agent\")\n",
        "\n",
        "# def custom_tools_condition(llm_output: dict) -> str:\n",
        "#     \"\"\"Return which node to go to next based on the LLM output.\"\"\"\n",
        "\n",
        "#     # The LLM's JSON might have a field like {\"name\": \"Recall Memory Tool\", \"arguments\": {...}}.\n",
        "#     tool_name = llm_output.get(\"name\", None)\n",
        "\n",
        "#     # If the LLM calls \"Summary Tool\", jump directly to the 'summary' node\n",
        "#     if tool_name == \"Summary Tool\":\n",
        "#         return \"summary\"\n",
        "\n",
        "#     # If the LLM calls any other recognized tool, go to 'tools'\n",
        "#     valid_tool_names = [t.name for t in tools]  # all tools in the main tool_node\n",
        "#     if tool_name in valid_tool_names:\n",
        "#         return \"tools\"\n",
        "\n",
        "#     # If there's no recognized tool name, assume we're done => go to summary\n",
        "#     return \"__end__\"\n",
        "\n",
        "# graph_builder.add_conditional_edges(\n",
        "#     \"agent\",\n",
        "#     custom_tools_condition,\n",
        "#     {\n",
        "#         \"tools\": \"tools\",\n",
        "#         \"summary\": \"summary\",\n",
        "#         \"__end__\": \"summary\",\n",
        "#     }\n",
        "# )\n",
        "\n",
        "# If LLM requests a tool, go to \"tools\", otherwise go to \"summary\"\n",
        "graph_builder.add_conditional_edges(\"agent\", tools_condition)\n",
        "#graph_builder.add_conditional_edges(\"agent\", tools_condition, {\"tools\": \"tools\", \"__end__\": \"summary\"})\n",
        "#graph_builder.add_conditional_edges(\"agent\", lambda llm_output: \"tools\" if llm_output.get(\"name\", None) in [t.name for t in tools] else \"summary\", {\"tools\": \"tools\", \"__end__\": \"summary\"}\n",
        "\n",
        "# If we used a tool, return to the agent for final answer or more tools\n",
        "graph_builder.add_edge(\"tools\", \"agent\")\n",
        "#graph_builder.add_edge(\"agent\", \"summary\")\n",
        "#graph_builder.set_finish_point(\"summary\")\n",
        "\n",
        "# Compile the graph with checkpointing and persistent store\n",
        "graph = graph_builder.compile(checkpointer=checkpointer, store=in_memory_store)\n",
        "\n",
        "#from langgraph.prebuilt import create_react_agent\n",
        "#graph = create_react_agent(llm_with_tools, tools=tool_node, checkpointer=checkpointer, store=in_memory_store)\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWELX-uoRJYJ"
      },
      "source": [
        "### Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaFl4Ke2RJYJ"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from gradio import ChatMessage\n",
        "\n",
        "system_prompt = \"You are a helpful Assistant. Always use the tools {tools}.\"\n",
        "\n",
        "########################################\n",
        "# Upload_documents\n",
        "########################################\n",
        "\n",
        "def upload_documents(file_list: List):\n",
        "    \"\"\"\n",
        "    Load documents into in-memory vector store.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "\n",
        "    for doc_path in file_list:\n",
        "        _documents.extend(load_file(doc_path))\n",
        "\n",
        "    # Split the documents into smaller chunks for better embedding coverage\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=300, # Split into chunks of 512 characters\n",
        "        chunk_overlap=50, # Overlap by 50 characters\n",
        "        add_start_index=True\n",
        "    )\n",
        "    chunked_texts = splitter.split_documents(_documents)\n",
        "    in_memory_vs.add_documents(chunked_texts)\n",
        "    return f\"Uploaded {len(file_list)} documents into in-memory vector store.\"\n",
        "\n",
        "\n",
        "########################################\n",
        "# Submit_queries (ChatInterface Function)\n",
        "########################################\n",
        "def submit_queries(message, _messages):\n",
        "    \"\"\"\n",
        "    - message: dict with {\"text\": ..., \"files\": [...]}\n",
        "    - history: list of ChatMessage\n",
        "    \"\"\"\n",
        "    _messages=[]\n",
        "    user_text = message.get(\"text\", \"\")\n",
        "    user_files = message.get(\"files\", [])\n",
        "\n",
        "    # Process user-uploaded files\n",
        "    if user_files:\n",
        "      for file_obj in user_files:\n",
        "          _messages.append(ChatMessage(role=\"user\", content=f\"Uploaded file: {file_obj}\"))\n",
        "      upload_response = upload_documents(user_files)\n",
        "      _messages.append(ChatMessage(role=\"assistant\", content=upload_response))\n",
        "      yield _messages\n",
        "      return # Exit early since we don't need to process text or call the LLM\n",
        "\n",
        "    # Append user text if present\n",
        "    if user_text:\n",
        "        events = graph.stream(\n",
        "      {\n",
        "          \"messages\": [\n",
        "              {\"role\": \"system\", \"content\": system_prompt},\n",
        "              {\"role\": \"user\",   \"content\": user_text},\n",
        "          ]\n",
        "        },\n",
        "        config,\n",
        "        stream_mode=\"values\"\n",
        "        )\n",
        "\n",
        "        for event in events:\n",
        "          response =  event[\"messages\"][-1]\n",
        "          if isinstance(response, AIMessage):\n",
        "            if \"tool_calls\" in response.additional_kwargs:\n",
        "              _messages.append(\n",
        "                  ChatMessage(role=\"assistant\",\n",
        "                              content=str(response.tool_calls[0][\"args\"]),\n",
        "                              metadata={\"title\": str(response.tool_calls[0][\"name\"]),\n",
        "                                        \"id\": config[\"configurable\"][\"thread_id\"]\n",
        "                                        }\n",
        "                              ))\n",
        "              yield _messages\n",
        "            else:\n",
        "              _messages.append(ChatMessage(role=\"assistant\",\n",
        "                                           content=response.content,\n",
        "                                           metadata={\"id\": config[\"configurable\"][\"thread_id\"]\n",
        "                                                     }\n",
        "                                           ))\n",
        "              yield _messages\n",
        "    return _messages\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "# 3) Save Chat History\n",
        "########################################\n",
        "CHAT_HISTORY_FILE = \"chat_history.json\"\n",
        "\n",
        "def save_chat_history(history):\n",
        "    \"\"\"\n",
        "    Saves the chat history into a JSON file.\n",
        "    \"\"\"\n",
        "    session_history = [\n",
        "        {\n",
        "            \"role\": \"user\" if msg.is_user else \"assistant\",\n",
        "            \"content\": msg.content\n",
        "        }\n",
        "        for msg in history\n",
        "    ]\n",
        "    with open(CHAT_HISTORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(session_history, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "########################################\n",
        "# 6) Main Gradio Interface\n",
        "########################################\n",
        "with gr.Blocks(theme=\"ocean\") as AI_Tutor:\n",
        "    gr.Markdown(\"# AI Tutor Chatbot (Gradio App)\")\n",
        "\n",
        "    # Primary Chat Interface\n",
        "    chat_interface = gr.ChatInterface(\n",
        "        fn=submit_queries,\n",
        "        type=\"messages\",\n",
        "        chatbot=gr.Chatbot(\n",
        "            label=\"Chat Window\",\n",
        "            height=500,\n",
        "            type=\"messages\"\n",
        "        ),\n",
        "        textbox=gr.MultimodalTextbox(\n",
        "            interactive=True,\n",
        "            file_count=\"multiple\",\n",
        "            file_types=[\".pdf\",\".ppt\",\".pptx\",\".doc\",\".docx\",\".md\",\"image\"],\n",
        "            sources=[\"upload\"],\n",
        "            label=\"Type your query here:\",\n",
        "            placeholder=\"Enter your question...\",\n",
        "        ),\n",
        "        title=\"AI Tutor Chatbot\",\n",
        "        description=\"Ask me anything about Artificial Intelligence!\",\n",
        "        multimodal=True,\n",
        "        save_history=True,\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    AI_Tutor.launch(inline=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7244OC8sKFr"
      },
      "source": [
        "## Apps Deployment (Azure and Docker Containers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDoSjNMFsKFr"
      },
      "source": [
        "Deploy both containers within the same ACI container group. This allows them to communicate with each other using the local network.\n",
        "\n",
        "**Considerations**:\n",
        "\n",
        "**Networking**:\n",
        "* The containers within the same container group can communicate using **localhost** or the container names. For example, Gradio app can make requests to the vLLM container at http://vllm-container:8000.\n",
        "\n",
        "* Only the Gradio app's port (7860) is exposed to the public internet. The vLLM port remains internal to the container group.\n",
        "\n",
        "- Reference: https://docs.vllm.ai/en/latest/deployment/docker.html#deployment-docker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbVM0P3_sKFs",
        "outputId": "563613fd-cec4-4c54-f07d-af25e9b524a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-identity\n",
            "  Downloading azure_identity-1.20.0-py3-none-any.whl.metadata (81 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m71.7/81.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-resource\n",
            "  Downloading azure_mgmt_resource-23.2.0-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting azure-mgmt-containerinstance\n",
            "  Downloading azure_mgmt_containerinstance-10.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting azure-core>=1.31.0 (from azure-identity)\n",
            "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.11/dist-packages (from azure-identity) (43.0.3)\n",
            "Collecting msal>=1.30.0 (from azure-identity)\n",
            "  Downloading msal-1.31.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity)\n",
            "  Downloading msal_extensions-1.2.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity) (4.12.2)\n",
            "Collecting isodate>=0.6.1 (from azure-mgmt-resource)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-common>=1.1 (from azure-mgmt-resource)\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting azure-mgmt-core>=1.3.2 (from azure-mgmt-resource)\n",
            "  Downloading azure_mgmt_core-1.5.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.31.0->azure-identity) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.31.0->azure-identity) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.5->azure-identity) (1.17.1)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity) (2.10.1)\n",
            "Collecting portalocker<3,>=1.4 (from msal-extensions>=1.2.0->azure-identity)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2025.1.31)\n",
            "Downloading azure_identity-1.20.0-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.2/188.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_resource-23.2.0-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_containerinstance-10.1.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_core-1.5.0-py3-none-any.whl (30 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading msal-1.31.1-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.2/113.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal_extensions-1.2.0-py3-none-any.whl (19 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: azure-common, portalocker, isodate, azure-core, azure-mgmt-core, msal, azure-mgmt-resource, azure-mgmt-containerinstance, msal-extensions, azure-identity\n",
            "Successfully installed azure-common-1.1.28 azure-core-1.32.0 azure-identity-1.20.0 azure-mgmt-containerinstance-10.1.0 azure-mgmt-core-1.5.0 azure-mgmt-resource-23.2.0 isodate-0.7.2 msal-1.31.1 msal-extensions-1.2.0 portalocker-2.10.1\n"
          ]
        }
      ],
      "source": [
        "# Install Azure SDK\n",
        "%pip install azure-identity azure-mgmt-resource azure-mgmt-containerinstance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install python-dotenv\n",
        "%pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTBcioS5vTOm",
        "outputId": "5e133953-1e7e-4118-9cef-48d9824a8718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Azure CLI\n",
        "%pip install azure-cli"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qbkENGx_w8cP",
        "outputId": "fab59201-a176-4926-c984-9bb747c4f3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-cli\n",
            "  Downloading azure_cli-2.69.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting antlr4-python3-runtime~=4.13.1 (from azure-cli)\n",
            "  Downloading antlr4_python3_runtime-4.13.2-py3-none-any.whl.metadata (304 bytes)\n",
            "Collecting azure-appconfiguration~=1.7.0 (from azure-cli)\n",
            "  Downloading azure_appconfiguration-1.7.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting azure-batch~=15.0.0b1 (from azure-cli)\n",
            "  Downloading azure_batch-15.0.0b1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting azure-cli-core==2.69.0 (from azure-cli)\n",
            "  Downloading azure_cli_core-2.69.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting azure-cosmos>=3.0.2,~=3.0 (from azure-cli)\n",
            "  Downloading azure_cosmos-3.2.0-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting azure-data-tables==12.4.0 (from azure-cli)\n",
            "  Downloading azure_data_tables-12.4.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting azure-datalake-store~=0.0.53 (from azure-cli)\n",
            "  Downloading azure_datalake_store-0.0.53-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting azure-keyvault-administration==4.4.0b2 (from azure-cli)\n",
            "  Downloading azure_keyvault_administration-4.4.0b2-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting azure-keyvault-certificates==4.7.0 (from azure-cli)\n",
            "  Downloading azure_keyvault_certificates-4.7.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting azure-keyvault-keys==4.9.0b3 (from azure-cli)\n",
            "  Downloading azure_keyvault_keys-4.9.0b3-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-keyvault-secrets==4.7.0 (from azure-cli)\n",
            "  Downloading azure_keyvault_secrets-4.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting azure-mgmt-advisor==9.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_advisor-9.0.0-py2.py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting azure-mgmt-apimanagement==4.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_apimanagement-4.0.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting azure-mgmt-appconfiguration==3.1.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_appconfiguration-3.1.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting azure-mgmt-appcontainers==2.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_appcontainers-2.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting azure-mgmt-applicationinsights~=1.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_applicationinsights-1.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-mgmt-authorization~=4.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_authorization-4.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting azure-mgmt-batchai==7.0.0b1 (from azure-cli)\n",
            "  Downloading azure_mgmt_batchai-7.0.0b1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting azure-mgmt-batch~=17.3.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_batch-17.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting azure-mgmt-billing==6.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_billing-6.0.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting azure-mgmt-botservice~=2.0.0b3 (from azure-cli)\n",
            "  Downloading azure_mgmt_botservice-2.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting azure-mgmt-cdn==12.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_cdn-12.0.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting azure-mgmt-cognitiveservices~=13.5.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_cognitiveservices-13.5.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting azure-mgmt-compute~=33.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_compute-33.0.0-py3-none-any.whl.metadata (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.4/69.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-containerinstance==10.2.0b1 (from azure-cli)\n",
            "  Downloading azure_mgmt_containerinstance-10.2.0b1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting azure-mgmt-containerregistry==10.3.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_containerregistry-10.3.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting azure-mgmt-containerservice~=33.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_containerservice-33.0.0-py3-none-any.whl.metadata (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-cosmosdb==9.7.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_cosmosdb-9.7.0-py3-none-any.whl.metadata (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.8/112.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-databoxedge~=1.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_databoxedge-1.0.0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting azure-mgmt-datalake-store~=1.1.0b1 (from azure-cli)\n",
            "  Downloading azure_mgmt_datalake_store-1.1.0b1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting azure-mgmt-datamigration~=10.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_datamigration-10.0.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting azure-mgmt-devtestlabs~=4.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_devtestlabs-4.0.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting azure-mgmt-dns~=8.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_dns-8.0.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting azure-mgmt-eventgrid==10.2.0b2 (from azure-cli)\n",
            "  Downloading azure_mgmt_eventgrid-10.2.0b2-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting azure-mgmt-eventhub~=10.1.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_eventhub-10.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting azure-mgmt-extendedlocation==1.0.0b2 (from azure-cli)\n",
            "  Downloading azure_mgmt_extendedlocation-1.0.0b2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting azure-mgmt-hdinsight==9.0.0b3 (from azure-cli)\n",
            "  Downloading azure_mgmt_hdinsight-9.0.0b3-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting azure-mgmt-imagebuilder~=1.3.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_imagebuilder-1.3.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting azure-mgmt-iotcentral~=10.0.0b1 (from azure-cli)\n",
            "  Downloading azure_mgmt_iotcentral-10.0.0b2-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting azure-mgmt-iothub==3.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_iothub-3.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting azure-mgmt-iothubprovisioningservices==1.1.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_iothubprovisioningservices-1.1.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting azure-mgmt-keyvault==10.3.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_keyvault-10.3.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting azure-mgmt-loganalytics==13.0.0b4 (from azure-cli)\n",
            "  Downloading azure_mgmt_loganalytics-13.0.0b4-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting azure-mgmt-managementgroups~=1.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_managementgroups-1.0.0-py2.py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting azure-mgmt-maps~=2.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_maps-2.0.0-py2.py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting azure-mgmt-marketplaceordering==1.1.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_marketplaceordering-1.1.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting azure-mgmt-media~=9.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_media-9.0.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting azure-mgmt-monitor~=5.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_monitor-5.0.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting azure-mgmt-msi~=7.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_msi-7.0.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting azure-mgmt-netapp~=10.1.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_netapp-10.1.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting azure-mgmt-policyinsights==1.1.0b4 (from azure-cli)\n",
            "  Downloading azure_mgmt_policyinsights-1.1.0b4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-mgmt-postgresqlflexibleservers==1.1.0b2 (from azure-cli)\n",
            "  Downloading azure_mgmt_postgresqlflexibleservers-1.1.0b2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting azure-mgmt-privatedns~=1.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_privatedns-1.0.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting azure-mgmt-rdbms==10.2.0b17 (from azure-cli)\n",
            "  Downloading azure_mgmt_rdbms-10.2.0b17-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting azure-mgmt-mysqlflexibleservers==1.0.0b3 (from azure-cli)\n",
            "  Downloading azure_mgmt_mysqlflexibleservers-1.0.0b3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting azure-mgmt-recoveryservicesbackup~=9.1.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_recoveryservicesbackup-9.1.0-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting azure-mgmt-recoveryservices~=3.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_recoveryservices-3.0.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting azure-mgmt-redhatopenshift~=1.5.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_redhatopenshift-1.5.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting azure-mgmt-redis~=14.5.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_redis-14.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting azure-mgmt-resource==23.1.1 (from azure-cli)\n",
            "  Downloading azure_mgmt_resource-23.1.1-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting azure-mgmt-search~=9.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_search-9.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting azure-mgmt-security==6.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_security-6.0.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting azure-mgmt-servicebus~=8.2.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_servicebus-8.2.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting azure-mgmt-servicefabricmanagedclusters==2.1.0b1 (from azure-cli)\n",
            "  Downloading azure_mgmt_servicefabricmanagedclusters-2.1.0b1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting azure-mgmt-servicelinker==1.2.0b3 (from azure-cli)\n",
            "  Downloading azure_mgmt_servicelinker-1.2.0b3-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting azure-mgmt-servicefabric~=2.1.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_servicefabric-2.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting azure-mgmt-signalr==2.0.0b2 (from azure-cli)\n",
            "  Downloading azure_mgmt_signalr-2.0.0b2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting azure-mgmt-sqlvirtualmachine==1.0.0b5 (from azure-cli)\n",
            "  Downloading azure_mgmt_sqlvirtualmachine-1.0.0b5-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting azure-mgmt-sql==4.0.0b20 (from azure-cli)\n",
            "  Downloading azure_mgmt_sql-4.0.0b20-py3-none-any.whl.metadata (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.0/73.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-storage==21.2.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_storage-21.2.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting azure-mgmt-synapse==2.1.0b5 (from azure-cli)\n",
            "  Downloading azure_mgmt_synapse-2.1.0b5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting azure-mgmt-trafficmanager~=1.0.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_trafficmanager-1.0.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting azure-mgmt-web==7.2.0 (from azure-cli)\n",
            "  Downloading azure_mgmt_web-7.2.0-py3-none-any.whl.metadata (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-monitor-query==1.2.0 (from azure-cli)\n",
            "  Downloading azure_monitor_query-1.2.0-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting azure-multiapi-storage~=1.3.0 (from azure-cli)\n",
            "  Downloading azure_multiapi_storage-1.3.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting azure-storage-common~=1.4 (from azure-cli)\n",
            "  Downloading azure_storage_common-1.4.2-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting azure-synapse-accesscontrol~=0.5.0 (from azure-cli)\n",
            "  Downloading azure_synapse_accesscontrol-0.5.0-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting azure-synapse-artifacts~=0.19.0 (from azure-cli)\n",
            "  Downloading azure_synapse_artifacts-0.19.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting azure-synapse-managedprivateendpoints~=0.4.0 (from azure-cli)\n",
            "  Downloading azure_synapse_managedprivateendpoints-0.4.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting azure-synapse-spark~=0.2.0 (from azure-cli)\n",
            "  Downloading azure_synapse_spark-0.2.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: chardet~=5.2.0 in /usr/local/lib/python3.11/dist-packages (from azure-cli) (5.2.0)\n",
            "Collecting colorama~=0.4.4 (from azure-cli)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.11/dist-packages (from azure-cli) (1.9.0)\n",
            "Collecting fabric~=3.2.2 (from azure-cli)\n",
            "  Downloading fabric-3.2.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting javaproperties~=0.5.1 (from azure-cli)\n",
            "  Downloading javaproperties-0.5.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jsondiff~=2.0.0 (from azure-cli)\n",
            "  Downloading jsondiff-2.0.0-py3-none-any.whl.metadata (562 bytes)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from azure-cli) (24.2)\n",
            "Collecting paramiko<4.0.0,>=2.0.8 (from azure-cli)\n",
            "  Downloading paramiko-3.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pycomposefile>=0.0.32 (from azure-cli)\n",
            "  Downloading pycomposefile-0.0.32-py3-none-any.whl.metadata (356 bytes)\n",
            "Collecting PyGithub~=1.38 (from azure-cli)\n",
            "  Downloading PyGithub-1.59.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting PyNaCl~=1.5.0 (from azure-cli)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Collecting scp~=0.13.2 (from azure-cli)\n",
            "  Downloading scp-0.13.6-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting semver==2.13.0 (from azure-cli)\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from azure-cli) (75.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from azure-cli) (1.17.0)\n",
            "Collecting sshtunnel~=0.1.4 (from azure-cli)\n",
            "  Downloading sshtunnel-0.1.5-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from azure-cli) (0.9.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from azure-cli) (2.3.0)\n",
            "Collecting websocket-client~=1.3.1 (from azure-cli)\n",
            "  Downloading websocket_client-1.3.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting xmltodict~=0.12 (from azure-cli)\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting argcomplete~=3.5.2 (from azure-cli-core==2.69.0->azure-cli)\n",
            "  Downloading argcomplete-3.5.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting azure-cli-telemetry==1.1.0.* (from azure-cli-core==2.69.0->azure-cli)\n",
            "  Downloading azure_cli_telemetry-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: azure-mgmt-core<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from azure-cli-core==2.69.0->azure-cli) (1.5.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from azure-cli-core==2.69.0->azure-cli) (43.0.3)\n",
            "Collecting humanfriendly~=10.0 (from azure-cli-core==2.69.0->azure-cli)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting jmespath (from azure-cli-core==2.69.0->azure-cli)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting knack~=0.11.0 (from azure-cli-core==2.69.0->azure-cli)\n",
            "  Downloading knack-0.11.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: msal-extensions==1.2.0 in /usr/local/lib/python3.11/dist-packages (from azure-cli-core==2.69.0->azure-cli) (1.2.0)\n",
            "Collecting msal==1.31.2b1 (from msal[broker]==1.31.2b1->azure-cli-core==2.69.0->azure-cli)\n",
            "  Downloading msal-1.31.2b1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msrestazure~=0.6.4 (from azure-cli-core==2.69.0->azure-cli)\n",
            "  Downloading msrestazure-0.6.4.post1-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pkginfo>=1.5.0.1 (from azure-cli-core==2.69.0->azure-cli)\n",
            "  Downloading pkginfo-1.12.1.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: psutil>=5.9 in /usr/local/lib/python3.11/dist-packages (from azure-cli-core==2.69.0->azure-cli) (5.9.5)\n",
            "Requirement already satisfied: PyJWT>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from azure-cli-core==2.69.0->azure-cli) (2.10.1)\n",
            "Requirement already satisfied: pyopenssl>=17.1.0 in /usr/local/lib/python3.11/dist-packages (from azure-cli-core==2.69.0->azure-cli) (24.2.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from azure-cli-core==2.69.0->azure-cli) (2.32.3)\n",
            "Collecting microsoft-security-utilities-secret-masker~=1.0.0b2 (from azure-cli-core==2.69.0->azure-cli)\n",
            "  Downloading microsoft_security_utilities_secret_masker-1.0.0b3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from azure-data-tables==12.4.0->azure-cli) (1.32.0)\n",
            "Collecting msrest>=0.6.21 (from azure-data-tables==12.4.0->azure-cli)\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: azure-common~=1.1 in /usr/local/lib/python3.11/dist-packages (from azure-keyvault-administration==4.4.0b2->azure-cli) (1.1.28)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from azure-keyvault-administration==4.4.0b2->azure-cli) (0.7.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from azure-keyvault-administration==4.4.0b2->azure-cli) (4.12.2)\n",
            "Collecting applicationinsights<0.12,>=0.11.1 (from azure-cli-telemetry==1.1.0.*->azure-cli-core==2.69.0->azure-cli)\n",
            "  Downloading applicationinsights-0.11.10-py2.py3-none-any.whl.metadata (982 bytes)\n",
            "Requirement already satisfied: portalocker<3,>=1.6 in /usr/local/lib/python3.11/dist-packages (from azure-cli-telemetry==1.1.0.*->azure-cli-core==2.69.0->azure-cli) (2.10.1)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.11/dist-packages (from azure-datalake-store~=0.0.53->azure-cli) (1.17.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from azure-multiapi-storage~=1.3.0->azure-cli) (2.8.2)\n",
            "Collecting invoke>=2.0 (from fabric~=3.2.2->azure-cli)\n",
            "  Downloading invoke-2.2.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting decorator>=5 (from fabric~=3.2.2->azure-cli)\n",
            "  Downloading decorator-5.2.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.11/dist-packages (from fabric~=3.2.2->azure-cli) (1.2.18)\n",
            "Collecting bcrypt>=3.2 (from paramiko<4.0.0,>=2.0.8->azure-cli)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from pycomposefile>=0.0.32->azure-cli) (6.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi->azure-datalake-store~=0.0.53->azure-cli) (2.22)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2->fabric~=3.2.2->azure-cli) (1.17.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from knack~=0.11.0->azure-cli-core==2.69.0->azure-cli) (2.18.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from msrest>=0.6.21->azure-data-tables==12.4.0->azure-cli) (2025.1.31)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from msrest>=0.6.21->azure-data-tables==12.4.0->azure-cli) (2.0.0)\n",
            "Collecting adal<2.0.0,>=0.6.0 (from msrestazure~=0.6.4->azure-cli-core==2.69.0->azure-cli)\n",
            "  Downloading adal-1.2.7-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->azure-cli-core==2.69.0->azure-cli) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->azure-cli-core==2.69.0->azure-cli) (3.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->azure-cli-core==2.69.0->azure-cli) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-data-tables==12.4.0->azure-cli) (3.2.2)\n",
            "Downloading azure_cli-2.69.0-py3-none-any.whl (14.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_cli_core-2.69.0-py3-none-any.whl (260 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_data_tables-12.4.0-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_keyvault_administration-4.4.0b2-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_keyvault_certificates-4.7.0-py3-none-any.whl (428 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.1/428.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_keyvault_keys-4.9.0b3-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_keyvault_secrets-4.7.0-py3-none-any.whl (348 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.6/348.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_advisor-9.0.0-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_apimanagement-4.0.0-py3-none-any.whl (804 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.5/804.5 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_appconfiguration-3.1.0-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.5/321.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_appcontainers-2.0.0-py3-none-any.whl (214 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.1/214.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_batchai-7.0.0b1-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_billing-6.0.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_cdn-12.0.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_containerinstance-10.2.0b1-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.1/101.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_containerregistry-10.3.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_cosmosdb-9.7.0-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.9/389.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_eventgrid-10.2.0b2-py3-none-any.whl (248 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.5/248.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_extendedlocation-1.0.0b2-py2.py3-none-any.whl (37 kB)\n",
            "Downloading azure_mgmt_hdinsight-9.0.0b3-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_iothub-3.0.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_iothubprovisioningservices-1.1.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_keyvault-10.3.0-py3-none-any.whl (933 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m933.0/933.0 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_loganalytics-13.0.0b4-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_marketplaceordering-1.1.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading azure_mgmt_mysqlflexibleservers-1.0.0b3-py3-none-any.whl (195 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_policyinsights-1.1.0b4-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.0/127.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_postgresqlflexibleservers-1.1.0b2-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.1/235.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_rdbms-10.2.0b17-py3-none-any.whl (975 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.1/975.1 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_resource-23.1.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_security-6.0.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_servicefabricmanagedclusters-2.1.0b1-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.5/203.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_servicelinker-1.2.0b3-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.7/91.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_signalr-2.0.0b2-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_sql-4.0.0b20-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_sqlvirtualmachine-1.0.0b5-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_storage-21.2.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_synapse-2.1.0b5-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.1/547.1 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_web-7.2.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_monitor_query-1.2.0-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Downloading azure_cli_telemetry-1.1.0-py3-none-any.whl (11 kB)\n",
            "Downloading msal-1.31.2b1-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_appconfiguration-1.7.1-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.0/91.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_batch-15.0.0b1-py3-none-any.whl (892 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m892.2/892.2 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_cosmos-3.2.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_datalake_store-0.0.53-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_applicationinsights-1.0.0-py2.py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.0/303.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_authorization-4.0.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_batch-17.3.0-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_botservice-2.0.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_cognitiveservices-13.5.0-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.3/144.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_compute-33.0.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_containerservice-33.0.0-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_databoxedge-1.0.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_datalake_store-1.1.0b1-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_datamigration-10.0.0-py2.py3-none-any.whl (174 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.5/174.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_devtestlabs-4.0.0-py2.py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_dns-8.0.0-py2.py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_eventhub-10.1.0-py3-none-any.whl (598 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.9/598.9 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_imagebuilder-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.1/78.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_iotcentral-10.0.0b2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_managementgroups-1.0.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_maps-2.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading azure_mgmt_media-9.0.0-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_monitor-5.0.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_msi-7.0.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_netapp-10.1.0-py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.7/200.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_privatedns-1.0.0-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_recoveryservices-3.0.0-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_recoveryservicesbackup-9.1.0-py3-none-any.whl (570 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.9/570.9 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_redhatopenshift-1.5.0-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.4/408.4 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_redis-14.5.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_search-9.1.0-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.3/110.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_servicebus-8.2.1-py3-none-any.whl (904 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m904.2/904.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_servicefabric-2.1.0-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_trafficmanager-1.0.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_multiapi_storage-1.3.0-py2.py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_storage_common-1.4.2-py2.py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_synapse_accesscontrol-0.5.0-py2.py3-none-any.whl (30 kB)\n",
            "Downloading azure_synapse_artifacts-0.19.0-py3-none-any.whl (495 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.8/495.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_synapse_managedprivateendpoints-0.4.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_synapse_spark-0.2.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading fabric-3.2.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading javaproperties-0.5.2-py2.py3-none-any.whl (19 kB)\n",
            "Downloading jsondiff-2.0.0-py3-none-any.whl (6.6 kB)\n",
            "Downloading paramiko-3.5.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycomposefile-0.0.32-py3-none-any.whl (28 kB)\n",
            "Downloading PyGithub-1.59.1-py3-none-any.whl (342 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.2/342.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scp-0.13.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading sshtunnel-0.1.5-py2.py3-none-any.whl (23 kB)\n",
            "Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading argcomplete-3.5.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading decorator-5.2.0-py3-none-any.whl (9.1 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading invoke-2.2.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading knack-0.11.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading microsoft_security_utilities_secret_masker-1.0.0b3-py3-none-any.whl (16 kB)\n",
            "Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msrestazure-0.6.4.post1-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pkginfo-1.12.1.2-py3-none-any.whl (32 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading applicationinsights-0.11.10-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: microsoft-security-utilities-secret-masker, jsondiff, applicationinsights, antlr4-python3-runtime, xmltodict, websocket-client, semver, pycomposefile, pkginfo, jmespath, javaproperties, invoke, humanfriendly, decorator, colorama, bcrypt, azure-cli-telemetry, argcomplete, PyNaCl, knack, azure-cosmos, paramiko, msrest, azure-storage-common, azure-monitor-query, azure-keyvault-secrets, azure-keyvault-keys, azure-keyvault-certificates, azure-keyvault-administration, azure-batch, azure-appconfiguration, adal, sshtunnel, scp, PyGithub, msrestazure, msal, fabric, azure-synapse-spark, azure-synapse-managedprivateendpoints, azure-synapse-artifacts, azure-synapse-accesscontrol, azure-multiapi-storage, azure-mgmt-web, azure-mgmt-trafficmanager, azure-mgmt-synapse, azure-mgmt-storage, azure-mgmt-sqlvirtualmachine, azure-mgmt-sql, azure-mgmt-signalr, azure-mgmt-servicelinker, azure-mgmt-servicefabricmanagedclusters, azure-mgmt-servicefabric, azure-mgmt-servicebus, azure-mgmt-security, azure-mgmt-search, azure-mgmt-resource, azure-mgmt-redis, azure-mgmt-redhatopenshift, azure-mgmt-recoveryservicesbackup, azure-mgmt-recoveryservices, azure-mgmt-rdbms, azure-mgmt-privatedns, azure-mgmt-postgresqlflexibleservers, azure-mgmt-policyinsights, azure-mgmt-netapp, azure-mgmt-mysqlflexibleservers, azure-mgmt-msi, azure-mgmt-monitor, azure-mgmt-media, azure-mgmt-marketplaceordering, azure-mgmt-maps, azure-mgmt-managementgroups, azure-mgmt-loganalytics, azure-mgmt-keyvault, azure-mgmt-iothubprovisioningservices, azure-mgmt-iothub, azure-mgmt-iotcentral, azure-mgmt-imagebuilder, azure-mgmt-hdinsight, azure-mgmt-extendedlocation, azure-mgmt-eventhub, azure-mgmt-eventgrid, azure-mgmt-dns, azure-mgmt-datamigration, azure-mgmt-datalake-store, azure-mgmt-databoxedge, azure-mgmt-cosmosdb, azure-mgmt-containerservice, azure-mgmt-containerregistry, azure-mgmt-containerinstance, azure-mgmt-compute, azure-mgmt-cognitiveservices, azure-mgmt-cdn, azure-mgmt-botservice, azure-mgmt-billing, azure-mgmt-batchai, azure-mgmt-batch, azure-mgmt-authorization, azure-mgmt-applicationinsights, azure-mgmt-appcontainers, azure-mgmt-appconfiguration, azure-mgmt-apimanagement, azure-mgmt-advisor, azure-data-tables, azure-mgmt-devtestlabs, azure-datalake-store, azure-cli-core, azure-cli\n",
            "  Attempting uninstall: websocket-client\n",
            "    Found existing installation: websocket-client 1.8.0\n",
            "    Uninstalling websocket-client-1.8.0:\n",
            "      Successfully uninstalled websocket-client-1.8.0\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: msal\n",
            "    Found existing installation: msal 1.31.1\n",
            "    Uninstalling msal-1.31.1:\n",
            "      Successfully uninstalled msal-1.31.1\n",
            "  Attempting uninstall: azure-mgmt-resource\n",
            "    Found existing installation: azure-mgmt-resource 23.2.0\n",
            "    Uninstalling azure-mgmt-resource-23.2.0:\n",
            "      Successfully uninstalled azure-mgmt-resource-23.2.0\n",
            "  Attempting uninstall: azure-mgmt-containerinstance\n",
            "    Found existing installation: azure-mgmt-containerinstance 10.1.0\n",
            "    Uninstalling azure-mgmt-containerinstance-10.1.0:\n",
            "      Successfully uninstalled azure-mgmt-containerinstance-10.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyGithub-1.59.1 PyNaCl-1.5.0 adal-1.2.7 antlr4-python3-runtime-4.13.2 applicationinsights-0.11.10 argcomplete-3.5.3 azure-appconfiguration-1.7.1 azure-batch-15.0.0b1 azure-cli-2.69.0 azure-cli-core-2.69.0 azure-cli-telemetry-1.1.0 azure-cosmos-3.2.0 azure-data-tables-12.4.0 azure-datalake-store-0.0.53 azure-keyvault-administration-4.4.0b2 azure-keyvault-certificates-4.7.0 azure-keyvault-keys-4.9.0b3 azure-keyvault-secrets-4.7.0 azure-mgmt-advisor-9.0.0 azure-mgmt-apimanagement-4.0.0 azure-mgmt-appconfiguration-3.1.0 azure-mgmt-appcontainers-2.0.0 azure-mgmt-applicationinsights-1.0.0 azure-mgmt-authorization-4.0.0 azure-mgmt-batch-17.3.0 azure-mgmt-batchai-7.0.0b1 azure-mgmt-billing-6.0.0 azure-mgmt-botservice-2.0.0 azure-mgmt-cdn-12.0.0 azure-mgmt-cognitiveservices-13.5.0 azure-mgmt-compute-33.0.0 azure-mgmt-containerinstance-10.2.0b1 azure-mgmt-containerregistry-10.3.0 azure-mgmt-containerservice-33.0.0 azure-mgmt-cosmosdb-9.7.0 azure-mgmt-databoxedge-1.0.0 azure-mgmt-datalake-store-1.1.0b1 azure-mgmt-datamigration-10.0.0 azure-mgmt-devtestlabs-4.0.0 azure-mgmt-dns-8.0.0 azure-mgmt-eventgrid-10.2.0b2 azure-mgmt-eventhub-10.1.0 azure-mgmt-extendedlocation-1.0.0b2 azure-mgmt-hdinsight-9.0.0b3 azure-mgmt-imagebuilder-1.3.0 azure-mgmt-iotcentral-10.0.0b2 azure-mgmt-iothub-3.0.0 azure-mgmt-iothubprovisioningservices-1.1.0 azure-mgmt-keyvault-10.3.0 azure-mgmt-loganalytics-13.0.0b4 azure-mgmt-managementgroups-1.0.0 azure-mgmt-maps-2.0.0 azure-mgmt-marketplaceordering-1.1.0 azure-mgmt-media-9.0.0 azure-mgmt-monitor-5.0.1 azure-mgmt-msi-7.0.0 azure-mgmt-mysqlflexibleservers-1.0.0b3 azure-mgmt-netapp-10.1.0 azure-mgmt-policyinsights-1.1.0b4 azure-mgmt-postgresqlflexibleservers-1.1.0b2 azure-mgmt-privatedns-1.0.0 azure-mgmt-rdbms-10.2.0b17 azure-mgmt-recoveryservices-3.0.0 azure-mgmt-recoveryservicesbackup-9.1.0 azure-mgmt-redhatopenshift-1.5.0 azure-mgmt-redis-14.5.0 azure-mgmt-resource-23.1.1 azure-mgmt-search-9.1.0 azure-mgmt-security-6.0.0 azure-mgmt-servicebus-8.2.1 azure-mgmt-servicefabric-2.1.0 azure-mgmt-servicefabricmanagedclusters-2.1.0b1 azure-mgmt-servicelinker-1.2.0b3 azure-mgmt-signalr-2.0.0b2 azure-mgmt-sql-4.0.0b20 azure-mgmt-sqlvirtualmachine-1.0.0b5 azure-mgmt-storage-21.2.0 azure-mgmt-synapse-2.1.0b5 azure-mgmt-trafficmanager-1.0.0 azure-mgmt-web-7.2.0 azure-monitor-query-1.2.0 azure-multiapi-storage-1.3.0 azure-storage-common-1.4.2 azure-synapse-accesscontrol-0.5.0 azure-synapse-artifacts-0.19.0 azure-synapse-managedprivateendpoints-0.4.0 azure-synapse-spark-0.2.0 bcrypt-4.2.1 colorama-0.4.6 decorator-5.2.0 fabric-3.2.2 humanfriendly-10.0 invoke-2.2.0 javaproperties-0.5.2 jmespath-1.0.1 jsondiff-2.0.0 knack-0.11.0 microsoft-security-utilities-secret-masker-1.0.0b3 msal-1.31.2b1 msrest-0.7.1 msrestazure-0.6.4.post1 paramiko-3.5.1 pkginfo-1.12.1.2 pycomposefile-0.0.32 scp-0.13.6 semver-2.13.0 sshtunnel-0.1.5 websocket-client-1.3.3 xmltodict-0.14.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "decorator"
                ]
              },
              "id": "65668b1dae9b41809a6309c019307f3c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to Azure\n",
        "!az login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeNgzfjJy2Hk",
        "outputId": "0c05ed86-4a2b-480f-826d-23f75e1cdb0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code ANJQDPLL4 to authenticate.\u001b[0m\n",
            "\n",
            "Retrieving tenants and subscriptions for the selection...\n",
            "\n",
            "[Tenant and subscription selection]\n",
            "\n",
            "No     Subscription name     Subscription ID                       Tenant\n",
            "-----  --------------------  ------------------------------------  -----------------\n",
            "\u001b[96m[1]\u001b[0m *  \u001b[96mAzure subscription 1\u001b[0m  \u001b[96md7d3e3dc-6c53-4e42-ae2e-1f7410d1b793\u001b[0m  \u001b[96mDefault Directory\u001b[0m\n",
            "\n",
            "The default is marked with an *; the default tenant is 'Default Directory' and subscription is 'Azure subscription 1' (d7d3e3dc-6c53-4e42-ae2e-1f7410d1b793).\n",
            "\n",
            "Select a subscription and tenant (Type a number or Enter for no changes): 1\n",
            "\n",
            "Tenant: Default Directory\n",
            "Subscription: Azure subscription 1 (d7d3e3dc-6c53-4e42-ae2e-1f7410d1b793)\n",
            "\n",
            "[Announcements]\n",
            "With the new Azure CLI login experience, you can select the subscription you want to use more easily. Learn more about it and its configuration at https://go.microsoft.com/fwlink/?linkid=2271236\n",
            "\n",
            "If you encounter any problem, please open an issue at https://aka.ms/azclibug\n",
            "\n",
            "\u001b[93m[Warning] The login output has been updated. Please be aware that it no longer displays the full list of available subscriptions by default.\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Subscription ID as Secret in Colab\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "result=subprocess.run([\"az\", \"account\", \"show\", \"--query\", \"id\", \"-o\", \"tsv\"],capture_output=True, text=True, check=True)\n",
        "os.environ[\"AZURE_SUBSCRIPTION_ID\"]=result.stdout.strip()"
      ],
      "metadata": {
        "id": "l6DcxK7L6Jju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Subscription ID from the environment variable (make sure it's set correctly)\n",
        "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
        "\n",
        "# Format the scope string correctly using f-string\n",
        "scope = f\"/subscriptions/{subscription_id}\"\n",
        "\n",
        "# Execute the command with the formatted scope\n",
        "result = subprocess.run(\n",
        "    [\"az\", \"ad\", \"sp\", \"create-for-rbac\", \"--name\", \"my-colab-app\", \"--role\", \"contributor\", \"--scopes\", scope],\n",
        "    capture_output=True,\n",
        "    text=True,\n",
        "    check=True\n",
        ")\n",
        "sp_credentials = json.loads(result.stdout.strip())\n",
        "\n",
        "os.environ[\"AZURE_CLIENT_ID\"]  = sp_credentials[\"appId\"] # from the JSON clientId\n",
        "os.environ[\"AZURE_CLIENT_SECRET\"] = sp_credentials[\"password\"] # from the JSON clientSecret\n",
        "os.environ[\"AZURE_TENANT_ID\"] = sp_credentials[\"tenant\"] # from the JSON tenantId"
      ],
      "metadata": {
        "id": "jSVRiVZh9-Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run these command to get all the credentials (careful not to reveal it!)\n",
        "#!az ad sp create-for-rbac --name \"my-colab-app\" --role contributor --scopes /subscriptions/$(az account show --query id -o tsv)"
      ],
      "metadata": {
        "id": "dRLnx7YE1Pw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the stored credentials to create a ClientSecretCredential and initialize the Container Instance client.\n",
        "import os\n",
        "from azure.identity import ClientSecretCredential\n",
        "from azure.mgmt.containerinstance import ContainerInstanceManagementClient\n",
        "\n",
        "# Retrieve credentials from environment variables\n",
        "tenant_id = os.environ.get(\"AZURE_TENANT_ID\")\n",
        "client_id = os.environ.get(\"AZURE_CLIENT_ID\")\n",
        "client_secret = os.environ.get(\"AZURE_CLIENT_SECRET\")\n",
        "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
        "\n",
        "# Create a credential and initialize the client\n",
        "credential = ClientSecretCredential(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret)\n",
        "aci_client = ContainerInstanceManagementClient(credential, subscription_id)\n"
      ],
      "metadata": {
        "id": "Et0V9CIkEJCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Resource Group via CLI\n",
        "!az group create --name ITI110_Project --location eastus\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb7K0YfPEgF5",
        "outputId": "e061dd5c-ba7b-4716-8812-d577c4bd8aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"/subscriptions/d7d3e3dc-6c53-4e42-ae2e-1f7410d1b793/resourceGroups/ITI110_Project\",\n",
            "  \"location\": \"eastus\",\n",
            "  \"managedBy\": null,\n",
            "  \"name\": \"ITI110_Project\",\n",
            "  \"properties\": {\n",
            "    \"provisioningState\": \"Succeeded\"\n",
            "  },\n",
            "  \"tags\": null,\n",
            "  \"type\": \"Microsoft.Resources/resourceGroups\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide the login crediential for Docker Hub\n",
        "from azure.mgmt.containerinstance.models import ImageRegistryCredential\n",
        "\n",
        "docker_hub_credentials = [\n",
        "    ImageRegistryCredential(\n",
        "         server=\"index.docker.io\",\n",
        "         username=userdata.get(\"docker_id\"),\n",
        "         password=userdata.get(\"docker_pw\")\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "-9kmu1I3LYfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Deployment Parameters and the vLLM and Gradio App in the same Container Group\n",
        "from azure.mgmt.containerinstance.models import (\n",
        "    ContainerGroup,\n",
        "    Container,\n",
        "    ContainerPort,\n",
        "    IpAddress,\n",
        "    Port,\n",
        "    ResourceRequests,\n",
        "    ResourceRequirements,\n",
        "    OperatingSystemTypes,\n",
        "    GpuResource\n",
        ")\n",
        "\n",
        "\n",
        "# --- Deployment Configuration ---\n",
        "resource_group = \"ITI110_Project\"  # Ensure this group exists\n",
        "location = \"eastus\"                 # Choose your Azure region\n",
        "container_group_name = \"vllm-gradio-group\"\n",
        "\n",
        "# --- vLLM Container Configuration ---\n",
        "vllm_container_name = \"vllm\"\n",
        "vllm_image = \"vllm/vllm-openai:latest\"\n",
        "vllm_cpu = 3\n",
        "vllm_memory = 12\n",
        "vllm_port = 8000\n",
        "\n",
        "# Define resource requirements for the vLLM container\n",
        "vllm_resource_requests = ResourceRequests(cpu=vllm_cpu, memory_in_gb=vllm_memory)\n",
        "vllm_resources = ResourceRequirements(requests=vllm_resource_requests)\n",
        "\n",
        "# Startup command downloads the chat template and starts the vLLM server\n",
        "vllm_command = [\n",
        "    \"bash\", \"-c\",\n",
        "    \"wget -O /tmp/tool_chat_template_llama3.1_json.jinja \"\n",
        "    \"https://github.com/vllm-project/vllm/raw/refs/heads/main/examples/tool_chat_template_llama3.1_json.jinja && \"\n",
        "    \"vllm.entrypoints.openai.api_server \"\n",
        "    \"--model unsloth/llama-3-8b-Instruct-bnb-4bit \"\n",
        "    \"--enable-auto-tool-choice \"\n",
        "    \"--tool-call-parser llama3_json \"\n",
        "    \"--chat-template /tmp/tool_chat_template_llama3.1_json.jinja \"\n",
        "    \"--quantization bitsandbytes \"\n",
        "    \"--load-format bitsandbytes \"\n",
        "    \"--dtype half \"\n",
        "    \"--max-model-len 8192 \"\n",
        "    \"--download-dir models/vllm\"\n",
        "]\n",
        "\n",
        "vllm_container = Container(\n",
        "    name=vllm_container_name,\n",
        "    image=vllm_image,\n",
        "    resources=vllm_resources,\n",
        "    ports=[ContainerPort(port=vllm_port)],\n",
        "    command=vllm_command\n",
        ")\n",
        "\n",
        "# --- Gradio Container Configuration ---\n",
        "gradio_container_name = \"gradio\"\n",
        "gradio_image = \"sumkh/my-gradio-app:latest\"  # This image must be accessible (e.g., from Docker Hub or ACR). Ensure you have push your docker image to docker hub\n",
        "gradio_cpu = 1\n",
        "gradio_memory = 2\n",
        "gradio_port = 7860\n",
        "\n",
        "gradio_resource_requests = ResourceRequests(cpu=gradio_cpu, memory_in_gb=gradio_memory)\n",
        "gradio_resources = ResourceRequirements(requests=gradio_resource_requests)\n",
        "\n",
        "# (Assuming the image's Dockerfile has the correct CMD to start your Gradio app.)\n",
        "gradio_container = Container(\n",
        "    name=gradio_container_name,\n",
        "    image=gradio_image,\n",
        "    resources=gradio_resources,\n",
        "    ports=[ContainerPort(port=gradio_port)]\n",
        ")\n",
        "\n",
        "# --- Container Group Configuration ---\n",
        "# In a single container group, we can expose one public endpoint.\n",
        "# Here we expose the Gradio container’s port (7860) to receive external traffic.\n",
        "ip_address = IpAddress(\n",
        "    ports=[Port(protocol=\"TCP\", port=gradio_port)],\n",
        "    dns_name_label=\"gradio-chatbot-group-unique\",  # Must be unique within the region\n",
        "    type=\"Public\"\n",
        ")\n",
        "\n",
        "# Define the container group with both containers\n",
        "container_group = ContainerGroup(\n",
        "    location=location,\n",
        "    containers=[vllm_container, gradio_container],\n",
        "    os_type=OperatingSystemTypes.linux,\n",
        "    ip_address=ip_address,\n",
        "    restart_policy=\"OnFailure\",\n",
        "    image_registry_credentials=docker_hub_credentials\n",
        ")\n",
        "\n",
        "# --- Deploy the Container Group ---\n",
        "print(\"Deploying container group with vLLM and Gradio containers...\")\n",
        "deployment = aci_client.container_groups.begin_create_or_update(\n",
        "    resource_group, container_group_name, container_group\n",
        ").result()\n",
        "\n",
        "print(\"Deployment complete. Container group name:\", deployment.name)\n",
        "print(f\"Gradio endpoint: http://{ip_address.dns_name_label}.{location}.azurecontainer.io:{gradio_port}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngwhKNohQoZY",
        "outputId": "ffc56a57-cc32-4de4-eb2b-2b2a6a1afbb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deploying container group with vLLM and Gradio containers...\n",
            "Deployment complete. Container group name: vllm-gradio-group\n",
            "Gradio endpoint: http://gradio-chatbot-group-unique.eastus.azurecontainer.io:7860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.mgmt.containerinstance.models import (\n",
        "    ContainerGroup,\n",
        "    Container,\n",
        "    ContainerPort,\n",
        "    IpAddress,\n",
        "    Port,\n",
        "    ResourceRequests,\n",
        "    ResourceRequirements,\n",
        "    OperatingSystemTypes,\n",
        "    GpuResource\n",
        ")\n",
        "\n",
        "# --- GPU Configuration for vLLM Container ---\n",
        "# GPU SKU options often include \"NvidiaK80\", \"NvidiaP100\", \"NvidiaV100\", \"NvidiaT4\".\n",
        "# Check which ones are supported in your chosen region.\n",
        "\n",
        "vllm_gpu_count = 1\n",
        "vllm_gpu_sku = \"NvidiaT4\"  # Example; ensure it's supported in your region\n",
        "\n",
        "# Original CPU/Memory requirements\n",
        "vllm_cpu = 3\n",
        "vllm_memory = 12\n",
        "\n",
        "# GpuResource configuration\n",
        "vllm_gpu_resource = GpuResource(count=vllm_gpu_count, sku=vllm_gpu_sku)\n",
        "\n",
        "# ResourceRequests now includes the GPU resource\n",
        "vllm_resource_requests = ResourceRequests(\n",
        "    cpu=vllm_cpu,\n",
        "    memory_in_gb=vllm_memory,\n",
        "    gpu=vllm_gpu_resource\n",
        ")\n",
        "\n",
        "vllm_resources = ResourceRequirements(\n",
        "    requests=vllm_resource_requests\n",
        ")\n",
        "\n",
        "# --- vLLM Container Configuration ---\n",
        "vllm_command = [\n",
        "    \"bash\", \"-c\",\n",
        "    \"wget -O /tmp/tool_chat_template_llama3.1_json.jinja \"\n",
        "    \"https://github.com/vllm-project/vllm/raw/refs/heads/main/examples/tool_chat_template_llama3.1_json.jinja && \"\n",
        "    \"vllm.entrypoints.openai.api_server \"\n",
        "    \"--model unsloth/llama-3-8b-Instruct-bnb-4bit \"\n",
        "    \"--enable-auto-tool-choice \"\n",
        "    \"--tool-call-parser llama3_json \"\n",
        "    \"--chat-template /tmp/tool_chat_template_llama3.1_json.jinja \"\n",
        "    \"--quantization bitsandbytes \"\n",
        "    \"--load-format bitsandbytes \"\n",
        "    \"--dtype half \"\n",
        "    \"--max-model-len 8192 \"\n",
        "    \"--download-dir models/vllm\"\n",
        "]\n",
        "\n",
        "vllm_container = Container(\n",
        "    name=\"vllm\",\n",
        "    image=\"vllm/vllm-openai:latest\",\n",
        "    resources=vllm_resources,\n",
        "    ports=[ContainerPort(port=8000)],\n",
        "    command=vllm_command\n",
        ")\n",
        "\n",
        "# --- Gradio Container (CPU) remains unchanged ---\n",
        "gradio_cpu = 1\n",
        "gradio_memory = 2\n",
        "...\n"
      ],
      "metadata": {
        "id": "D0VjDdTeUOv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsDp4gHHsKFr"
      },
      "source": [
        "#### Build the Gradio App Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSX_Ht1esKFt"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "docker run \\\n",
        "  --runtime nvidia --gpus all \\\n",
        "  --name ITI110_vllm_container \\\n",
        "  ##--network my-network \\\n",
        "  -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
        "  --env \"HUGGING_FACE_HUB_TOKEN=$SECRETS/HF_TOKEN\" \\\n",
        "  -p 8000:8000 \\\n",
        "  --ipc=host \\\n",
        "  -v /content/examples:/examples \\ # Mount the directory\n",
        "  vllm/vllm-openai:latest \\\n",
        "  vllm.entrypoints.openai.api_server \\  # Start the vllm server explicitly\n",
        "    --model unsloth/llama-3-8b-Instruct-bnb-4bit \\\n",
        "    --enable-auto-tool-choice \\\n",
        "    --tool-call-parser llama3_json \\\n",
        "    --chat-template examples/tool_chat_template_llama3.1_json.jinja \\\n",
        "    --quantization bitsandbytes \\\n",
        "    --load-format bitsandbytes \\\n",
        "    --dtype half \\\n",
        "    --max-model-len 8192 \\\n",
        "    --download-dir models/vllm \\\n",
        "    > vllm.log &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeAEbecrsKFu"
      },
      "source": [
        " http://my-vllm-instance.eastus.azurecontainer.io:8000"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KmeoL-_lcs8Y",
        "s_2h8tWUc9yA",
        "QNgX4ZVYrS8Z",
        "LuhaRfR6dLno"
      ],
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "531a1613b7884b98a4f6ecb738407550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89f28159d26b486eb75397e4ecbeb612",
              "IPY_MODEL_bc0bf1f8b76043ac996e74c643013271",
              "IPY_MODEL_773165eb96844842b2795ff7da4d762e"
            ],
            "layout": "IPY_MODEL_7b78bea13f8d4e798e220ae1c02df8df"
          }
        },
        "89f28159d26b486eb75397e4ecbeb612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4285bbc9e83449e58da3f37716aa1edd",
            "placeholder": "​",
            "style": "IPY_MODEL_3da443126e4c434ca6c694c6d2d653eb",
            "value": "modules.json: 100%"
          }
        },
        "bc0bf1f8b76043ac996e74c643013271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_394d406cbe5d49e6a630b97a85b3b0d2",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a3226bafe1e452b830ddf7936b52b3e",
            "value": 349
          }
        },
        "773165eb96844842b2795ff7da4d762e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c001473620b4b1aa097c5c8bec2f563",
            "placeholder": "​",
            "style": "IPY_MODEL_aa1651313c76474d845be1e39b00936b",
            "value": " 349/349 [00:00&lt;00:00, 39.2kB/s]"
          }
        },
        "7b78bea13f8d4e798e220ae1c02df8df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4285bbc9e83449e58da3f37716aa1edd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3da443126e4c434ca6c694c6d2d653eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "394d406cbe5d49e6a630b97a85b3b0d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a3226bafe1e452b830ddf7936b52b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c001473620b4b1aa097c5c8bec2f563": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa1651313c76474d845be1e39b00936b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dadf3cfdb3fe4fb7ae0e95bd5acdc8c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a9bad03829542f78aabad029e0aa49c",
              "IPY_MODEL_21329eaa3cba4a438bcd67297b588b8a",
              "IPY_MODEL_03b1aa02da9a4996b042aa233611a713"
            ],
            "layout": "IPY_MODEL_6d8f1b6222bb4e398bffd7950e471349"
          }
        },
        "1a9bad03829542f78aabad029e0aa49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5506bef9ca9940cdb5a4eb257a019d74",
            "placeholder": "​",
            "style": "IPY_MODEL_a2ca9d561aae4bd0a83fd79a85a01412",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "21329eaa3cba4a438bcd67297b588b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fca1515dce9c4a80a758d927692666c5",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e19ad2d93bd4c9cbde705dc7ba7fc23",
            "value": 116
          }
        },
        "03b1aa02da9a4996b042aa233611a713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bddede6d2ac24d208044a0595f1e45a3",
            "placeholder": "​",
            "style": "IPY_MODEL_569db0b4eb7f442eb90f2cf4599ad7e9",
            "value": " 116/116 [00:00&lt;00:00, 14.1kB/s]"
          }
        },
        "6d8f1b6222bb4e398bffd7950e471349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5506bef9ca9940cdb5a4eb257a019d74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2ca9d561aae4bd0a83fd79a85a01412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fca1515dce9c4a80a758d927692666c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e19ad2d93bd4c9cbde705dc7ba7fc23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bddede6d2ac24d208044a0595f1e45a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "569db0b4eb7f442eb90f2cf4599ad7e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb957dc078a44668bd7f89190d1d895a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_101ccc855a06494b866410875860e639",
              "IPY_MODEL_c6e7d3e344124534a13a88cf10eaeb10",
              "IPY_MODEL_5bbd4d0438934c69b73f1060acb166ae"
            ],
            "layout": "IPY_MODEL_457a7fa9b29a40ad99c4424aa17b948d"
          }
        },
        "101ccc855a06494b866410875860e639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8beebab95b043e2a386a08e3e75794f",
            "placeholder": "​",
            "style": "IPY_MODEL_99a004a87396434f94a009ffc7130129",
            "value": "README.md: 100%"
          }
        },
        "c6e7d3e344124534a13a88cf10eaeb10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfc68326b37d449d8362738b9e14cbf2",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f47434872d6645f1a9ea6d4c85cd23f1",
            "value": 10659
          }
        },
        "5bbd4d0438934c69b73f1060acb166ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1070868417ec48b68e782b65c58d2cce",
            "placeholder": "​",
            "style": "IPY_MODEL_0041ed8cac1647eea8793fb9c575a9e1",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 1.27MB/s]"
          }
        },
        "457a7fa9b29a40ad99c4424aa17b948d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8beebab95b043e2a386a08e3e75794f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99a004a87396434f94a009ffc7130129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfc68326b37d449d8362738b9e14cbf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f47434872d6645f1a9ea6d4c85cd23f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1070868417ec48b68e782b65c58d2cce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0041ed8cac1647eea8793fb9c575a9e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "749a032695fe49439eee3a41bd55c36c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_21988dfce8a849c6ad9b06f57b9d1adb",
              "IPY_MODEL_17494705d9be452280b8852c1fbe90aa",
              "IPY_MODEL_e1019d54f44d45cf82247674009460bd"
            ],
            "layout": "IPY_MODEL_50adf5bf509a486180041b40fd069a3f"
          }
        },
        "21988dfce8a849c6ad9b06f57b9d1adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0b20200816740a8a5d0dce540543874",
            "placeholder": "​",
            "style": "IPY_MODEL_dddbfbdb39ec4ab79fc41d52f40e8c7b",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "17494705d9be452280b8852c1fbe90aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cb117d9facc40468dfaa10dc0a62db5",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35d174d802264196b9da42f6b521d9b3",
            "value": 53
          }
        },
        "e1019d54f44d45cf82247674009460bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_270d4f24059740d98e6d9d59b20173c4",
            "placeholder": "​",
            "style": "IPY_MODEL_00131d1ed8434d95897ca2e18cc5081d",
            "value": " 53.0/53.0 [00:00&lt;00:00, 6.16kB/s]"
          }
        },
        "50adf5bf509a486180041b40fd069a3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0b20200816740a8a5d0dce540543874": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dddbfbdb39ec4ab79fc41d52f40e8c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cb117d9facc40468dfaa10dc0a62db5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d174d802264196b9da42f6b521d9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "270d4f24059740d98e6d9d59b20173c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00131d1ed8434d95897ca2e18cc5081d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d49eb67d8ba643b88e44f23cacc86c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_559389f7b09f4138b38d8b35b1584738",
              "IPY_MODEL_c344468e154d4e91be66141d409f0b3b",
              "IPY_MODEL_78273d214c034060a2ae3a00d1425715"
            ],
            "layout": "IPY_MODEL_5c2c87195010404e9d94558329a2cf86"
          }
        },
        "559389f7b09f4138b38d8b35b1584738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac606b1aa9a94c6d9e87dbbceb14c427",
            "placeholder": "​",
            "style": "IPY_MODEL_e3a50b4a79b14d83948c2b4a2178863d",
            "value": "config.json: 100%"
          }
        },
        "c344468e154d4e91be66141d409f0b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a46b1a51bd3b44b6bc86f9987d56901e",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34f6eb2bfc7943c8a4542a0d229e25db",
            "value": 612
          }
        },
        "78273d214c034060a2ae3a00d1425715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f752284f6bc240fb9d0992d09aa3e75c",
            "placeholder": "​",
            "style": "IPY_MODEL_0173607844b048fcbf7dfd00176818ad",
            "value": " 612/612 [00:00&lt;00:00, 80.3kB/s]"
          }
        },
        "5c2c87195010404e9d94558329a2cf86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac606b1aa9a94c6d9e87dbbceb14c427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3a50b4a79b14d83948c2b4a2178863d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a46b1a51bd3b44b6bc86f9987d56901e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34f6eb2bfc7943c8a4542a0d229e25db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f752284f6bc240fb9d0992d09aa3e75c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0173607844b048fcbf7dfd00176818ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5b5997ecb524b30a457d85b2e1a9efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c15d801015454884919c24965fe71542",
              "IPY_MODEL_c707929302f148699bc200402fa78e8d",
              "IPY_MODEL_66396ce11a10415c98d5ff5059d3daf0"
            ],
            "layout": "IPY_MODEL_c45a860c65924eb4b48e966c56edda56"
          }
        },
        "c15d801015454884919c24965fe71542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f927d0f5718c4e869a1efc6a22bb0d12",
            "placeholder": "​",
            "style": "IPY_MODEL_f9679bd7cdb84168bbfba6c7afb5cc54",
            "value": "model.safetensors: 100%"
          }
        },
        "c707929302f148699bc200402fa78e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87d547c8d15d4d7198f353b0ff827456",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0c7a1d43e994eaf8780742c63101054",
            "value": 90868376
          }
        },
        "66396ce11a10415c98d5ff5059d3daf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d17848c8cc1e43b7bea9f10584cde1d0",
            "placeholder": "​",
            "style": "IPY_MODEL_8a65e8ac48ac43d79dc093ed1bde70d6",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 241MB/s]"
          }
        },
        "c45a860c65924eb4b48e966c56edda56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f927d0f5718c4e869a1efc6a22bb0d12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9679bd7cdb84168bbfba6c7afb5cc54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87d547c8d15d4d7198f353b0ff827456": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0c7a1d43e994eaf8780742c63101054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d17848c8cc1e43b7bea9f10584cde1d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a65e8ac48ac43d79dc093ed1bde70d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f159346c6c84876a3053575868bf9b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_343d1e4d764d493c8803d0938553cf4c",
              "IPY_MODEL_02ca03f0c572445a91a166d9cc12890a",
              "IPY_MODEL_1c932836a78c41d8941269872d0b4650"
            ],
            "layout": "IPY_MODEL_1f42e6294d144fed904c61001cd2d45a"
          }
        },
        "343d1e4d764d493c8803d0938553cf4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8d1955f428b403f93a96a6943a6f94a",
            "placeholder": "​",
            "style": "IPY_MODEL_c2a4d8be05ae43eca3a6ea6655f03582",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "02ca03f0c572445a91a166d9cc12890a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3f4d9b4087e4898b7f61350f7a16e71",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72732fed485d42e7b49e8eb1b6a800dd",
            "value": 350
          }
        },
        "1c932836a78c41d8941269872d0b4650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1170bd0bfe940e6a783de1a2ab174b8",
            "placeholder": "​",
            "style": "IPY_MODEL_cb36b2059ab44b1abc53ec9186df6814",
            "value": " 350/350 [00:00&lt;00:00, 46.8kB/s]"
          }
        },
        "1f42e6294d144fed904c61001cd2d45a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8d1955f428b403f93a96a6943a6f94a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2a4d8be05ae43eca3a6ea6655f03582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3f4d9b4087e4898b7f61350f7a16e71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72732fed485d42e7b49e8eb1b6a800dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1170bd0bfe940e6a783de1a2ab174b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb36b2059ab44b1abc53ec9186df6814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff0d1857feff43a2b88211301c4195bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f29e325edde64e2a8460d85c7d4a31be",
              "IPY_MODEL_543633e9d7e64d0c8ade5e7cb4e24e34",
              "IPY_MODEL_180f306246e24d01bb3a0883b2c688aa"
            ],
            "layout": "IPY_MODEL_c14e22e125fd4fdfb985628851beec6b"
          }
        },
        "f29e325edde64e2a8460d85c7d4a31be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10e6f2f694ac4e5a8672a74279754701",
            "placeholder": "​",
            "style": "IPY_MODEL_74beb33967934d5ba7ab6e67f53093ec",
            "value": "vocab.txt: 100%"
          }
        },
        "543633e9d7e64d0c8ade5e7cb4e24e34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58b7a171059c4e83bb54dc358a8c651d",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a98f1b2de82430c8fbadbc33fa9cfa0",
            "value": 231508
          }
        },
        "180f306246e24d01bb3a0883b2c688aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f96d05cc3aa46d78b5473a848e1b169",
            "placeholder": "​",
            "style": "IPY_MODEL_2228c2582d7e479db4967bd0db8c13e9",
            "value": " 232k/232k [00:00&lt;00:00, 1.07MB/s]"
          }
        },
        "c14e22e125fd4fdfb985628851beec6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10e6f2f694ac4e5a8672a74279754701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74beb33967934d5ba7ab6e67f53093ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58b7a171059c4e83bb54dc358a8c651d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a98f1b2de82430c8fbadbc33fa9cfa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f96d05cc3aa46d78b5473a848e1b169": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2228c2582d7e479db4967bd0db8c13e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ede9260f6249448694e8732fc7d0d905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6da6040066254e459681f66c3fd97ed0",
              "IPY_MODEL_bbfcc6a38c5242cea642bba08d25b8a2",
              "IPY_MODEL_1ea39038072d4a878b0f5f8d3f28d63d"
            ],
            "layout": "IPY_MODEL_889f8e568fbf450abb49738545395ce0"
          }
        },
        "6da6040066254e459681f66c3fd97ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afd0fa15b6cd48389c6c824dc3eabf4f",
            "placeholder": "​",
            "style": "IPY_MODEL_69da5725393c46b7b933550c6e907ce9",
            "value": "tokenizer.json: 100%"
          }
        },
        "bbfcc6a38c5242cea642bba08d25b8a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76bfc330e55b47c2b9d4ccf8a90b4265",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac84606f330140259bde954e459a61ee",
            "value": 466247
          }
        },
        "1ea39038072d4a878b0f5f8d3f28d63d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a68e774302e24a3f8ab8358ce07b883e",
            "placeholder": "​",
            "style": "IPY_MODEL_628ff5609f184a46b33b8f4a83fafcdc",
            "value": " 466k/466k [00:00&lt;00:00, 1.07MB/s]"
          }
        },
        "889f8e568fbf450abb49738545395ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afd0fa15b6cd48389c6c824dc3eabf4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69da5725393c46b7b933550c6e907ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76bfc330e55b47c2b9d4ccf8a90b4265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac84606f330140259bde954e459a61ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a68e774302e24a3f8ab8358ce07b883e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "628ff5609f184a46b33b8f4a83fafcdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4527f02956614ed7ba6633ae130d795a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47735b4899594106bfc53c203f67b5bb",
              "IPY_MODEL_8197cc2063694f4491d4bd4f37197f8c",
              "IPY_MODEL_a72af582bd3b4e0fbdb3f45d54649392"
            ],
            "layout": "IPY_MODEL_350a02d010bc4d50afddd2984aa7ee0b"
          }
        },
        "47735b4899594106bfc53c203f67b5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d2b5232d2ce4afa8879aac76c037cd6",
            "placeholder": "​",
            "style": "IPY_MODEL_53f99850e67e4b398042c025ba5add2d",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "8197cc2063694f4491d4bd4f37197f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7b7044cabed4220a8ef96b932c9049a",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75144eebbbf1420785ea0c6928b787fd",
            "value": 112
          }
        },
        "a72af582bd3b4e0fbdb3f45d54649392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6164ff8fe85e4bd3872778ee043c57e5",
            "placeholder": "​",
            "style": "IPY_MODEL_b53dbcc2222f4283aa366c11d1bd58e0",
            "value": " 112/112 [00:00&lt;00:00, 15.2kB/s]"
          }
        },
        "350a02d010bc4d50afddd2984aa7ee0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d2b5232d2ce4afa8879aac76c037cd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53f99850e67e4b398042c025ba5add2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7b7044cabed4220a8ef96b932c9049a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75144eebbbf1420785ea0c6928b787fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6164ff8fe85e4bd3872778ee043c57e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b53dbcc2222f4283aa366c11d1bd58e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "456670d20ca348c986d78b61f479cfa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8d958b1081541f6a8f96fe45164c46b",
              "IPY_MODEL_4aa32b85c71d4d218c51707225007d96",
              "IPY_MODEL_36d334a121d54588bf9890e30092a674"
            ],
            "layout": "IPY_MODEL_6e402d423f65478dbcb681b8396df6e8"
          }
        },
        "e8d958b1081541f6a8f96fe45164c46b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7c1d404f22b4ac697e1b90fe2025e4a",
            "placeholder": "​",
            "style": "IPY_MODEL_677f92b3bf164a1fb28683f4ab974a3c",
            "value": "1_Pooling%2Fconfig.json: 100%"
          }
        },
        "4aa32b85c71d4d218c51707225007d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aac4fbbe7a904ec3b3404f84a972bb18",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_011f7121a5d2485284049740e82765e6",
            "value": 190
          }
        },
        "36d334a121d54588bf9890e30092a674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8773ab8f564644b394523bc9195dd98e",
            "placeholder": "​",
            "style": "IPY_MODEL_ab6b0e3935eb45c9963df8975dbce117",
            "value": " 190/190 [00:00&lt;00:00, 23.7kB/s]"
          }
        },
        "6e402d423f65478dbcb681b8396df6e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7c1d404f22b4ac697e1b90fe2025e4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "677f92b3bf164a1fb28683f4ab974a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aac4fbbe7a904ec3b3404f84a972bb18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "011f7121a5d2485284049740e82765e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8773ab8f564644b394523bc9195dd98e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab6b0e3935eb45c9963df8975dbce117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
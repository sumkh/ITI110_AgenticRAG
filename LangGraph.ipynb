{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumkh/ITI110_AgenticRAG/blob/main/LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVfMT6l1cobl"
      },
      "source": [
        "## PRE-PRODUCTION - AI Tutor Chatbot (Version 2.16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmeoL-_lcs8Y"
      },
      "source": [
        "### Setting Up - Install Requirements (Restart Session after installation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkI4XOWxSXvl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -qU vllm accelerate bitsandbytes langchain-openai langchain-groq huggingface_hub transformers langchain langchain_huggingface langgraph langchain-core langchain-text-splitters langchain-community chromadb langchain-chroma langsmith docling langchain-docling sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_2h8tWUc9yA"
      },
      "source": [
        "### Load Packages and Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lzrhAq9cbbU",
        "outputId": "909403de-b43f-466f-dcbe-1f9ef4e1bbdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-02-19 17:23:47--  https://github.com/sumkh/NYP_Dataset/raw/refs/heads/main/Documents.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sumkh/NYP_Dataset/refs/heads/main/Documents.zip [following]\n",
            "--2025-02-19 17:23:47--  https://raw.githubusercontent.com/sumkh/NYP_Dataset/refs/heads/main/Documents.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18510909 (18M) [application/zip]\n",
            "Saving to: ‘Documents.zip’\n",
            "\n",
            "Documents.zip       100%[===================>]  17.65M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-02-19 17:23:48 (186 MB/s) - ‘Documents.zip’ saved [18510909/18510909]\n",
            "\n",
            "Archive:  /content/Documents.zip\n",
            "   creating: Documents/\n",
            "   creating: Documents/general/\n",
            "  inflating: Documents/general/Topic 1 Introduction to AI and AI on Azure.pdf  \n",
            "  inflating: Documents/general/deeplearningreview.docx  \n",
            "  inflating: Documents/general/slide_1.pptx  \n",
            "   creating: Documents/mcq/\n",
            "  inflating: Documents/mcq/mcq.csv   \n",
            "  inflating: Documents/mcq/mcq2.csv  \n",
            "   creating: general_db/\n",
            "   creating: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/\n",
            "  inflating: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/data_level0.bin  \n",
            "  inflating: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/header.bin  \n",
            "  inflating: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/length.bin  \n",
            " extracting: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/link_lists.bin  \n",
            "  inflating: general_db/chroma.sqlite3  \n",
            "   creating: mcq_db/\n",
            "  inflating: mcq_db/chroma.sqlite3   \n",
            "   creating: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/\n",
            "  inflating: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/data_level0.bin  \n",
            "  inflating: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/header.bin  \n",
            "  inflating: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/length.bin  \n",
            " extracting: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/link_lists.bin  \n",
            "  inflating: requirements.txt        \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "LANGSMITH_API_KEY=userdata.get(\"LANGSMITH_API_KEY\")\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Disable tokenizers parallelism, as it causes issues with multiprocessing\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # LangSmith for Observability\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"AgenticRAG\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY # Optional\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=HF_TOKEN) # May be optional for getting model download from Huggingface\n",
        "\n",
        "# Download required files from Github repo\n",
        "!wget https://github.com/sumkh/NYP_Dataset/raw/refs/heads/main/Documents.zip\n",
        "!unzip -o /content/Documents.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Znk0gm1ce67",
        "outputId": "3889fc12-44ba-407a-aab4-b78d049606e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import json\n",
        "import hashlib\n",
        "import uuid\n",
        "import logging\n",
        "from typing import List, Optional, Union, Literal, Dict\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "# LangChain & related imports\n",
        "from langchain_core.tools import tool, StructuredTool\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever\n",
        "\n",
        "# Extraction for Documents\n",
        "from langchain_docling.loader import ExportType\n",
        "from langchain_docling import DoclingLoader\n",
        "from docling.chunking import HybridChunker\n",
        "\n",
        "# Extraction for HTML\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configurations and Get the API key from the environment variable\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SDpdN9Yck4w"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "#                         Document Extraction Functions\n",
        "# =============================================================================\n",
        "\n",
        "def extract_documents(doc_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Recursively collects all file paths from folder 'doc_path'.\n",
        "    Used by ExtractDocument.load_files() to find documents to parse.\n",
        "    \"\"\"\n",
        "    extracted_docs = []\n",
        "\n",
        "    for root, _, files in os.walk(doc_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            extracted_docs.append(file_path)\n",
        "    return extracted_docs\n",
        "\n",
        "\n",
        "def _generate_uuid(page_content: str) -> str:\n",
        "    \"\"\"Generate a UUID for a chunk of text using MD5 hashing.\"\"\"\n",
        "    md5_hash = hashlib.md5(page_content.encode()).hexdigest()\n",
        "    return str(uuid.UUID(md5_hash[0:32]))\n",
        "\n",
        "\n",
        "def load_file(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load a file from the given path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "\n",
        "    # Load the file and extract the text chunks\n",
        "    try:\n",
        "        loader = DoclingLoader(\n",
        "            file_path = file_path,\n",
        "            export_type = ExportType.DOC_CHUNKS,\n",
        "            chunker = HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
        "        )\n",
        "        docs = loader.load()\n",
        "        logger.info(f\"Total parsed doc-chunks: {len(docs)} from Source: {file_path}\")\n",
        "\n",
        "        for d in docs:\n",
        "            # Tag each document's chunk with the source file and a unique ID\n",
        "            doc = Document(\n",
        "                page_content=d.page_content,\n",
        "                metadata={\n",
        "                    \"source\": file_path,\n",
        "                    \"doc_id\": _generate_uuid(d.page_content),\n",
        "                    \"source_type\": \"file\",\n",
        "                }\n",
        "            )\n",
        "            _documents.append(doc)\n",
        "        logger.info(f\"Total generated LangChain document chunks: {len(_documents)}\\n.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {file_path}. Exception: {e}\\n.\")\n",
        "\n",
        "    return _documents\n",
        "\n",
        "\n",
        "# Define function to load documents from a folder\n",
        "def load_files_from_folder(doc_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load documents from the given folder path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "    # Extract all files path from the given folder\n",
        "    extracted_docs = extract_documents(doc_path)\n",
        "\n",
        "    # Iterate through each document and extract the text chunks\n",
        "    for file_path in extracted_docs:\n",
        "        _documents.extend(load_file(file_path))\n",
        "\n",
        "    return _documents\n",
        "\n",
        "# =============================================================================\n",
        "# Load structured data in csv file to LangChain Document format\n",
        "def load_mcq_csvfiles(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load structured data in mcq csv file from the given file path and return a list of Document object.\n",
        "    Expected format: each row of csv is comma separated into \"mcq_number\", \"mcq_type\", \"text_content\"\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "\n",
        "    # iterate through each csv file and load each row into _dict_per_question format\n",
        "    # Ensure we process only CSV files\n",
        "    if not file_path.endswith(\".csv\"):\n",
        "        return _documents  # Skip non-CSV files\n",
        "    try:\n",
        "        # Open and read the CSV file\n",
        "        with open(file_path, mode='r', encoding='utf-8') as file:\n",
        "            reader = csv.DictReader(file)\n",
        "            for row in reader:\n",
        "                # Ensure required columns exist in the row\n",
        "                if not all(k in row for k in [\"mcq_number\", \"mcq_type\", \"text_content\"]): # Ensure required columns exist and exclude header\n",
        "                    logger.error(f\"Skipping row due to missing fields: {row}\")\n",
        "                    continue\n",
        "                # Tag each row of csv is comma separated into \"mcq_number\", \"mcq_type\", \"text_content\"\n",
        "                doc = Document(\n",
        "                    page_content = row[\"text_content\"], # text_content segment is separated by \"|\"\n",
        "                    metadata={\n",
        "                        \"source\": f\"{file_path}_{row['mcq_number']}\",  # file_path + mcq_number\n",
        "                        \"doc_id\": _generate_uuid(f\"{file_path}_{row['mcq_number']}\"),  # Unique ID\n",
        "                        \"source_type\": row[\"mcq_type\"],  # MCQ type\n",
        "                    }\n",
        "                )\n",
        "                _documents.append(doc)\n",
        "            logger.info(f\"Successfully loaded {len(_documents)} LangChain document chunks from {file_path}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {file_path}. Exception: {e}\\n.\")\n",
        "\n",
        "    return _documents\n",
        "\n",
        "# Define function to load documents from a folder for structured data in csv file\n",
        "def load_files_from_folder_mcq(doc_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load mcq csv file from the given folder path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "    # Extract all files path from the given folder\n",
        "    extracted_docs = [\n",
        "        os.path.join(doc_path, file) for file in os.listdir(doc_path)\n",
        "        if file.endswith(\".csv\")  # Process only CSV files\n",
        "    ]\n",
        "\n",
        "    # Iterate through each document and extract the text chunks\n",
        "    for file_path in extracted_docs:\n",
        "        _documents.extend(load_mcq_csvfiles(file_path))\n",
        "\n",
        "    return _documents\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                         Website Extraction Functions\n",
        "# =============================================================================\n",
        "def _generate_uuid(page_content: str) -> str:\n",
        "    \"\"\"Generate a UUID for a chunk of text using MD5 hashing.\"\"\"\n",
        "    md5_hash = hashlib.md5(page_content.encode()).hexdigest()\n",
        "    return str(uuid.UUID(md5_hash[0:32]))\n",
        "\n",
        "def ensure_scheme(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    if not parsed_url.scheme:\n",
        "        return 'http://' + url  # Default to http, or use 'https://' if preferred\n",
        "    return url\n",
        "\n",
        "def extract_html(url: List[str]) -> List[Document]:\n",
        "    if isinstance(url, str):\n",
        "        url = [url]\n",
        "    \"\"\"\n",
        "    Extracts text from the HTML content of web pages listed in 'web_path'.\n",
        "    Returns a list of LangChain 'Document' objects.\n",
        "    \"\"\"\n",
        "    # Ensure all URLs have a scheme\n",
        "    web_paths = [ensure_scheme(u) for u in url]\n",
        "\n",
        "    loader = WebBaseLoader(web_paths)\n",
        "    loader.requests_per_second = 1\n",
        "    docs = loader.load()\n",
        "\n",
        "    # Iterate through each document, clean the content, removing excessive line return and store it in a LangChain Document\n",
        "    _documents = []\n",
        "    for doc in docs:\n",
        "        # Clean the concent\n",
        "        doc.page_content = doc.page_content.strip()\n",
        "        doc.page_content = doc.page_content.replace(\"\\n\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"\\r\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"\\t\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"  \", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"   \", \" \")\n",
        "\n",
        "        # Store it in a LangChain Document\n",
        "        web_doc = Document(\n",
        "            page_content=doc.page_content,\n",
        "            metadata={\n",
        "                \"source\": doc.metadata.get(\"source\"),\n",
        "                \"doc_id\": _generate_uuid(doc.page_content),\n",
        "                \"source_type\": \"web\"\n",
        "            }\n",
        "        )\n",
        "        _documents.append(web_doc)\n",
        "    return _documents\n",
        "\n",
        "# =============================================================================\n",
        "#                         Vector Store Initialisation\n",
        "# =============================================================================\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
        "\n",
        "# Initialise vector stores\n",
        "general_vs = Chroma(\n",
        "    collection_name=\"general_vstore\",\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"./general_db\"\n",
        ")\n",
        "\n",
        "mcq_vs = Chroma(\n",
        "    collection_name=\"mcq_vstore\",\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"./mcq_db\"\n",
        ")\n",
        "\n",
        "in_memory_vs = Chroma(\n",
        "    collection_name=\"in_memory_vstore\",\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "# Split the documents into smaller chunks for better embedding coverage\n",
        "def split_text_into_chunks(docs: List[Document]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Splits a list of Documents into smaller text chunks using\n",
        "    RecursiveCharacterTextSplitter while preserving metadata.\n",
        "    Returns a list of Document objects.\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000, # Split into chunks of 1000 characters\n",
        "        chunk_overlap=200, # Overlap by 200 characters\n",
        "        add_start_index=True\n",
        "    )\n",
        "    chunked_docs = splitter.split_documents(docs)\n",
        "    return chunked_docs # List of Document objects\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                         Retrieval Tools\n",
        "# =============================================================================\n",
        "\n",
        "# Define a simple similarity search retrieval tool on msq_vs\n",
        "class MCQRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"Search topic.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def mcq_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Retrieve the top k most similar mcq question documents from the vector store\n",
        "    docs_func = mcq_vs.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\n",
        "        'k': k,\n",
        "        'filter':{\"source_type\": \"mcq_question\"}\n",
        "    },\n",
        "    )\n",
        "    docs_qns = docs_func.invoke(input, k=k)\n",
        "\n",
        "    # Extract the document IDs from the retrieved documents\n",
        "    doc_ids = [d.metadata.get(\"doc_id\") for d in docs_qns if \"doc_id\" in d.metadata]\n",
        "\n",
        "    # Retrieve full documents based on the doc_ids\n",
        "    docs = mcq_vs.get(where = {'doc_id': {\"$in\":doc_ids}})\n",
        "\n",
        "    qns_list = {}\n",
        "    for i, d in enumerate(docs['metadatas']):\n",
        "        qns_list[d['source'] + \" \" + d['source_type']] = docs['documents'][i]\n",
        "\n",
        "    return qns_list\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "mcq_retriever_tool = StructuredTool.from_function(\n",
        "    func = mcq_retriever,\n",
        "    name = \"MCQ Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve MCQ questions set when Human asks to generate a quiz related to a topic.\n",
        "    DO NOT GIVE THE ANSWERS to Human before Human has answered all the questions.\n",
        "\n",
        "    If Human give answers for questions you do not know, SAY you do not have the questions for the answer\n",
        "    and ASK if the Human want you to generate a new quiz and then SAVE THE QUIZ with Summary Tool before ending the conversation.\n",
        "\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The search topic to retrieve MCQ questions set related to the topic.\n",
        "        - k (int): Number of question set to retrieve.\n",
        "        Example usage: input='What is AI?', k=5\n",
        "\n",
        "    Returns:\n",
        "    - A dict of MCQ questions:\n",
        "    Key: 'metadata of question' e.g. './Documents/mcq/mcq.csv_Qn31 mcq_question' with suffix ['question', 'answer', 'answer_reason', 'options', 'wrong_options_reason']\n",
        "    Value: Text Content\n",
        "\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = MCQRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Return the response as a list of strings\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Retrieve more documents with higher diversity using MMR (Maximal Marginal Relevance) from the general vector store\n",
        "# Useful if the dataset has many similar documents\n",
        "class GenRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def gen_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = general_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "general_retriever_tool = StructuredTool.from_function(\n",
        "    func = gen_retriever,\n",
        "    name = \"Assistant References Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve reference information from Assistant reference database for Human queries related to a topic or\n",
        "    and when Human asked to generate guides to learn or study about a topic.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "        Example usage: input='What is AI?', k=5\n",
        "    Returns:\n",
        "    - A list of retrieved document's content string.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = GenRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Return the content of the documents\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Retrieve more documents with higher diversity using MMR (Maximal Marginal Relevance) from the in-memory vector store\n",
        "# Query in-memory vector store only\n",
        "class InMemoryRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def in_memory_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "in_memory_retriever_tool = StructuredTool.from_function(\n",
        "    func = in_memory_retriever,\n",
        "    name = \"In-Memory Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool when Human ask Assistant to retrieve information from documents that Human has uploaded.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = InMemoryRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Whether to return the tool’s output directly\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Web Extraction Tool\n",
        "class WebExtractionRequest(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"Search text.\")\n",
        "    url: str = Field(\n",
        "        ...,\n",
        "        title=\"url\",\n",
        "        description=\"Web URL(s) to extract content from. If multiple URLs, separate them with a comma.\"\n",
        "    )\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "# Extract content from a web URL, load into in_memory_vstore\n",
        "def extract_web_path_tool(input: str, url: str, k: int = 5) -> List[str]:\n",
        "    if isinstance(url, str):\n",
        "        url = [url]\n",
        "    \"\"\"\n",
        "    Extract content from the web URLs based on user's input.\n",
        "    Args:\n",
        "    - input: The input text to search for.\n",
        "    - url: URLs to extract content from.\n",
        "    - k: Number of results to retrieve.\n",
        "    Returns:\n",
        "     - A list of retrieved document's content string.\n",
        "    \"\"\"\n",
        "    # Extract content from the web\n",
        "    html_docs = extract_html(url)\n",
        "    if not html_docs:\n",
        "        return f\"No content extracted from {url}.\"\n",
        "\n",
        "    # Split the documents into smaller chunks for better embedding coverage\n",
        "    chunked_texts = split_text_into_chunks(html_docs)\n",
        "    in_memory_vs.add_documents(chunked_texts) # Add the chunked texts to the in-memory vector store\n",
        "\n",
        "    # Extract content from the in-memory vector store\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\n",
        "        'k': k,\n",
        "        'lambda_mult': 0.25,\n",
        "        'filter':{\"source\": {\"$in\": url}}\n",
        "    },\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "web_extraction_tool = StructuredTool.from_function(\n",
        "    func = extract_web_path_tool,\n",
        "    name = \"Web Extraction Tool\",\n",
        "    description = (\n",
        "        \"Assistant should use this tool to extract content from web URLs based on user's input, \"\n",
        "        \"Web extraction is initially load into database and then return k: Number of results to retrieve\"\n",
        "    ),\n",
        "    args_schema = WebExtractionRequest,\n",
        "    return_direct = False, # Whether to return the tool’s output directly\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Ensemble Retrieval from General and In-Memory Vector Stores\n",
        "class EnsembleRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"Number of results.\")\n",
        "\n",
        "def ensemble_retriever(input: str, k: int = 5) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    general_retrieval = general_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    in_memory_retrieval = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "        retrievers=[general_retrieval, in_memory_retrieval],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "    docs = ensemble_retriever.invoke(input)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "ensemble_retriever_tool = StructuredTool.from_function(\n",
        "    func = ensemble_retriever,\n",
        "    name = \"Ensemble Retriever Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve information from reference database and\n",
        "    extraction of documents that Human has uploaded.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = EnsembleRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuhaRfR6dLno"
      },
      "source": [
        "### Load vLLM Model and Serving Online using OpenAI Wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upf7Jhj5KkIZ"
      },
      "source": [
        "#### Run vLLM SubProcess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "226HJOl4KmWc",
        "outputId": "c2ca8009-23a9-4e8f-ffaa-5b21e37b35b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ],
      "source": [
        "# https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/#-tool-calling-(8b/70b/405b)-\n",
        "# https://medium.com/@hakimnaufal/trying-out-vllm-deepseek-r1-in-google-colab-a-quick-guide-a4fe682b8665\n",
        "# https://github.com/naufalhakim23/deepseek-r1-playground/blob/main/deepseek_r1_distill_qwen_fast_api.ipynb\n",
        "# https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/vllm_inference_engine.ipynb\n",
        "# https://docs.vllm.ai/_/downloads/en/v0.4.2/pdf/\n",
        "# https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
        "# https://docs.vllm.ai/en/latest/serving/env_vars.html\n",
        "# https://docs.vllm.ai/en/latest/deployment/docker.html\n",
        "# https://docs.vllm.ai/en/latest/features/quantization/bnb.html\n",
        "\n",
        "# https://huggingface.co/casperhansen/llama-3-8b-instruct-awq/tree/main\n",
        "# https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit\n",
        "\n",
        "!wget -q -P examples/ https://github.com/vllm-project/vllm/raw/refs/heads/main/examples/tool_chat_template_llama3.1_json.jinja\n",
        "\n",
        "# we prepend \"nohup\" and postpend \"&\" to make the Colab cell run in background\n",
        "! nohup python -m vllm.entrypoints.openai.api_server \\\n",
        "                  --model unsloth/llama-3-8b-Instruct-bnb-4bit \\\n",
        "                  --enable-auto-tool-choice \\\n",
        "                  --tool-call-parser llama3_json \\\n",
        "                  --chat-template examples/tool_chat_template_llama3.1_json.jinja \\\n",
        "                  --quantization bitsandbytes \\\n",
        "                  --load-format bitsandbytes \\\n",
        "                  --dtype half \\\n",
        "                  --max-model-len 8192 \\\n",
        "                  --download-dir models/vllm \\\n",
        "                  > vllm.log &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29Eis09vLIyZ",
        "outputId": "2f4019a7-6237-405d-a493-cb961762c516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-19 17:24:35 api_server.py:206] Started engine process with PID 3503\n",
            "WARNING 02-19 17:24:37 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "E0000 00:00:1739985883.719463    3503 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "WARNING 02-19 17:24:50 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "WARNING 02-19 17:24:50 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "WARNING 02-19 17:24:59 config.py:621] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
            "INFO 02-19 17:25:05 config.py:542] This model supports multiple tasks: {'reward', 'score', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-19 17:25:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "INFO 02-19 17:28:13 model_runner.py:1115] Loading model weights took 5.3131 GB\n",
            "INFO 02-19 17:28:13 model_runner.py:1115] Loading model weights took 5.3131 GB\n",
            "INFO 02-19 17:28:13 model_runner.py:1115] Loading model weights took 5.3131 GB\n",
            "INFO 02-19 17:28:13 model_runner.py:1115] Loading model weights took 5.3131 GB\n",
            "INFO 02-19 17:28:13 model_runner.py:1115] Loading model weights took 5.3131 GB\n",
            "INFO 02-19 17:28:13 model_runner.py:1115] Loading model weights took 5.3131 GB\n",
            "INFO 02-19 17:28:13 model_runner.py:1115] Loading model weights took 5.3131 GB\n",
            "INFO 02-19 17:28:13 model_runner.py:1115] Loading model weights took 5.3131 GB\n",
            "INFO 02-19 17:28:56 worker.py:267] model weights take 5.31GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 6.67GiB.\n",
            "Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [01:41<00:02,  2.14s/it]"
          ]
        }
      ],
      "source": [
        "# we check the logs until the server has been started correctly\n",
        "!while ! grep -q \"Application startup complete\" vllm.log; do tail -n 1 vllm.log; sleep 5; done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlJpisBPGUsC"
      },
      "source": [
        "find the process ID (PID) using a command like ps aux | grep vllm and then kill it using kill -9 <PID>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaqw6s9FETtA",
        "outputId": "459fd40a-7dae-45cb-c340-7c86bd5c74a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root        3440  3.5  8.5 6548852 1130792 ?     Sl   17:24   0:13 python3 -m vllm.entrypoints.opena\n",
            "root        5378  0.0  0.0   7376  3452 ?        S    17:30   0:00 /bin/bash -c ps aux | grep vllm\n",
            "root        5380  0.0  0.0   6484  2280 ?        S    17:30   0:00 grep vllm\n"
          ]
        }
      ],
      "source": [
        "# Find the process ID (PID)\n",
        "!ps aux | grep vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMDeFYy0Gjlt"
      },
      "outputs": [],
      "source": [
        "# To kill the process, look for the first set of digits\n",
        "#!kill -9 2120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k-GdaBnm2W6",
        "outputId": "8a1ae65d-d123-447b-fb58-eef9ae06f2c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vllm server is running\n",
            "The vllm server is ready to serve.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def check_vllm_status():\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:8000/health\")\n",
        "        if response.status_code == 200:\n",
        "            print(\"vllm server is running\")\n",
        "            return True\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"vllm server is not running\")\n",
        "        return False\n",
        "\n",
        "try:\n",
        "    # Monitor the process\n",
        "    while True:\n",
        "        if check_vllm_status() == True:\n",
        "            print(\"The vllm server is ready to serve.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"The vllm server has stopped.\")\n",
        "            stdout, stderr = vllm_process.communicate(timeout=10)\n",
        "            print(f\"STDOUT: {stdout.decode('utf-8')}\")\n",
        "            print(f\"STDERR: {stderr.decode('utf-8')}\")\n",
        "            break\n",
        "        time.sleep(5)  # Check every second\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Stopping the check of vllm...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23Gn89-wNN4b",
        "outputId": "fb08174c-36c4-4ca6-c332-ccec3918ff10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Why did the neural network go to therapy?\\n\\nBecause it had a lot of \"hidden layers\" and was struggling to \"backpropagate\" its emotions!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 52, 'total_tokens': 84, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'unsloth/llama-3-8b-Instruct-bnb-4bit', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-31488c76-daeb-4a16-a5fb-e222675c6d39-0', usage_metadata={'input_tokens': 52, 'output_tokens': 32, 'total_tokens': 84, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(\n",
        "    model=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    temperature=0.5,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    api_key=\"not_required\",\n",
        "    base_url=\"http://localhost:8000/v1\",\n",
        "    # organization=\"...\",\n",
        "    # other params...\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant and study companion.\",\n",
        "    ),\n",
        "    (\"human\", \"Tell me a joke about Deep Learning.\"),\n",
        "]\n",
        "ai_msg = model.invoke(messages)\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnyb6pAxLmzj"
      },
      "source": [
        "### Using Docker Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rcpGtb3uAB5"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "docker run \\\n",
        "  --runtime nvidia --gpus all \\\n",
        "  --name ITI110_vllm_container \\\n",
        "  -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
        "  --env \"HUGGING_FACE_HUB_TOKEN=$SECRETS/HF_TOKEN\" \\\n",
        "  -p 8000:8000 \\\n",
        "  --ipc=host \\\n",
        "  -v /content/examples:/examples \\ # Mount the directory\n",
        "  vllm/vllm-openai:latest \\\n",
        "  vllm.entrypoints.openai.api_server \\  # Start the vllm server explicitly\n",
        "    --model unsloth/llama-3-8b-Instruct-bnb-4bit \\\n",
        "    --enable-auto-tool-choice \\\n",
        "    --tool-call-parser llama3_json \\\n",
        "    --chat-template examples/tool_chat_template_llama3.1_json.jinja \\\n",
        "    --quantization bitsandbytes \\\n",
        "    --load-format bitsandbytes \\\n",
        "    --dtype half \\\n",
        "    --max-model-len 8192 \\\n",
        "    --download-dir models/vllm \\\n",
        "    > vllm.log &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiWuRZp3LrT2"
      },
      "outputs": [],
      "source": [
        "# Load and run the model:\n",
        "!docker exec -it my_vllm_container bash -c \"vllm serve unsloth/llama-3-8b-Instruct-bnb-4bit\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77OsFI4ULvID"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Call the server using curl:\n",
        "curl -X POST \"http://localhost:8000/v1/chat/completions\" \\\n",
        "\t-H \"Content-Type: application/json\" \\\n",
        "\t--data {\n",
        "\t\t\"model\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "\t\t\"messages\": [\n",
        "\t\t\t{\n",
        "\t\t\t\t\"role\": \"user\",\n",
        "\t\t\t\t\"content\": \"What is the capital of France?\"\n",
        "\t\t\t}\n",
        "\t\t]\n",
        "\t}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7soxxBDs_qr"
      },
      "source": [
        "### GROQ Serving (For Test References)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyup3FLGDE34",
        "outputId": "eeb67cb3-8630-4732-c1c1-802bf88ceef8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"<think>\\n\\n</think>\\n\\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 7, 'total_tokens': 51, 'completion_time': 0.16, 'prompt_time': 0.003501381, 'queue_time': 0.234579366, 'total_time': 0.163501381}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_492bd52206', 'finish_reason': 'stop', 'logprobs': None} id='run-013d938d-b492-4bb4-9490-de82fb8463a9-0' usage_metadata={'input_tokens': 7, 'output_tokens': 44, 'total_tokens': 51}\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Initialize Groq LLM\n",
        "model = ChatGroq(\n",
        "    model_name=\"deepseek-r1-distill-llama-70b\",   #\"llama-3.2-3b-preview\", \"deepseek-r1-distill-llama-70b\"\n",
        "    temperature=0.6,\n",
        "    api_key=GROQ_API_KEY,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(model.invoke(\"Who are you?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4cHTN0bHivs"
      },
      "source": [
        "### LangGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "SXMyAFM6bSiz",
        "outputId": "53735a56-bba2-4ca9-87d9-10c77b2c0575"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "from typing import (\n",
        "    Annotated,\n",
        "    Sequence,\n",
        "    TypedDict,\n",
        "    List,\n",
        ")\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# LangChain / LangGraph imports\n",
        "from langchain_core.messages import (\n",
        "    SystemMessage,\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    BaseMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "from langchain_core.tools import StructuredTool\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "from langgraph.prebuilt import InjectedStore\n",
        "from langgraph.store.base import BaseStore\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain.embeddings import init_embeddings\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "###############################################################################\n",
        "# 1. Initialize memory + config\n",
        "###############################################################################\n",
        "in_memory_store = InMemoryStore(\n",
        "    index={\n",
        "        \"embed\": init_embeddings(\"huggingface:sentence-transformers/all-MiniLM-L6-v2\"),\n",
        "        \"dims\": 384,  # Embedding dimensions\n",
        "    }\n",
        ")\n",
        "\n",
        "# A memory saver to checkpoint conversation states\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "# Initialize config with user & thread info\n",
        "config = {}\n",
        "config[\"configurable\"] = {\n",
        "    \"user_id\": \"user_1\",\n",
        "    \"thread_id\": 0,\n",
        "}\n",
        "\n",
        "###############################################################################\n",
        "# 2. Define MessagesState\n",
        "###############################################################################\n",
        "class MessagesState(TypedDict):\n",
        "    \"\"\"The state of the agent.\n",
        "\n",
        "    The key 'messages' uses add_messages as a reducer,\n",
        "    so each time this state is updated, new messages are appended.\n",
        "    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 3. Memory Tools\n",
        "###############################################################################\n",
        "def save_memory(summary_text: str, *, config: RunnableConfig, store: BaseStore) -> str:\n",
        "    \"\"\"Save the given memory for the current user and return the key.\"\"\"\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    thread_id = config.get(\"configurable\", {}).get(\"thread_id\")\n",
        "    namespace = (user_id, \"memories\")\n",
        "    memory_id = thread_id\n",
        "    store.put(namespace, memory_id, {\"memory\": summary_text})\n",
        "    return f\"Saved to memory key: {memory_id}\"\n",
        "\n",
        "def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
        "    # Extract the messages list from the event, handling potential missing key\n",
        "    messages = state[\"messages\"]\n",
        "    # Convert LangChain messages to dictionaries before storing\n",
        "    messages_dict = [{\"role\": msg.type, \"content\": msg.content} for msg in messages]\n",
        "\n",
        "    # Get the user id from the config\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    thread_id = config.get(\"configurable\", {}).get(\"thread_id\")\n",
        "    # Namespace the memory\n",
        "    namespace = (user_id, \"memories\")\n",
        "    # Create a new memory ID\n",
        "    memory_id = f\"{thread_id}\"\n",
        "    store.put(namespace, memory_id, {\"memory\": messages_dict})\n",
        "    return f\"Saved to memory key: {memory_id}\"\n",
        "\n",
        "\n",
        "# Define a Pydantic schema for the save_memory tool (if needed elsewhere)\n",
        "# https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.InMemoryStore.html\n",
        "class RecallMemory(BaseModel):\n",
        "    query_text: str = Field(..., title=\"Search Text\", description=\"The text to search from memories for similar records.\")\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"Number of results to retrieve.\")\n",
        "\n",
        "def recall_memory(query_text: str, k: int = 5) -> str:\n",
        "    \"\"\"Retrieve user memories from in_memory_store.\"\"\"\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    memories = [\n",
        "        m.value[\"memory\"] for m in in_memory_store.search((user_id, \"memories\"), query=query_text, limit=k)\n",
        "        if \"memory\" in m.value\n",
        "    ]\n",
        "    return f\"User memories: {memories}\"\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "recall_memory_tool = StructuredTool.from_function(\n",
        "    func=recall_memory,\n",
        "    name=\"Recall Memory Tool\",\n",
        "    description=\"\"\"\n",
        "      Retrieve memories relevant to the user's query.\n",
        "      \"\"\",\n",
        "    args_schema=RecallMemory,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "###############################################################################\n",
        "# 4. Summarize Node (using StructuredTool)\n",
        "###############################################################################\n",
        "# Define a Pydantic schema for the Summary tool\n",
        "class SummariseConversation(BaseModel):\n",
        "    summary_text: str = Field(..., title=\"text\", description=\"Write a summary of entire conversation here\")\n",
        "\n",
        "def summarise_node(summary_text: str):\n",
        "    \"\"\"\n",
        "    Final node that summarizes the entire conversation for the current thread,\n",
        "    saves it in memory, increments the thread_id, and ends the conversation.\n",
        "    Returns a confirmation string.\n",
        "    \"\"\"\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "    current_thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "    new_thread_id = str(int(current_thread_id) + 1)\n",
        "\n",
        "    # Prepare configuration for saving memory with updated thread id\n",
        "    config_for_saving = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": user_id,\n",
        "            \"thread_id\": new_thread_id\n",
        "        }\n",
        "    }\n",
        "    key = save_memory(summary_text, config=config_for_saving, store=in_memory_store)\n",
        "    #return f\"Summary saved under key: {key}\"\n",
        "\n",
        "# Create a StructuredTool from the function (this wraps summarise_node)\n",
        "summarise_tool = StructuredTool.from_function(\n",
        "    func=summarise_node,\n",
        "    name=\"Summary Tool\",\n",
        "    description=\"\"\"\n",
        "      Summarize the current conversation in no more than\n",
        "      1000 words. Also retain any unanswered quiz questions along with\n",
        "      your internal answers so the next conversation thread can continue.\n",
        "      Do not reveal solutions to the user yet. Use this tool to save\n",
        "      the current conversation to memory and then end the conversation.\n",
        "      \"\"\",\n",
        "    args_schema=SummariseConversation,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "def call_summary(state: MessagesState, config: RunnableConfig):\n",
        "    # Convert message dicts to HumanMessage instances if needed.\n",
        "    system_message=\"\"\"\n",
        "      Summarize the current conversation in no more than\n",
        "      1000 words. Also retain any unanswered quiz questions along with\n",
        "      your internal answers.\n",
        "      \"\"\"\n",
        "    messages = []\n",
        "    for m in state[\"messages\"]:\n",
        "        if isinstance(m, dict):\n",
        "            # Use role from dict (defaulting to 'user' if missing)\n",
        "            messages.append(AIMessage(content=system_message, role=m.get(\"role\", \"assistant\")))\n",
        "        else:\n",
        "            messages.append(m)\n",
        "\n",
        "    summaries = llm_with_tools.invoke(messages)\n",
        "\n",
        "    summary_content = summaries.content\n",
        "\n",
        "    # Call Tool Manually\n",
        "    message_with_single_tool_call = AIMessage(\n",
        "        content=\"\",\n",
        "        tool_calls=[\n",
        "            {\n",
        "                \"name\": \"Summary Tool\",\n",
        "                \"args\": {\"summary_text\": summary_content},\n",
        "                \"id\": \"tool_call_id\",\n",
        "                \"type\": \"tool_call\",\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    tool_node.invoke({\"messages\": [message_with_single_tool_call]})\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 5. Build the Graph\n",
        "###############################################################################\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "\n",
        "# Use the built-in ToolNode from langgraph that calls any declared tools.\n",
        "tools = [\n",
        "    mcq_retriever_tool,\n",
        "    web_extraction_tool,\n",
        "    ensemble_retriever_tool,\n",
        "    general_retriever_tool,\n",
        "    in_memory_retriever_tool,\n",
        "    recall_memory_tool,\n",
        "    summarise_tool,\n",
        "]\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "#end_node = ToolNode(tools=[summarise_tool])\n",
        "\n",
        "# Wrap your model with tools\n",
        "llm_with_tools = model.bind_tools(tools)\n",
        "\n",
        "###############################################################################\n",
        "# 6. The agent's main node: call_model\n",
        "###############################################################################\n",
        "def call_model(state: MessagesState, config: RunnableConfig):\n",
        "    \"\"\"\n",
        "    The main agent node that calls the LLM with the user + system messages.\n",
        "    Since our vLLM chat wrapper expects a list of BaseMessage objects,\n",
        "    we convert any dict messages to HumanMessage objects.\n",
        "    If the LLM requests a tool call, we'll route to the 'tools' node next\n",
        "    (depending on the condition).\n",
        "    \"\"\"\n",
        "    # Convert message dicts to HumanMessage instances if needed.\n",
        "    messages = []\n",
        "    for m in state[\"messages\"]:\n",
        "        if isinstance(m, dict):\n",
        "            # Use role from dict (defaulting to 'user' if missing)\n",
        "            messages.append(HumanMessage(content=m.get(\"content\", \"\"), role=m.get(\"role\", \"user\")))\n",
        "        else:\n",
        "            messages.append(m)\n",
        "\n",
        "    # Invoke the LLM (with tools) using the converted messages.\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "\n",
        "def call_summary(state: MessagesState, config: RunnableConfig):\n",
        "    # Convert message dicts to HumanMessage instances if needed.\n",
        "    system_message=\"\"\"\n",
        "      Summarize the current conversation in no more than\n",
        "      1000 words. Also retain any unanswered quiz questions along with\n",
        "      your internal answers.\n",
        "      \"\"\"\n",
        "    messages = []\n",
        "    for m in state[\"messages\"]:\n",
        "        if isinstance(m, dict):\n",
        "            # Use role from dict (defaulting to 'user' if missing)\n",
        "            messages.append(AIMessage(content=system_message, role=m.get(\"role\", \"assistant\")))\n",
        "        else:\n",
        "            messages.append(m)\n",
        "\n",
        "    summaries = llm_with_tools.invoke(messages)\n",
        "\n",
        "    summary_content = summaries.content\n",
        "\n",
        "    # Call Tool Manually\n",
        "    message_with_single_tool_call = AIMessage(\n",
        "        content=\"\",\n",
        "        tool_calls=[\n",
        "            {\n",
        "                \"name\": \"Summary Tool\",\n",
        "                \"args\": {\"summary_text\": summary_content},\n",
        "                \"id\": \"tool_call_id\",\n",
        "                \"type\": \"tool_call\",\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    tool_node.invoke({\"messages\": [message_with_single_tool_call]})\n",
        "\n",
        "###############################################################################\n",
        "# 7. Add Nodes & Edges, Then Compile\n",
        "###############################################################################\n",
        "graph_builder.add_node(\"agent\", call_model)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "#graph_builder.add_node(\"summary\", call_summary)\n",
        "\n",
        "# Entry point\n",
        "graph_builder.set_entry_point(\"agent\")\n",
        "\n",
        "# def custom_tools_condition(llm_output: dict) -> str:\n",
        "#     \"\"\"Return which node to go to next based on the LLM output.\"\"\"\n",
        "\n",
        "#     # The LLM's JSON might have a field like {\"name\": \"Recall Memory Tool\", \"arguments\": {...}}.\n",
        "#     tool_name = llm_output.get(\"name\", None)\n",
        "\n",
        "#     # If the LLM calls \"Summary Tool\", jump directly to the 'summary' node\n",
        "#     if tool_name == \"Summary Tool\":\n",
        "#         return \"summary\"\n",
        "\n",
        "#     # If the LLM calls any other recognized tool, go to 'tools'\n",
        "#     valid_tool_names = [t.name for t in tools]  # all tools in the main tool_node\n",
        "#     if tool_name in valid_tool_names:\n",
        "#         return \"tools\"\n",
        "\n",
        "#     # If there's no recognized tool name, assume we're done => go to summary\n",
        "#     return \"__end__\"\n",
        "\n",
        "# graph_builder.add_conditional_edges(\n",
        "#     \"agent\",\n",
        "#     custom_tools_condition,\n",
        "#     {\n",
        "#         \"tools\": \"tools\",\n",
        "#         \"summary\": \"summary\",\n",
        "#         \"__end__\": \"summary\",\n",
        "#     }\n",
        "# )\n",
        "\n",
        "# If LLM requests a tool, go to \"tools\", otherwise go to \"summary\"\n",
        "graph_builder.add_conditional_edges(\"agent\", tools_condition)\n",
        "#graph_builder.add_conditional_edges(\"agent\", tools_condition, {\"tools\": \"tools\", \"__end__\": \"summary\"})\n",
        "#graph_builder.add_conditional_edges(\"agent\", lambda llm_output: \"tools\" if llm_output.get(\"name\", None) in [t.name for t in tools] else \"summary\", {\"tools\": \"tools\", \"__end__\": \"summary\"}\n",
        "\n",
        "# If we used a tool, return to the agent for final answer or more tools\n",
        "graph_builder.add_edge(\"tools\", \"agent\")\n",
        "#graph_builder.add_edge(\"agent\", \"summary\")\n",
        "#graph_builder.set_finish_point(\"summary\")\n",
        "\n",
        "# Compile the graph with checkpointing and persistent store\n",
        "graph = graph_builder.compile(checkpointer=checkpointer, store=in_memory_store)\n",
        "\n",
        "#from langgraph.prebuilt import create_react_agent\n",
        "#graph = create_react_agent(llm_with_tools, tools=tool_node, checkpointer=checkpointer, store=in_memory_store)\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NGkUXHEiKofB",
        "outputId": "e5151c32-d4ce-4500-8276-501f0da22c60"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'User memories: '"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "recall_memory(\"Deep Learning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLxZHO87Hivt"
      },
      "source": [
        "#### Testing on Tool Calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEz4NvcD8S3O",
        "outputId": "94bf036c-f500-4329-81da-74889e69d694"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [ToolMessage(content='[\"Deep Learning\\\\na machine learning technique in which layers of neural networks are used to process data and make decisions\", \"How Do Children Learn?\\\\nThe Deep Learning Revolution\"]', name='Assistant References Retrieval Tool', tool_call_id='tool_call_id')]}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test to Call Tool Manually\n",
        "message_with_single_tool_call = AIMessage(\n",
        "    content=\"\",\n",
        "    tool_calls=[\n",
        "        {\n",
        "            \"name\": \"Assistant References Retrieval Tool\",\n",
        "            \"args\": {\"input\": \"Deep Learning\"},\n",
        "            \"id\": \"tool_call_id\",\n",
        "            \"type\": \"tool_call\",\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "tool_node.invoke({\"messages\": [message_with_single_tool_call]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7Bqlb3-tl_m",
        "outputId": "0b1fd231-cffd-4397-b8e2-22dac5a31632"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'Web Extraction Tool',\n",
              "  'args': {'input': 'Deep Learning',\n",
              "   'url': 'https://www.ibm.com/think/topics/artificial-intelligence',\n",
              "   'k': 5},\n",
              "  'id': 'chatcmpl-tool-079683f0cf764365b75627feafd48f7a',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Replacing the JSON with Tool calls\n",
        "llm_with_tools.invoke(question_1).tool_calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8cf5Jp5utElU",
        "outputId": "c0e6a6b6-9f12-4918-bc1c-34a7382219c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [ToolMessage(content='{\"./Documents/mcq/mcq.csv_Qn5 mcq_question\": \"Your organization is planning to deploy a new Azure AI solution to automate the processing of customer inquiries received through various communication channels, including email, chat, and social media. As part of the deployment process, you are tasked with creating an Azure AI resource to host the required services. The solution should be scalable, cost-effective, and comply with Responsible AI principles. Which steps should you follow to create the Azure AI resource while ensuring scalability, cost optimization, and compliance with Responsible AI principles? Select all answers that apply.\", \"./Documents/mcq/mcq.csv_Qn5 mcq_answer\": \"Answer - [A, C, D, E]\", \"./Documents/mcq/mcq.csv_Qn5 mcq_answer_reason\": \"Option A - Choosing an appropriate pricing tier and region ensures cost optimization and scalability of the Azure AI resource.|Option C - Defining resource tags helps categorize and track usage and costs, aiding in financial management.|Option D - Specifying required Azure AI services ensures the solution meets business needs while complying with Responsible AI principles.|Option E - Enabling encryption at rest and in transit protects data privacy and security.|EXAM FOCUS - You need to define resource tags and specify necessary AI services like NLP and Text Analytics to ensure cost-effectiveness. Make sure you select the appropriate pricing tier to match the scale of your application.|CAUTION ALERT - Stay cautioned against neglecting encryption requirements. Avoid thinking that configuring network security is the only step; scalability and cost optimization should be part of your planning.\", \"./Documents/mcq/mcq.csv_Qn5 mcq_options\": \"Option A) Choose an appropriate pricing tier and region for the Azure AI resource. |Option B) Configure network security settings, including virtual network integration and firewall rules. |Option C) Define resource tags to categorize and track usage and costs. |Option D) Specify the required Azure AI services, such as natural language processing and text analytics. |Option E) Enable encryption at rest and in transit to protect data privacy and security.\", \"./Documents/mcq/mcq.csv_Qn5 mcq_wrong_option_reason\": \"Option B - While important for security, configuring network security settings does not directly relate to creating the Azure AI resource or ensuring scalability and cost optimization.\", \"./Documents/mcq/mcq.csv_Qn20 mcq_question\": \"Your company is developing an AI-powered traffic management system for a smart city project. The system needs to analyze video feeds from surveillance cameras installed at various intersections to monitor vehicle flow and detect traffic congestion in real-time. You are tasked with selecting the appropriate Azure AI service to implement spatial analysis for people movement to optimize traffic flow. Which Azure AI service should you recommend based on the requirements and the need for accuracy and real-time processing?\", \"./Documents/mcq/mcq.csv_Qn20 mcq_answer\": \"Answer - [D] Azure AI Vision Spatial Analysis.\", \"./Documents/mcq/mcq.csv_Qn20 mcq_answer_reason\": \"EXAM FOCUS - Always remember to use Azure AI Vision Spatial Analysis for real-time processing and analysis of video feeds in traffic management systems. Keep in mind that this service is specifically designed for spatial analysis, making it ideal for monitoring vehicle flow and congestion.|CAUTION ALERT - Avoid selecting services like Video Indexer or Metrics Advisor for traffic flow analysis. Stay clear of relying on video metadata extraction services when you need real-time spatial analysis capabilities.\", \"./Documents/mcq/mcq.csv_Qn20 mcq_options\": \"Option A) Azure AI Video Indexer |Option B) Azure Cognitive Services - Computer Vision |Option C) Azure Cognitive Services - Video Analyzer |Option D) Azure AI Vision Spatial Analysis |Option E) Azure AI Metrics Advisor.\", \"./Documents/mcq/mcq.csv_Qn20 mcq_wrong_option_reason\": \"Option A - Azure AI Video Indexer focuses more on extracting insights and metadata from videos rather than real-time spatial analysis for traffic management.|Option B - While Azure Computer Vision offers image analysis capabilities, it may not provide specific features for spatial analysis and traffic optimization.|Option C - Azure Cognitive Services - Video Analyzer is geared towards real-time video processing but may lack the precision required for detailed spatial analysis.|Option E - Azure AI Metrics Advisor is designed for monitoring and analyzing metrics data rather than real-time video analysis for traffic management.\", \"./Documents/mcq/mcq2.csv_Qn6 mcq_question\": \"Which component of Azure AI Search allows you to apply AI-powered transformations on data before indexing?\", \"./Documents/mcq/mcq2.csv_Qn6 mcq_answer\": \"Answer - [C]\", \"./Documents/mcq/mcq2.csv_Qn6 mcq_answer_reason\": \" \", \"./Documents/mcq/mcq2.csv_Qn6 mcq_options\": \"Option A) Indexer|Option B) Index|Option C) Skillset|Option D) Query Pipeline\", \"./Documents/mcq/mcq2.csv_Qn6 mcq_wrong_option_reason\": \" \", \"./Documents/mcq/mcq2.csv_Qn27 mcq_question\": \"Which component of Azure AI Search allows you to apply AI-powered transformations on data before indexing?\", \"./Documents/mcq/mcq2.csv_Qn27 mcq_answer\": \"Answer - [C]\", \"./Documents/mcq/mcq2.csv_Qn27 mcq_answer_reason\": \" \", \"./Documents/mcq/mcq2.csv_Qn27 mcq_options\": \"Option A) Indexer|Option B) Index|Option C) Skillset|Option D) Query Pipeline\", \"./Documents/mcq/mcq2.csv_Qn27 mcq_wrong_option_reason\": \" \", \"./Documents/mcq/mcq2.csv_Qn47 mcq_question\": \"Which type of AI skill extracts text from images in Azure AI Search?\", \"./Documents/mcq/mcq2.csv_Qn47 mcq_answer\": \"Answer - [A]\", \"./Documents/mcq/mcq2.csv_Qn47 mcq_answer_reason\": \" \", \"./Documents/mcq/mcq2.csv_Qn47 mcq_options\": \"Option A) OCR Skill|Option B) Entity Recognition Skill|Option C) Translation Skill|Option D) Key Phrase Extraction Skill\", \"./Documents/mcq/mcq2.csv_Qn47 mcq_wrong_option_reason\": \" \"}', name='MCQ Retrieval Tool', tool_call_id='chatcmpl-tool-c0828f4058a3403d8b3fd1ecdd4cb33e')]}"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test to Call Tool Manually\n",
        "tool_call_msg = AIMessage(\n",
        "    content=\"\",\n",
        "    tool_calls=llm_with_tools.invoke(question_2).tool_calls,\n",
        ")\n",
        "\n",
        "tool_result = tool_node.invoke({\"messages\": [tool_call_msg]})\n",
        "tool_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F17kNlhSHivw"
      },
      "source": [
        "### Testing with Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "5K-VIAskyrtM",
        "outputId": "95c51bc9-2665-4ff5-b272-167a0c58b3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversation Thread ID: 10 -> 11\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Use Web Extraction Tool to search from website: https://www.ibm.com/think/topics/artificial-intelligence about Deep Learning\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Web Extraction Tool (chatcmpl-tool-48529beee9b345afab5c7bac804dc7ef)\n",
            " Call ID: chatcmpl-tool-48529beee9b345afab5c7bac804dc7ef\n",
            "  Args:\n",
            "    input: Deep Learning\n",
            "    url: https://www.ibm.com/think/topics/artificial-intelligence\n",
            "    k: 5\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: Web Extraction Tool\n",
            "\n",
            "[\"Each of these approaches is suited to different kinds of problems and data. But one of the most popular types of machine learning algorithm is called a neural network (or artificial neural network). Neural networks are modeled after the human brain's structure and function. A neural network consists of interconnected layers of nodes (analogous to neurons) that work together to process and analyze complex data. Neural networks are well suited to tasks that involve identifying complex patterns and relationships in large amounts of data. The simplest form of machine learning is called supervised learning, which involves the use of labeled data sets to train algorithms to classify data or predict outcomes accurately. In supervised learning, humans pair each training example with an output label. The goal is for the model to learn the mapping between inputs and outputs in the training data, so it can predict the labels of new, unseen data.     Deep learning   Deep learning is a subset of\", \"applications. The most common foundation models today are large language models (LLMs), created for text generation applications. But there are also foundation models for image, video, sound or music generation, and multimodal foundation models that support several kinds of content. To create a foundation model, practitioners train a deep learning algorithm on huge volumes of relevant raw, unstructured, unlabeled data, such as terabytes or petabytes of data text or images or video from the internet. The training yields a neural network of billions of parameters—encoded representations of the entities, patterns and relationships in the data—that can generate content autonomously in response to prompts. This is the foundation model. This training process is compute-intensive, time-consuming and expensive. It requires thousands of clustered graphics processing units (GPUs) and weeks of processing, all of which typically costs millions of dollars. Open source foundation model projects,\", \"numerical data. But over the last decade, they evolved to analyze and generate more complex data types. This evolution coincided with the emergence of three sophisticated deep learning model types: Variational autoencoders or VAEs, which were introduced in 2013, and enabled models that could generate multiple variations of content in response to a prompt or instruction. Diffusion models, first seen in 2014, which add \\\"noise\\\" to images until they are unrecognizable, and then remove the noise to generate original images in response to prompts. Transformers (also called transformer models), which are trained on sequenced data to generate extended sequences of content (such as words in sentences, shapes in an image, frames of a video or commands in software code). Transformers are at the core of most of today’s headline-making generative AI tools, including ChatGPT and GPT-4, Copilot, BERT, Bard and Midjourney.     Mixture of Experts | 14 February, episode 42         Decoding AI: Weekly\", \"Granite™   IBM® Granite™ is our family of open, performant and trusted AI models tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options. Meet Granite    Training    Level up your AI expertise   Access our full catalog of over 100 online courses by purchasing an individual or multi-user subscription today, enabling you to expand your skills across a range of our products at a low price. Start learning    Video    IBM AI Academy   Led by top IBM thought leaders, the curriculum is designed to help business leaders gain the knowledge needed to prioritize the AI investments that can drive growth. Explore the series    Guide    Put AI to work: Driving ROI with gen AI   Want to get a better return on your AI investments? Learn how scaling gen AI in key areas drives change by helping your best minds build and deliver innovative new solutions. Read the guide    Ebook    Unlock the power of generative AI and ML   Learn how to\", \"processing or production lines—AI can help maintain consistent work quality and output levels when used to complete repetitive or tedious tasks.  Reduced physical risk  By automating dangerous work—such as animal control, handling explosives, performing tasks in deep ocean water, high altitudes or in outer space—AI can eliminate the need to put human workers at risk of injury or worse. While they have yet to be perfected, self-driving cars and other vehicles offer the potential to reduce the risk of injury to passengers.   AI use cases    The real-world applications of AI are many. Here is just a small sampling of use cases across various industries to illustrate its potential:  Customer experience, service and support  Companies can implement AI-powered chatbots and virtual assistants to handle customer inquiries, support tickets and more. These tools use natural language processing (NLP) and generative AI capabilities to understand and respond to customer questions about order\"]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The output of the Web Extraction Tool is a JSON object containing a list of extracted content from the website https://www.ibm.com/think/topics/artificial-intelligence about Deep Learning. The content includes information on neural networks, supervised learning, deep learning, foundation models, and various use cases of AI in different industries.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Saved to memory key: 11'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# URL\n",
        "url1 = \"https://www.ibm.com/think/topics/artificial-intelligence\"\n",
        "url2 = \"https://www.ibm.com/think/topics/machine-learning\"\n",
        "\n",
        "question_1 = (f\"Use Web Extraction Tool to search from website: {url1} about Deep Learning\")\n",
        "question_2 = \"Provide 5 MCQ questions on Artificial Intelligence to help me with practice.\"\n",
        "question_3 = \"Here are my answers: 1. A, 2. B, 3. C, 4. D, 5. E. Please check mu answer, provide reasons for my wrong answers and provide the correct answers with explanations.\"\n",
        "question_4 = \"Provide another 5 MCQ questions on Artificial Intelligence to help me with practice.\"\n",
        "question_5 = \"Here are my answers: 1. A, 2. B, 3. C, 4. D, 5. E. Please check mu answer, provide reasons for my wrong answers and provide the correct answers with explanations.\"\n",
        "question_6 = \"Provide a study quide to help me learn for my wrong answers for the MCQ questions.\"\n",
        "question_7 = \"Based on your reference databases only, provide a study quide on Deep Learning.\"\n",
        "question_8 = \"Based on your memory, provide a summary of our conversation.\"\n",
        "question_9 = \"Summarise the conversation so far.\"\n",
        "\n",
        "# Grab the current user_id and thread_id from config\n",
        "user_id = \"user_1\"\n",
        "\n",
        "# Get last thread_id\n",
        "last_thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "thread_id = str(int(last_thread_id) + 1)\n",
        "\n",
        "# Print Config\n",
        "print(f\"Conversation Thread ID: {last_thread_id} -> {thread_id}\")\n",
        "\n",
        "# Update the config with the new thread_id\n",
        "config = {\"configurable\": {\"thread_id\": thread_id, \"user_id\": user_id}}\n",
        "\n",
        "\n",
        "# Create a system prompt (your overall instructions to the AI)\n",
        "system_prompt = (f\"\"\"\n",
        "You are a helpful AI Tutor assistant.\n",
        "\"\"\")\n",
        "\n",
        "#The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question_1}]},\n",
        "    config, # Pass the thread-level persistence to the graph\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    response = event[\"messages\"][-1]\n",
        "    response.pretty_print()\n",
        "\n",
        "update_memory(event, config, store=in_memory_store) # Update Memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rZtd4vjRJYG"
      },
      "outputs": [],
      "source": [
        "# Function for user to input their query and system prompt to the agent\n",
        "def submit_query(user_query, system_prompt, temperature):\n",
        "    \"\"\"\n",
        "    Triggered when the user clicks the submit icon.\n",
        "    - user_query -> str\n",
        "    - system_prompt -> str\n",
        "    - temperature -> float\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Call query_agent as before to get the streaming generator.\n",
        "    events = graph.stream(\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_query}\n",
        "            ]\n",
        "        },\n",
        "        new_config,  # user_id, thread_id, etc.\n",
        "        stream_mode=\"values\",\n",
        "    )\n",
        "\n",
        "    # 2) We'll capture each message's pretty_print() into a list for the console.\n",
        "    from io import StringIO\n",
        "    import sys\n",
        "\n",
        "    message_stream = []\n",
        "    final_response = \"\"\n",
        "    for event in events:\n",
        "        # Each event is a dict with \"messages\" as a list of BaseMessage objects.\n",
        "        for msg in event[\"messages\"]:\n",
        "            # If for some reason the msg doesn't support pretty_print, we fallback to .content\n",
        "            captured_output = StringIO()\n",
        "            original_stdout = sys.stdout\n",
        "            try:\n",
        "                sys.stdout = captured_output\n",
        "                # If it's an AI/Tool message with pretty_print(), call it:\n",
        "                if hasattr(msg, \"pretty_print\"):\n",
        "                    msg.pretty_print()\n",
        "                else:\n",
        "                    # Otherwise, just print its content\n",
        "                    print(msg.content)\n",
        "            finally:\n",
        "                sys.stdout = original_stdout\n",
        "\n",
        "            # Store the captured text into our message_stream\n",
        "            pretty_text = captured_output.getvalue()\n",
        "            message_stream.append(pretty_text.strip())\n",
        "\n",
        "        # If this event had at least one message, the last is presumably the LLM's response\n",
        "        if event[\"messages\"]:\n",
        "            final_response = event[\"messages\"][-1].content\n",
        "\n",
        "    # 3) Return final response for the chatbot's main window\n",
        "    #    and the entire stream for the console output.\n",
        "    return final_response, \"\\n\".join(message_stream)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzcOcBWOmWiP"
      },
      "source": [
        "#### Accessing and Testing LLM Chat Model Memory Persistence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtldTWDfRJYG"
      },
      "outputs": [],
      "source": [
        "# Define function to download the entire conversation history from the memory store and save it to a file\n",
        "def download_conversation_history(user_id: str, store: BaseStore = in_memory_store) -> str:\n",
        "    \"\"\"\n",
        "    Download the entire conversation history from memory store and save it to a JSON file.\n",
        "    \"\"\"\n",
        "    memories = store.search((user_id, \"memories\"))\n",
        "    conversation_history = [m.value.get(\"memory\") for m in memories if \"memory\" in m.value]\n",
        "    filename = f\"{user_id}_conversation_history.json\"\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(conversation_history, f, indent=2)\n",
        "    return f\"Conversation history saved to {filename}\"\n",
        "\n",
        "# Download the conversation history\n",
        "download_conversation_history(user_id, in_memory_store)\n",
        "\n",
        "# Print the conversation history\n",
        "with open(f\"{user_id}_conversation_history.json\", \"r\") as f:\n",
        "    conversation_history = json.load(f)\n",
        "    print(\"Conversation History:\")\n",
        "    print(json.dumps(conversation_history, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xgiL2CJRJYJ"
      },
      "source": [
        "## App Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTL9JhvhRJYJ"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqPpud8wRJYJ"
      },
      "outputs": [],
      "source": [
        "from io import StringIO\n",
        "import sys\n",
        "\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "import gradio as gr\n",
        "import json\n",
        "import hashlib\n",
        "import uuid\n",
        "import logging\n",
        "from typing import List, Dict, Sequence, TypedDict\n",
        "\n",
        "# LangChain & related imports\n",
        "from langchain_core.tools import tool, StructuredTool\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "# Extraction for Documents\n",
        "from langchain_docling.loader import ExportType\n",
        "from langchain_docling import DoclingLoader\n",
        "from docling.chunking import HybridChunker\n",
        "\n",
        "# Extraction for HTML\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import InjectedStore\n",
        "from langgraph.store.base import BaseStore\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain.embeddings import init_embeddings\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langchain_core.messages import (\n",
        "    SystemMessage,\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    BaseMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")  # Read from environment variable\n",
        "if HF_TOKEN:\n",
        "    login(token=HF_TOKEN)  # Log in to Hugging Face Hub\n",
        "else:\n",
        "    print(\"Warning: HF_TOKEN not found in environment variables.\")\n",
        "\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")  # Read from environment variable\n",
        "\n",
        "# =============================================================================\n",
        "# Configurations\n",
        "# =============================================================================\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Initialize the in-memory store for conversation threads\n",
        "in_memory_store = InMemoryStore(\n",
        "    index={\n",
        "        \"embed\": init_embeddings(\"huggingface:sentence-transformers/all-MiniLM-L6-v2\"),\n",
        "        \"dims\": 384,\n",
        "    }\n",
        ")\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "# Global config for user ID and conversation thread ID\n",
        "config = {\n",
        "    \"configurable\": {\n",
        "        \"user_id\": \"user_1\",  # Hard-coded user ID\n",
        "        \"thread_id\": 0\n",
        "    }\n",
        "}\n",
        "\n",
        "###############################################################################\n",
        "# Document Extraction and Utility Functions\n",
        "###############################################################################\n",
        "\n",
        "def extract_documents(doc_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Recursively collects all file paths from folder 'doc_path'.\n",
        "    \"\"\"\n",
        "    extracted_docs = []\n",
        "    for root, _, files in os.walk(doc_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            extracted_docs.append(file_path)\n",
        "    return extracted_docs\n",
        "\n",
        "\n",
        "def _generate_uuid(page_content: str) -> str:\n",
        "    \"\"\"Generate a UUID for a chunk of text using MD5 hashing.\"\"\"\n",
        "    md5_hash = hashlib.md5(page_content.encode()).hexdigest()\n",
        "    return str(uuid.UUID(md5_hash[0:32]))\n",
        "\n",
        "\n",
        "def load_file(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load a file from the given path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "    try:\n",
        "        loader = DoclingLoader(\n",
        "            file_path=file_path,\n",
        "            export_type=ExportType.DOC_CHUNKS,\n",
        "            chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
        "        )\n",
        "        docs = loader.load()\n",
        "        logger.info(f\"Total parsed doc-chunks: {len(docs)} from Source: {file_path}\")\n",
        "\n",
        "        for d in docs:\n",
        "            doc = Document(\n",
        "                page_content=d.page_content,\n",
        "                metadata={\n",
        "                    \"source\": file_path,\n",
        "                    \"doc_id\": _generate_uuid(d.page_content),\n",
        "                    \"source_type\": \"file\",\n",
        "                }\n",
        "            )\n",
        "            _documents.append(doc)\n",
        "        logger.info(f\"Total generated LangChain document chunks: {len(_documents)}\\n.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {file_path}. Exception: {e}\\n.\")\n",
        "\n",
        "    return _documents\n",
        "\n",
        "\n",
        "def load_files_from_folder(doc_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load documents from the given folder path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "    extracted_docs = extract_documents(doc_path)\n",
        "    for file_path in extracted_docs:\n",
        "        _documents.extend(load_file(file_path))\n",
        "    return _documents\n",
        "\n",
        "\n",
        "def ensure_scheme(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    if not parsed_url.scheme:\n",
        "        return 'http://' + url\n",
        "    return url\n",
        "\n",
        "\n",
        "def extract_html(urls):\n",
        "    \"\"\"\n",
        "    Extract text from the HTML content of web pages.\n",
        "    \"\"\"\n",
        "    if isinstance(urls, str):\n",
        "        urls = [urls]\n",
        "    web_paths = [ensure_scheme(u) for u in urls]\n",
        "    loader = WebBaseLoader(web_paths)\n",
        "    loader.requests_per_second = 1\n",
        "    docs = loader.load()\n",
        "    _documents = []\n",
        "    for doc in docs:\n",
        "        cleaned = doc.page_content.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "        cleaned = cleaned.replace(\"\\t\", \" \").replace(\"  \", \" \").replace(\"   \", \" \")\n",
        "        web_doc = Document(\n",
        "            page_content=cleaned,\n",
        "            metadata={\n",
        "                \"source\": doc.metadata.get(\"source\"),\n",
        "                \"doc_id\": _generate_uuid(cleaned),\n",
        "                \"source_type\": \"web\"\n",
        "            }\n",
        "        )\n",
        "        _documents.append(web_doc)\n",
        "    return _documents\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Vector Stores and Document Splitting\n",
        "###############################################################################\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
        "\n",
        "general_vs = Chroma(\n",
        "    collection_name=\"general_vstore\",\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"./general_db\"\n",
        ")\n",
        "mcq_vs = Chroma(\n",
        "    collection_name=\"mcq_vstore\",\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"./mcq_db\"\n",
        ")\n",
        "in_memory_vs = Chroma(\n",
        "    collection_name=\"in_memory_vstore\",\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "\n",
        "def split_text_into_chunks(docs: List[Document]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Splits Documents into smaller text chunks for better embedding coverage.\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        add_start_index=True\n",
        "    )\n",
        "    chunked_docs = splitter.split_documents(docs)\n",
        "    return chunked_docs\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Retrieval Tools (MCQ, general, in-memory, web extraction, ensemble)\n",
        "###############################################################################\n",
        "class MCQRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"Search topic.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"Number of question sets to retrieve.\")\n",
        "\n",
        "\n",
        "def mcq_retriever(input: str, k: int = 2) -> Dict[str, str]:\n",
        "    docs_func = mcq_vs.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={'k': k, 'filter': {\"source_type\": {\"$eq\": \"mcq_question\"}}},\n",
        "    )\n",
        "    docs_qns = docs_func.invoke(input, k=k)\n",
        "    doc_ids = [d.metadata.get(\"doc_id\") for d in docs_qns if \"doc_id\" in d.metadata]\n",
        "    docs = mcq_vs.get(where={'doc_id': {\"$in\": doc_ids}})\n",
        "    qns_list = {}\n",
        "    for i, d in enumerate(docs['metadatas']):\n",
        "        qns_list[d['source'] + \" \" + d['source_type']] = docs['documents'][i]\n",
        "    return qns_list\n",
        "\n",
        "\n",
        "mcq_retriever_tool = StructuredTool.from_function(\n",
        "    func=mcq_retriever,\n",
        "    name=\"MCQ Retrieval Tool\",\n",
        "    description=(\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve MCQ questions set when Human asks to generate a quiz related to a topic.\n",
        "    DO NOT GIVE THE ANSWERS to Human before Human has answered all the questions.\n",
        "\n",
        "    If Human give answers for questions you do not know, SAY you do not have the questions for the answer\n",
        "    and ASK if the Human want you to generate a new quiz and then SAVE THE QUIZ with Summary Tool before ending the conversation.\n",
        "\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The search topic to retrieve MCQ questions set related to the topic.\n",
        "        - k (int): Number of question set to retrieve.\n",
        "        Example usage: input='What is AI?', k=5\n",
        "\n",
        "    Returns:\n",
        "    - A dict of MCQ questions:\n",
        "    Key: 'metadata of question' e.g. './Documents/mcq/mcq.csv_Qn31 mcq_question' with suffix ['question', 'answer', 'answer_reason', 'options', 'wrong_options_reason']\n",
        "    Value: Text Content\n",
        "\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema=MCQRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "\n",
        "class GenRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"Number of documents to retrieve.\")\n",
        "\n",
        "\n",
        "def gen_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    docs_func = general_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "\n",
        "general_retriever_tool = StructuredTool.from_function(\n",
        "    func=gen_retriever,\n",
        "    name=\"Assistant References Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve reference information from Assistant reference database for Human queries related to a topic or\n",
        "    and when Human asked to generate guides to learn or study about a topic.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "        Example usage: input='What is AI?', k=5\n",
        "    Returns:\n",
        "    - A list of retrieved document's content string.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema=GenRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "\n",
        "class InMemoryRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"Number of documents to retrieve.\")\n",
        "\n",
        "\n",
        "def in_memory_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    docs_func = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "\n",
        "in_memory_retriever_tool = StructuredTool.from_function(\n",
        "    func=in_memory_retriever,\n",
        "    name=\"In-Memory Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool when Human ask Assistant to retrieve information from documents that Human has uploaded.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema=InMemoryRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "\n",
        "class WebExtractionRequest(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"Search text.\")\n",
        "    url: str = Field(..., title=\"url\", description=\"Web URL(s) to extract content from. If multiple URLs, separate them with a comma.\")\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"Number of documents to retrieve.\")\n",
        "\n",
        "\n",
        "def extract_web_path_tool(input: str, url: str, k: int = 5) -> List[str]:\n",
        "    if isinstance(url, str):\n",
        "        url = [url]\n",
        "    html_docs = extract_html(url)\n",
        "    if not html_docs:\n",
        "        return [f\"No content extracted from {url}.\"]\n",
        "    chunked_texts = split_text_into_chunks(html_docs)\n",
        "    in_memory_vs.add_documents(chunked_texts)\n",
        "    docs_func = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={'k': k, 'lambda_mult': 0.25, 'filter': {\"source\": {\"$in\": url}}},\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "\n",
        "web_extraction_tool = StructuredTool.from_function(\n",
        "    func=extract_web_path_tool,\n",
        "    name=\"Web Extraction Tool\",\n",
        "    description = (\n",
        "        \"Assistant should use this tool to extract content from web URLs based on user's input, \"\n",
        "        \"Web extraction is initially load into database and then return k: Number of results to retrieve\"\n",
        "    ),\n",
        "    args_schema=WebExtractionRequest,\n",
        "    return_direct=False,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "\n",
        "class EnsembleRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"User query.\")\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"Number of results.\")\n",
        "\n",
        "\n",
        "def ensemble_retriever(input: str, k: int = 5) -> List[str]:\n",
        "    general_retrieval = general_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    in_memory_retrieval = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    ensemble = EnsembleRetriever(\n",
        "        retrievers=[general_retrieval, in_memory_retrieval],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "    docs = ensemble.invoke(input)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "\n",
        "ensemble_retriever_tool = StructuredTool.from_function(\n",
        "    func=ensemble_retriever,\n",
        "    name=\"Ensemble Retriever Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve information from reference database and\n",
        "    extraction of documents that Human has uploaded.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema=EnsembleRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False\n",
        ")\n",
        "\n",
        "###############################################################################\n",
        "# LLM Model Setup\n",
        "###############################################################################\n",
        "\n",
        "TEMPERATURE = 0.5\n",
        "# model = ChatOpenAI(\n",
        "#     model=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "#     temperature=TEMPERATURE,\n",
        "#     timeout=None,\n",
        "#     max_retries=2,\n",
        "#     api_key=\"not_required\",\n",
        "#     base_url=\"http://localhost:8000/v1\",\n",
        "# )\n",
        "\n",
        "model = ChatGroq(\n",
        "    model_name=\"deepseek-r1-distill-llama-70b\",\n",
        "    temperature=TEMPERATURE,\n",
        "    api_key=GROQ_API_KEY,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "###############################################################################\n",
        "# LangGraph Setup\n",
        "###############################################################################\n",
        "\n",
        "# Define messages state\n",
        "class MessagesState(TypedDict):\n",
        "    messages: Sequence[BaseMessage]\n",
        "\n",
        "\n",
        "def save_memory(summary_text: str, *, config, store: BaseStore) -> str:\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    thread_id = config.get(\"configurable\", {}).get(\"thread_id\")\n",
        "    namespace = (user_id, \"memories\")\n",
        "    memory_id = thread_id\n",
        "    store.put(namespace, memory_id, {\"summary\": summary_text})\n",
        "    return f\"Saved to memory key: {memory_id}\"\n",
        "\n",
        "\n",
        "def update_memory(state: MessagesState, config, store: BaseStore):\n",
        "    messages = state[\"messages\"]\n",
        "    messages_dict = [{\"role\": msg.type, \"content\": msg.content} for msg in messages]\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    thread_id = config.get(\"configurable\", {}).get(\"thread_id\")\n",
        "    namespace = (user_id, \"memories\")\n",
        "    memory_id = f\"{thread_id}\"\n",
        "    store.put(namespace, memory_id, {\"memory\": messages_dict})\n",
        "    return f\"Saved to memory key: {memory_id}\"\n",
        "\n",
        "\n",
        "class RecallMemory(BaseModel):\n",
        "    query_text: str = Field(..., title=\"Search Text\", description=\"The text to search.\")\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"Number of results.\")\n",
        "\n",
        "\n",
        "def recall_memory(query_text: str, k: int = 5) -> str:\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
        "    memories = [\n",
        "        m.value[\"memory\"] for m in in_memory_store.search((user_id, \"memories\"), query=query_text, limit=k)\n",
        "        if \"memory\" in m.value\n",
        "    ]\n",
        "    return f\"User memories: {memories}\"\n",
        "\n",
        "\n",
        "recall_memory_tool = StructuredTool.from_function(\n",
        "    func=recall_memory,\n",
        "    name=\"Recall Memory Tool\",\n",
        "    description=\"Retrieve user memories relevant to the query.\",\n",
        "    args_schema=RecallMemory,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "\n",
        "class SummariseConversation(BaseModel):\n",
        "    summary_text: str = Field(..., title=\"text\", description=\"A summary of entire conversation.\")\n",
        "\n",
        "\n",
        "def summarise_node(summary_text: str):\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "    current_thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "    new_thread_id = str(int(current_thread_id) + 1)\n",
        "    config_for_saving = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": user_id,\n",
        "            \"thread_id\": new_thread_id\n",
        "        }\n",
        "    }\n",
        "    _ = save_memory(summary_text, config=config_for_saving, store=in_memory_store)\n",
        "\n",
        "\n",
        "summarise_tool = StructuredTool.from_function(\n",
        "    func=summarise_node,\n",
        "    name=\"Summary Tool\",\n",
        "    description=\"\"\"\n",
        "      Summarize the current conversation in no more than\n",
        "      1000 words. Also retain any unanswered quiz questions along with\n",
        "      your internal answers so the next conversation thread can continue.\n",
        "      Do not reveal solutions to the user yet. Use this tool to save\n",
        "      the current conversation to memory and then end the conversation.\n",
        "      \"\"\",\n",
        "    args_schema=SummariseConversation,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "tools = [\n",
        "    mcq_retriever_tool,\n",
        "    web_extraction_tool,\n",
        "    ensemble_retriever_tool,\n",
        "    general_retriever_tool,\n",
        "    in_memory_retriever_tool,\n",
        "    recall_memory_tool,\n",
        "    summarise_tool,\n",
        "]\n",
        "\n",
        "llm_with_tools = model.bind_tools(tools)\n",
        "\n",
        "\n",
        "def call_model(state: MessagesState, config):\n",
        "    \"\"\"\n",
        "    The main agent node that calls the LLM with the user + system messages.\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    for m in state[\"messages\"]:\n",
        "        if isinstance(m, dict):\n",
        "            messages.append(HumanMessage(content=m.get(\"content\", \"\"), role=m.get(\"role\", \"user\")))\n",
        "        else:\n",
        "            messages.append(m)\n",
        "\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "graph_builder.add_node(\"agent\", call_model)\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.set_entry_point(\"agent\")\n",
        "graph_builder.add_conditional_edges(\"agent\", tools_condition)\n",
        "graph_builder.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "graph = graph_builder.compile(checkpointer=checkpointer, store=in_memory_store)\n",
        "\n",
        "###############################################################################\n",
        "# Public Functions for the Gradio App\n",
        "###############################################################################\n",
        "\n",
        "def upload_documents(doc_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Load documents from the given folder path and upload into in-memory vector store.\n",
        "    \"\"\"\n",
        "    documents = load_files_from_folder(doc_path)\n",
        "    if not documents:\n",
        "        return f\"No documents found in {doc_path}.\"\n",
        "    chunked_texts = split_text_into_chunks(documents)\n",
        "    in_memory_vs.add_documents(chunked_texts)\n",
        "    return f\"Uploaded {len(documents)} documents from {doc_path} to in-memory vector store.\"\n",
        "\n",
        "\n",
        "\n",
        "def download_conversation_history(user_id: str, store: BaseStore = in_memory_store) -> str:\n",
        "    \"\"\"\n",
        "    Download the entire conversation history from memory store and save it to a JSON file.\n",
        "    \"\"\"\n",
        "    memories = store.search((user_id, \"memories\"))\n",
        "    conversation_history = [m.value.get(\"memory\") for m in memories if \"memory\" in m.value]\n",
        "    filename = f\"{user_id}_conversation_history.json\"\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(conversation_history, f, indent=2)\n",
        "    return f\"Conversation history saved to {filename}\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWELX-uoRJYJ"
      },
      "source": [
        "### Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaFl4Ke2RJYJ"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "###############################################################################\n",
        "# Gradio App\n",
        "###############################################################################\n",
        "\n",
        "# 'user_1' from the config in utils.\n",
        "USER_ID = config[\"configurable\"][\"user_id\"]\n",
        "# Get last thread_id\n",
        "last_thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "thread_id = str(int(last_thread_id) + 1)\n",
        "\n",
        "# Update the config with the new thread_id\n",
        "config = {\"configurable\": {\"thread_id\": thread_id, \"user_id\": USER_ID}}\n",
        "\n",
        "\n",
        "def submit_query(user_query, system_prompt, temperature):\n",
        "    \"\"\"\n",
        "    Triggered when the user clicks the submit (➡️) icon.\n",
        "    - user_query -> str\n",
        "    - system_prompt -> str\n",
        "    - temperature -> float\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Call query_agent as before to get the streaming generator.\n",
        "    events = graph.stream(\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_query}\n",
        "            ]\n",
        "        },\n",
        "        config,  # user_id, thread_id, etc.\n",
        "        stream_mode=\"values\",\n",
        "    )\n",
        "\n",
        "    # 2) We'll capture each message's pretty_print() into a list for the console.\n",
        "\n",
        "    message_stream = []\n",
        "    final_response = \"\"\n",
        "    for event in events:\n",
        "        # Each event is a dict with \"messages\" as a list of BaseMessage objects.\n",
        "        for msg in event[\"messages\"]:\n",
        "            # If for some reason the msg doesn't support pretty_print, we fallback to .content\n",
        "            captured_output = StringIO()\n",
        "            original_stdout = sys.stdout\n",
        "            try:\n",
        "                sys.stdout = captured_output\n",
        "                # If it's an AI/Tool message with pretty_print(), call it:\n",
        "                if hasattr(msg, \"pretty_print\"):\n",
        "                    msg.pretty_print()\n",
        "                else:\n",
        "                    # Otherwise, just print its content\n",
        "                    print(msg.content)\n",
        "            finally:\n",
        "                sys.stdout = original_stdout\n",
        "\n",
        "            # Store the captured text into our message_stream\n",
        "            pretty_text = captured_output.getvalue()\n",
        "            message_stream.append(pretty_text.strip())\n",
        "\n",
        "        # If this event had at least one message, the last is presumably the LLM's response\n",
        "        if event[\"messages\"]:\n",
        "            final_response = event[\"messages\"][-1].content\n",
        "\n",
        "    # 3) Return final response for the chatbot's main window\n",
        "    #    and the entire stream for the console output.\n",
        "    return final_response, \"\\n\".join(message_stream)\n",
        "\n",
        "def process_upload(files_list):\n",
        "    \"\"\"\n",
        "    Triggered when the user clicks the folder icon to upload documents.\n",
        "    Receives a list of Gradio file objects.\n",
        "    \"\"\"\n",
        "    if not files_list:\n",
        "        return \"No files to upload.\"\n",
        "\n",
        "    temp_folder = \"./uploaded_files\"\n",
        "    os.makedirs(temp_folder, exist_ok=True)\n",
        "\n",
        "    for f in files_list:\n",
        "        file_path = os.path.join(temp_folder, f.name)\n",
        "        with open(file_path, \"wb\") as outfile:\n",
        "            outfile.write(f.read())\n",
        "\n",
        "    result = upload_documents(temp_folder)\n",
        "    return result\n",
        "\n",
        "def toggle_advanced_settings(show_advanced):\n",
        "    \"\"\"\n",
        "    Toggles the visibility of the advanced settings panel.\n",
        "    \"\"\"\n",
        "    return gr.update(visible=show_advanced)\n",
        "\n",
        "def generate_json_history():\n",
        "    \"\"\"\n",
        "    Generates a conversation-history JSON file for the user.\n",
        "    \"\"\"\n",
        "    result = download_conversation_history(USER_ID)\n",
        "    return result\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# AI Tutor Chatbot (Gradio App)\")\n",
        "\n",
        "    # Main chat window:\n",
        "    chatbot = gr.Chatbot(label=\"Chat Window\")\n",
        "\n",
        "    # The user input box (5 lines, scrollable):\n",
        "    with gr.Row():\n",
        "        user_input = gr.Textbox(\n",
        "            lines=5,\n",
        "            max_lines=5,\n",
        "            label=\"Type your query here:\",\n",
        "            placeholder=\"Enter your question...\",\n",
        "            show_label=False,\n",
        "            elem_id=\"user_query_box\"\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1, min_width=50):\n",
        "            file_upload = gr.Files(\n",
        "                file_types=[\"pdf\", \"ppt\", \"pptx\", \"doc\", \"docx\", \"txt\"],\n",
        "                file_count=\"multiple\",\n",
        "                label=\"Upload\"\n",
        "            )\n",
        "            upload_button = gr.Button(\n",
        "                \"📁\",\n",
        "                elem_id=\"upload_icon\"\n",
        "            )\n",
        "        with gr.Column(scale=4):\n",
        "            submit_button = gr.Button(\n",
        "                \"➡️\",\n",
        "                elem_id=\"submit_icon\"\n",
        "            )\n",
        "\n",
        "    bot_reply = gr.Textbox(\n",
        "        label=\"Chatbot's Response:\",\n",
        "        interactive=False,\n",
        "        lines=6,\n",
        "        placeholder=\"Assistant's answer will appear here.\"\n",
        "    )\n",
        "\n",
        "    console_output = gr.Textbox(\n",
        "        label=\"Console Window (Message Stream)\",\n",
        "        interactive=False,\n",
        "        lines=10,\n",
        "        placeholder=\"Stream of messages...\"\n",
        "    )\n",
        "\n",
        "    # Toggle advanced settings:\n",
        "    advanced_toggle = gr.Checkbox(\n",
        "        label=\"Show Advanced Settings\",\n",
        "        value=False\n",
        "    )\n",
        "\n",
        "    with gr.Column(visible=False) as advanced_panel:\n",
        "        temp_slider = gr.Slider(\n",
        "            minimum=0.0,\n",
        "            maximum=1.0,\n",
        "            value=0.5,\n",
        "            step=0.1,\n",
        "            label=\"Temperature\"\n",
        "        )\n",
        "        system_prompt_input = gr.Textbox(\n",
        "            label=\"System Prompt\",\n",
        "            value=\"You are a helpful assistant.\",\n",
        "            lines=3\n",
        "        )\n",
        "        json_button = gr.Button(\n",
        "            \"Download Conversation History\",\n",
        "            variant=\"secondary\"\n",
        "        )\n",
        "        json_result = gr.Textbox(\n",
        "            label=\"JSON Export Info\",\n",
        "            interactive=False,\n",
        "            lines=2\n",
        "        )\n",
        "\n",
        "    # Define interactions\n",
        "    advanced_toggle.change(\n",
        "        fn=toggle_advanced_settings,\n",
        "        inputs=[advanced_toggle],\n",
        "        outputs=[advanced_panel]\n",
        "    )\n",
        "\n",
        "    upload_button.click(\n",
        "        fn=process_upload,\n",
        "        inputs=[file_upload],\n",
        "        outputs=[console_output]\n",
        "    )\n",
        "\n",
        "    user_input.submit(\n",
        "        fn=submit_query,\n",
        "        inputs=[user_input, system_prompt_input, temp_slider],\n",
        "        outputs=[bot_reply, console_output]\n",
        "    )\n",
        "\n",
        "    submit_button.click(\n",
        "        fn=submit_query,\n",
        "        inputs=[user_input, system_prompt_input, temp_slider],\n",
        "        outputs=[bot_reply, console_output]\n",
        "    )\n",
        "\n",
        "    json_button.click(\n",
        "        fn=generate_json_history,\n",
        "        outputs=[json_result]\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ### Instructions:\n",
        "    1. Type your query in the textbox and click the submit icon on the right.\n",
        "    2. To upload documents for the chatbot to reference, select or drag your files and then click the folder icon.\n",
        "    3. For advanced settings (temperature and custom system prompt), enable the checkbox below.\n",
        "    4. You can download the conversation history anytime.\n",
        "    \"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KmeoL-_lcs8Y",
        "s_2h8tWUc9yA",
        "QNgX4ZVYrS8Z",
        "LuhaRfR6dLno"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "yourenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
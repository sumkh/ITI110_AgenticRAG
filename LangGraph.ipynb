{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumkh/ITI110_AgenticRAG/blob/main/LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVfMT6l1cobl"
      },
      "source": [
        "## AI Tutor Chatbot (Version 2.7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmeoL-_lcs8Y"
      },
      "source": [
        "### Setting Up - Install Requirements (Restart Session after installation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MkI4XOWxSXvl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install vllm huggingface_hub transformers langchain langchain_huggingface langgraph accelerate bitsandbytes langchain-core langchain-text-splitters langchain-community chromadb langchain-chroma langsmith docling langchain-docling sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#%pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "dx4Mp2j2NBep"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_2h8tWUc9yA"
      },
      "source": [
        "### Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lzrhAq9cbbU",
        "outputId": "d5041613-84e2-44b5-8f67-13060fe08926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-14 15:58:07--  https://github.com/sumkh/NYP_Dataset/raw/refs/heads/main/Documents.zip\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sumkh/NYP_Dataset/refs/heads/main/Documents.zip [following]\n",
            "--2025-02-14 15:58:07--  https://raw.githubusercontent.com/sumkh/NYP_Dataset/refs/heads/main/Documents.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18510909 (18M) [application/zip]\n",
            "Saving to: ‘Documents.zip.1’\n",
            "\n",
            "Documents.zip.1     100%[===================>]  17.65M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-02-14 15:58:07 (139 MB/s) - ‘Documents.zip.1’ saved [18510909/18510909]\n",
            "\n",
            "Archive:  /content/Documents.zip\n",
            "  inflating: Documents/general/Topic 1 Introduction to AI and AI on Azure.pdf  \n",
            "  inflating: Documents/general/deeplearningreview.docx  \n",
            "  inflating: Documents/general/slide_1.pptx  \n",
            "  inflating: Documents/mcq/mcq.csv   \n",
            "  inflating: Documents/mcq/mcq2.csv  \n",
            "  inflating: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/data_level0.bin  \n",
            "  inflating: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/header.bin  \n",
            "  inflating: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/length.bin  \n",
            " extracting: general_db/0ab351f4-2d03-423f-bbf2-093d3b8eba80/link_lists.bin  \n",
            "  inflating: general_db/chroma.sqlite3  \n",
            "  inflating: mcq_db/chroma.sqlite3   \n",
            "  inflating: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/data_level0.bin  \n",
            "  inflating: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/header.bin  \n",
            "  inflating: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/length.bin  \n",
            " extracting: mcq_db/d897d22c-91fc-4db5-8a6b-aacbd9c5f57d/link_lists.bin  \n",
            "  inflating: requirements.txt        \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Disable tokenizers parallelism, as it causes issues with multiprocessing\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # LangSmith for Observability\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"AgenticRAG\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=\"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\")\n",
        "\n",
        "# Download required files from Github repo\n",
        "!wget https://github.com/sumkh/NYP_Dataset/raw/refs/heads/main/Documents.zip\n",
        "!unzip -o /content/Documents.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Znk0gm1ce67",
        "outputId": "076c67f1-edd6-4ef1-b92f-5bdd16aea99d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import json\n",
        "import hashlib\n",
        "import uuid\n",
        "import logging\n",
        "from typing import List, Optional, Union, Literal, Dict\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "# LangChain & related imports\n",
        "from langchain_core.tools import tool, StructuredTool\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever\n",
        "\n",
        "# Extraction for Documents\n",
        "from langchain_docling.loader import ExportType\n",
        "from langchain_docling import DoclingLoader\n",
        "from docling.chunking import HybridChunker\n",
        "\n",
        "# Extraction for HTML\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configurations and Get the API key from the environment variable\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_ki8T8DdE8C"
      },
      "source": [
        "### Load Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2SDpdN9Yck4w"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "#                         Document Extraction Functions\n",
        "# =============================================================================\n",
        "\n",
        "def extract_documents(doc_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Recursively collects all file paths from folder 'doc_path'.\n",
        "    Used by ExtractDocument.load_files() to find documents to parse.\n",
        "    \"\"\"\n",
        "    extracted_docs = []\n",
        "\n",
        "    for root, _, files in os.walk(doc_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            extracted_docs.append(file_path)\n",
        "    return extracted_docs\n",
        "\n",
        "\n",
        "def _generate_uuid(page_content: str) -> str:\n",
        "    \"\"\"Generate a UUID for a chunk of text using MD5 hashing.\"\"\"\n",
        "    md5_hash = hashlib.md5(page_content.encode()).hexdigest()\n",
        "    return str(uuid.UUID(md5_hash[0:32]))\n",
        "\n",
        "\n",
        "def load_file(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load a file from the given path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "\n",
        "    # Load the file and extract the text chunks\n",
        "    try:\n",
        "        loader = DoclingLoader(\n",
        "            file_path = file_path,\n",
        "            export_type = ExportType.DOC_CHUNKS,\n",
        "            chunker = HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
        "        )\n",
        "        docs = loader.load()\n",
        "        logger.info(f\"Total parsed doc-chunks: {len(docs)} from Source: {file_path}\")\n",
        "\n",
        "        for d in docs:\n",
        "            # Tag each document's chunk with the source file and a unique ID\n",
        "            doc = Document(\n",
        "                page_content=d.page_content,\n",
        "                metadata={\n",
        "                    \"source\": file_path,\n",
        "                    \"doc_id\": _generate_uuid(d.page_content),\n",
        "                    \"source_type\": \"file\",\n",
        "                }\n",
        "            )\n",
        "            _documents.append(doc)\n",
        "        logger.info(f\"Total generated LangChain document chunks: {len(_documents)}\\n.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {file_path}. Exception: {e}\\n.\")\n",
        "\n",
        "    return _documents\n",
        "\n",
        "\n",
        "# Define function to load documents from a folder\n",
        "def load_files_from_folder(doc_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load documents from the given folder path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "    # Extract all files path from the given folder\n",
        "    extracted_docs = extract_documents(doc_path)\n",
        "\n",
        "    # Iterate through each document and extract the text chunks\n",
        "    for file_path in extracted_docs:\n",
        "        _documents.extend(load_file(file_path))\n",
        "\n",
        "    return _documents\n",
        "\n",
        "# =============================================================================\n",
        "# Load structured data in csv file to LangChain Document format\n",
        "def load_mcq_csvfiles(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load structured data in mcq csv file from the given file path and return a list of Document object.\n",
        "    Expected format: each row of csv is comma separated into \"mcq_number\", \"mcq_type\", \"text_content\"\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "\n",
        "    # iterate through each csv file and load each row into _dict_per_question format\n",
        "    # Ensure we process only CSV files\n",
        "    if not file_path.endswith(\".csv\"):\n",
        "        return _documents  # Skip non-CSV files\n",
        "    try:\n",
        "        # Open and read the CSV file\n",
        "        with open(file_path, mode='r', encoding='utf-8') as file:\n",
        "            reader = csv.DictReader(file)\n",
        "            for row in reader:\n",
        "                # Ensure required columns exist in the row\n",
        "                if not all(k in row for k in [\"mcq_number\", \"mcq_type\", \"text_content\"]): # Ensure required columns exist and exclude header\n",
        "                    logger.error(f\"Skipping row due to missing fields: {row}\")\n",
        "                    continue\n",
        "                # Tag each row of csv is comma separated into \"mcq_number\", \"mcq_type\", \"text_content\"\n",
        "                doc = Document(\n",
        "                    page_content = row[\"text_content\"], # text_content segment is separated by \"|\"\n",
        "                    metadata={\n",
        "                        \"source\": f\"{file_path}_{row['mcq_number']}\",  # file_path + mcq_number\n",
        "                        \"doc_id\": _generate_uuid(f\"{file_path}_{row['mcq_number']}\"),  # Unique ID\n",
        "                        \"source_type\": row[\"mcq_type\"],  # MCQ type\n",
        "                    }\n",
        "                )\n",
        "                _documents.append(doc)\n",
        "            logger.info(f\"Successfully loaded {len(_documents)} LangChain document chunks from {file_path}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {file_path}. Exception: {e}\\n.\")\n",
        "\n",
        "    return _documents\n",
        "\n",
        "# Define function to load documents from a folder for structured data in csv file\n",
        "def load_files_from_folder_mcq(doc_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load mcq csv file from the given folder path and return a list of Document objects.\n",
        "    \"\"\"\n",
        "    _documents = []\n",
        "    # Extract all files path from the given folder\n",
        "    extracted_docs = [\n",
        "        os.path.join(doc_path, file) for file in os.listdir(doc_path)\n",
        "        if file.endswith(\".csv\")  # Process only CSV files\n",
        "    ]\n",
        "\n",
        "    # Iterate through each document and extract the text chunks\n",
        "    for file_path in extracted_docs:\n",
        "        _documents.extend(load_mcq_csvfiles(file_path))\n",
        "\n",
        "    return _documents\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                         Website Extraction Functions\n",
        "# =============================================================================\n",
        "def _generate_uuid(page_content: str) -> str:\n",
        "    \"\"\"Generate a UUID for a chunk of text using MD5 hashing.\"\"\"\n",
        "    md5_hash = hashlib.md5(page_content.encode()).hexdigest()\n",
        "    return str(uuid.UUID(md5_hash[0:32]))\n",
        "\n",
        "def ensure_scheme(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    if not parsed_url.scheme:\n",
        "        return 'http://' + url  # Default to http, or use 'https://' if preferred\n",
        "    return url\n",
        "\n",
        "def extract_html(url: List[str]) -> List[Document]:\n",
        "    if isinstance(url, str):\n",
        "        url = [url]\n",
        "    \"\"\"\n",
        "    Extracts text from the HTML content of web pages listed in 'web_path'.\n",
        "    Returns a list of LangChain 'Document' objects.\n",
        "    \"\"\"\n",
        "    # Ensure all URLs have a scheme\n",
        "    web_paths = [ensure_scheme(u) for u in url]\n",
        "\n",
        "    loader = WebBaseLoader(web_paths)\n",
        "    loader.requests_per_second = 1\n",
        "    docs = loader.load()\n",
        "\n",
        "    # Iterate through each document, clean the content, removing excessive line return and store it in a LangChain Document\n",
        "    _documents = []\n",
        "    for doc in docs:\n",
        "        # Clean the concent\n",
        "        doc.page_content = doc.page_content.strip()\n",
        "        doc.page_content = doc.page_content.replace(\"\\n\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"\\r\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"\\t\", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"  \", \" \")\n",
        "        doc.page_content = doc.page_content.replace(\"   \", \" \")\n",
        "\n",
        "        # Store it in a LangChain Document\n",
        "        web_doc = Document(\n",
        "            page_content=doc.page_content,\n",
        "            metadata={\n",
        "                \"source\": doc.metadata.get(\"source\"),\n",
        "                \"doc_id\": _generate_uuid(doc.page_content),\n",
        "                \"source_type\": \"web\"\n",
        "            }\n",
        "        )\n",
        "        _documents.append(web_doc)\n",
        "    return _documents\n",
        "\n",
        "# =============================================================================\n",
        "#                         Vector Store Initialisation\n",
        "# =============================================================================\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
        "\n",
        "# Initialise vector stores\n",
        "general_vs = Chroma(\n",
        "    collection_name=\"general_vstore\",\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"./general_db\"\n",
        ")\n",
        "\n",
        "mcq_vs = Chroma(\n",
        "    collection_name=\"mcq_vstore\",\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=\"./mcq_db\"\n",
        ")\n",
        "\n",
        "in_memory_vs = Chroma(\n",
        "    collection_name=\"in_memory_vstore\",\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "# Split the documents into smaller chunks for better embedding coverage\n",
        "def split_text_into_chunks(docs: List[Document]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Splits a list of Documents into smaller text chunks using\n",
        "    RecursiveCharacterTextSplitter while preserving metadata.\n",
        "    Returns a list of Document objects.\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000, # Split into chunks of 1000 characters\n",
        "        chunk_overlap=200, # Overlap by 200 characters\n",
        "        add_start_index=True\n",
        "    )\n",
        "    chunked_docs = splitter.split_documents(docs)\n",
        "    return chunked_docs # List of Document objects\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                         Retrieval Tools\n",
        "# =============================================================================\n",
        "\n",
        "# Define a simple similarity search retrieval tool on msq_vs\n",
        "class MCQRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"The input text to search for.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def mcq_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Retrieve the top k most similar mcq question documents from the vector store\n",
        "    docs_func = mcq_vs.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\n",
        "        'k': k,\n",
        "        'filter':{\"source_type\": \"mcq_question\"}\n",
        "    },\n",
        "    )\n",
        "    docs_qns = docs_func.invoke(input, k=k)\n",
        "\n",
        "    # Extract the document IDs from the retrieved documents\n",
        "    doc_ids = [d.metadata.get(\"doc_id\") for d in docs_qns if \"doc_id\" in d.metadata]\n",
        "\n",
        "    # Retrieve full documents based on the doc_ids\n",
        "    docs = mcq_vs.get(where = {'doc_id': {\"$in\":doc_ids}})\n",
        "\n",
        "    qns_list = {}\n",
        "    for i, d in enumerate(docs['metadatas']):\n",
        "        qns_list[d['source'] + \" \" + d['source_type']] = docs['documents'][i]\n",
        "\n",
        "    return qns_list\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "mcq_retriever_tool = StructuredTool.from_function(\n",
        "    func = mcq_retriever,\n",
        "    name = \"MCQ Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve MCQ questions set when Human asks to generate a quiz related to a topic.\n",
        "    DO NOT GIVE THE ANSWERS to Human before Human has answered all the questions.\n",
        "\n",
        "    If Human give answers for questions you do not know, SAY you do not have the questions for the answer\n",
        "    and ASK if the Human want you to generate a new quiz and then SAVE THE QUIZ with Summary Tool before ending the conversation.\n",
        "\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The search topic to retrieve MCQ questions set related to the topic.\n",
        "        - k (int): Number of question set to retrieve.\n",
        "        Example usage: input='What is AI?', k=5\n",
        "\n",
        "    Returns:\n",
        "    - A dict of MCQ questions:\n",
        "    Key: 'metadata of question' e.g. './Documents/mcq/mcq.csv_Qn31 mcq_question' with suffix ['question', 'answer', 'answer_reason', 'options', 'wrong_options_reason']\n",
        "    Value: Text Content\n",
        "\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = MCQRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Return the response as a list of strings\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Retrieve more documents with higher diversity using MMR (Maximal Marginal Relevance) from the general vector store\n",
        "# Useful if the dataset has many similar documents\n",
        "class GenRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"The input text to search for.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def gen_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = general_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "general_retriever_tool = StructuredTool.from_function(\n",
        "    func = gen_retriever,\n",
        "    name = \"Assistant References Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve reference information from Assistant reference database for Human queries related to a topic or\n",
        "    and when Human asked to generate guides to learn or study about a topic.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "        Example usage: input='What is AI?', k=5\n",
        "    Returns:\n",
        "    - A list of retrieved document's content string.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = GenRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Return the content of the documents\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Retrieve more documents with higher diversity using MMR (Maximal Marginal Relevance) from the in-memory vector store\n",
        "# Query in-memory vector store only\n",
        "class InMemoryRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"The input text to search for.\")\n",
        "    k: int = Field(2, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def in_memory_retriever(input: str, k: int = 2) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "in_memory_retriever_tool = StructuredTool.from_function(\n",
        "    func = in_memory_retriever,\n",
        "    name = \"In-Memory Retrieval Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool when Human ask Assistant to retrieve information from documents that Human has uploaded.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = InMemoryRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False, # Whether to return the tool’s output directly\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Web Extraction Tool\n",
        "class WebExtractionRequest(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"The input text to search for.\")\n",
        "    url: str = Field(\n",
        "        ...,\n",
        "        title=\"url\",\n",
        "        description=\"urls to extract content from\"\n",
        "    )\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "# Extract content from a web URL, load into in_memory_vstore\n",
        "def extract_web_path_tool(input: str, url: str, k: int = 5) -> List[str]:\n",
        "    if isinstance(url, str):\n",
        "        url = [url]\n",
        "    \"\"\"\n",
        "    Extract content from the web URLs based on user's input.\n",
        "    Args:\n",
        "    - input: The input text to search for.\n",
        "    - url: URLs to extract content from.\n",
        "    - k: Number of results to retrieve.\n",
        "    Returns:\n",
        "     - A list of retrieved document's content string.\n",
        "    \"\"\"\n",
        "    # Extract content from the web\n",
        "    html_docs = extract_html(url)\n",
        "    if not html_docs:\n",
        "        return f\"No content extracted from {url}.\"\n",
        "\n",
        "    # Split the documents into smaller chunks for better embedding coverage\n",
        "    chunked_texts = split_text_into_chunks(html_docs)\n",
        "    in_memory_vs.add_documents(chunked_texts) # Add the chunked texts to the in-memory vector store\n",
        "\n",
        "    # Extract content from the in-memory vector store\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    docs_func = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\n",
        "        'k': k,\n",
        "        'lambda_mult': 0.25,\n",
        "        'filter':{\"source\": {\"$in\": url}}\n",
        "    },\n",
        "    )\n",
        "    docs = docs_func.invoke(input, k=k)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "web_extraction_tool = StructuredTool.from_function(\n",
        "    func = extract_web_path_tool,\n",
        "    name = \"Web Extraction Tool\",\n",
        "    description = (\n",
        "        \"Assistant should use this tool to extract content from web URLs based on user's input, \"\n",
        "        \"Web extraction is initially load into database and then return k: Number of results to retrieve\"\n",
        "    ),\n",
        "    args_schema = WebExtractionRequest,\n",
        "    return_direct = False, # Whether to return the tool’s output directly\n",
        "    verbose = False  # To log tool's progress\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Ensemble Retrieval from General and In-Memory Vector Stores\n",
        "class EnsembleRetrievalTool(BaseModel):\n",
        "    input: str = Field(..., title=\"input\", description=\"The input text to search for.\")\n",
        "    k: int = Field(5, title=\"Number of Results\", description=\"The number of results to retrieve.\")\n",
        "\n",
        "def ensemble_retriever(input: str, k: int = 5) -> List[str]:\n",
        "    # Use retriever of vector store to retrieve documents\n",
        "    general_retrieval = general_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    in_memory_retrieval = in_memory_vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs = {'k': k, 'lambda_mult': 0.25}\n",
        "    )\n",
        "\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "        retrievers=[general_retrieval, in_memory_retrieval],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "    docs = ensemble_retriever.invoke(input)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "ensemble_retriever_tool = StructuredTool.from_function(\n",
        "    func = ensemble_retriever,\n",
        "    name = \"Ensemble Retriever Tool\",\n",
        "    description = (\n",
        "    \"\"\"\n",
        "    Use this tool to retrieve information from reference database and\n",
        "    extraction of documents that Human has uploaded.\n",
        "\n",
        "    Input must be a JSON string with the schema:\n",
        "        - input (str): The user query.\n",
        "        - k (int): Number of results to retrieve.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    args_schema = EnsembleRetrievalTool,\n",
        "    response_format=\"content\",\n",
        "    return_direct = False\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model Using Huggingface (Experimental)"
      ],
      "metadata": {
        "id": "QNgX4ZVYrS8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "local_path = \"models/SmolLM2-Instruct\"\n",
        "\n",
        "snapshot_download(\n",
        "    repo_id=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
        "    local_dir=local_path,\n",
        "    revision=\"main\",\n",
        "    resume_download=True,\n",
        "    cache_dir=\"./cache\"\n",
        ")\n",
        "print(\"Model downloaded into \"+ local_path)"
      ],
      "metadata": {
        "id": "XA2wQpsTrZ8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/docs/transformers/main_classes/pipelines\n",
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=local_path)\n",
        "pipe(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ii5CWRZL-Yf",
        "outputId": "9d465320-ff9f-4172-acf6-e3abefb6534d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
              "   {'role': 'assistant',\n",
              "    'content': \"I'm SmolLM, an AI model trained by Hugging Face. I'm here to assist\"}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipe(messages)[0]['generated_text'][-1]['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivA2HbsXIpxN",
        "outputId": "aa316a11-d79c-465b-d9dc-acba6d63bf5f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm SmolLM, an AI model trained by Hugging Face. I'm here to assist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "local_path = \"models/SmolLM2-Instruct\"\n",
        "\n",
        "# https://python.langchain.com/docs/integrations/chat/huggingface/\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    #model_id=local_path,\n",
        "    model_id=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs=dict(\n",
        "        max_new_tokens=8192,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.03,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dA7Jd7daOvX",
        "outputId": "d9ca2abc-dda7-4240-c52d-ffd2a9f37e3a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | llm  # using llm directly from HuggingfacePipeline\n",
        "\n",
        "question = \"What is electroencephalography?\"\n",
        "\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UboTZh5YyH81",
        "outputId": "4f2c0fb9-d29e-4ae4-b75f-376bfcd6a7a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is electroencephalography?\n",
            "\n",
            "Answer: Let's think step by step. Electroencephalography (EEG) is a non-invasive neuroimaging technique used to measure the electrical activity of the brain. It is commonly used in clinical and research settings to diagnose and monitor neurological disorders, such as epilepsy, seizure disorders, and coma.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatHuggingFace(llm=llm, verbose=False)\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "\n",
        "events = model.invoke(messages)\n",
        "for event in events:\n",
        "    print(event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNHRrCY4ANLH",
        "outputId": "b645462d-9eba-4df9-e2de-8f33d7fd7254"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('content', \"<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nWho are you?<|im_end|>\\n<|im_start|>assistant\\nI'm SmolLM, an AI model trained on the Hugging Face library. I'm here to assist you with your questions and provide information on various topics. I'm designed to be friendly, helpful, and easy to understand. Feel free to ask me anything you'd like to know!\")\n",
            "('additional_kwargs', {})\n",
            "('response_metadata', {})\n",
            "('type', 'ai')\n",
            "('name', None)\n",
            "('id', 'run-89bd3b9d-fb08-4f25-bf3a-14e65b6147d1-0')\n",
            "('example', False)\n",
            "('tool_calls', [])\n",
            "('invalid_tool_calls', [])\n",
            "('usage_metadata', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"You are a helpful assistant.\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "])\n",
        "\n",
        "messages= template.format_messages(input=\"Who are you?\")\n",
        "\n",
        "events = model.invoke(messages)\n",
        "for event in events:\n",
        "    print(event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztFlzuyd05Db",
        "outputId": "6566d6b9-4d00-4148-8f19-953e3a398fb6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('content', \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nWho are you?<|im_end|>\\n<|im_start|>assistant\\nI'm an AI language model, developed to assist and answer questions to the best of my ability. I don't have personal experiences or opinions, but I can provide information on a wide range of topics. I'm here to help you with your queries, answer your questions, and provide information based on my training data.\")\n",
            "('additional_kwargs', {})\n",
            "('response_metadata', {})\n",
            "('type', 'ai')\n",
            "('name', None)\n",
            "('id', 'run-3a7d49ee-3ac1-4a18-b1b0-2414f7cd2933-0')\n",
            "('example', False)\n",
            "('tool_calls', [])\n",
            "('invalid_tool_calls', [])\n",
            "('usage_metadata', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "question = \"What is electroencephalography?\"\n",
        "\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYDtAHifi3fs",
        "outputId": "5d2f8642-d9de-4288-d826-94c68aa560c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is electroencephalography?\n",
            "\n",
            "Answer: Let's think step by step. Electroencephalography (EEG) is a non-invasive neuroimaging technique used to measure the electrical activity of the brain. It is commonly used in clinical and research settings to diagnose and monitor neurological disorders, such as epilepsy, seizure disorders, and coma.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chain.stream(question):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eM2O-qCj9sw",
        "outputId": "b959618b-89ff-438b-befa-f8dee2118176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Electroencephalography (EEG) is a non-invasive neuroimaging technique used to measure the electrical activity of the brain. It is commonly used in clinical and research settings to diagnose and monitor neurological disorders, such as epilepsy, seizure disorders, and coma."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuhaRfR6dLno"
      },
      "source": [
        "### Load Model and Serve Online (Experimental)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and run the model:\n",
        "!vllm serve \"HuggingFaceTB/SmolLM2-1.7B-Instruct\" --dtype=half"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdnTPhDHWUy1",
        "outputId": "144aaed9-42bb-44fd-edb1-15a73d885069"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-13 17:35:29.364457: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739468129.384918   48223 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739468129.391108   48223 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-13 17:35:29.418049: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 02-13 17:35:32 __init__.py:190] Automatically detected platform cuda.\n",
            "INFO 02-13 17:35:33 api_server.py:840] vLLM API server version 0.7.2\n",
            "INFO 02-13 17:35:33 api_server.py:841] args: Namespace(subparser='serve', model_tag='HuggingFaceTB/SmolLM2-1.7B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='HuggingFaceTB/SmolLM2-1.7B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='half', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7fdbece63ba0>)\n",
            "INFO 02-13 17:35:34 api_server.py:206] Started engine process with PID 48288\n",
            "WARNING 02-13 17:35:35 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "2025-02-13 17:35:41.712717: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739468141.745171   48288 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739468141.755121   48288 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "INFO 02-13 17:35:46 __init__.py:190] Automatically detected platform cuda.\n",
            "WARNING 02-13 17:35:50 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 02-13 17:35:52 config.py:542] This model supports multiple tasks: {'embed', 'score', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\n",
            "INFO 02-13 17:36:03 config.py:542] This model supports multiple tasks: {'reward', 'score', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n",
            "INFO 02-13 17:36:03 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='HuggingFaceTB/SmolLM2-1.7B-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-1.7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-1.7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n",
            "INFO 02-13 17:36:04 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 02-13 17:36:04 cuda.py:227] Using XFormers backend.\n",
            "INFO 02-13 17:36:05 model_runner.py:1110] Starting to load model HuggingFaceTB/SmolLM2-1.7B-Instruct...\n",
            "INFO 02-13 17:36:05 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-13 17:36:06 weight_utils.py:297] No model.safetensors.index.json found in remote.\n",
            "Loading safetensors checkpoint shards: 100% 1/1 [00:18<00:00, 18.23s/it]\n",
            "INFO 02-13 17:36:24 model_runner.py:1115] Loading model weights took 3.1887 GB\n",
            "INFO 02-13 17:36:27 worker.py:267] Memory profiling takes 1.98 seconds\n",
            "INFO 02-13 17:36:27 worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
            "INFO 02-13 17:36:27 worker.py:267] model weights take 3.19GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.48GiB; the rest of the memory reserved for KV Cache is 9.57GiB.\n",
            "INFO 02-13 17:36:27 executor_base.py:110] # CUDA blocks: 3265, # CPU blocks: 1365\n",
            "INFO 02-13 17:36:27 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 6.38x\n",
            "INFO 02-13 17:36:31 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "Capturing CUDA graph shapes: 100% 35/35 [00:31<00:00,  1.11it/s]\n",
            "INFO 02-13 17:37:03 model_runner.py:1562] Graph capturing finished in 32 secs, took 0.16 GiB\n",
            "INFO 02-13 17:37:03 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 38.48 seconds\n",
            "INFO 02-13 17:37:03 api_server.py:756] Using supplied chat template:\n",
            "INFO 02-13 17:37:03 api_server.py:756] None\n",
            "INFO 02-13 17:37:03 launcher.py:21] Available routes are:\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /openapi.json, Methods: HEAD, GET\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /docs, Methods: HEAD, GET\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /redoc, Methods: HEAD, GET\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /health, Methods: GET\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /ping, Methods: GET, POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /tokenize, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /detokenize, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /v1/models, Methods: GET\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /version, Methods: GET\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /v1/completions, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /v1/embeddings, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /pooling, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /score, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /v1/score, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /rerank, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /v1/rerank, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /v2/rerank, Methods: POST\n",
            "INFO 02-13 17:37:03 launcher.py:29] Route: /invocations, Methods: POST\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m48223\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "INFO 02-13 17:38:18 launcher.py:59] Shutting down FastAPI HTTP server.\n",
            "Exception ignored in: <function Socket.__del__ at 0x7b9f2a52c9a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/zmq/sugar/socket.py\", line 111, in __del__\n",
            "    def __del__(self):\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 374, in signal_handler\n",
            "    raise KeyboardInterrupt(\"MQLLMEngine terminated\")\n",
            "KeyboardInterrupt: MQLLMEngine terminated\n",
            "[rank0]:[W213 17:38:19.073703183 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
            "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "inference_server_url = \"http://localhost:8000/v1\"\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
        "    openai_api_key=\"EMPTY\", # max context window of 8192\n",
        "    openai_api_base=inference_server_url,\n",
        "    max_tokens=512,\n",
        "    temperature=0.5\n",
        ")\n"
      ],
      "metadata": {
        "id": "lVs_BOIZS2jm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model Offline Locally"
      ],
      "metadata": {
        "id": "aVe1Tbz2pY4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define a Customised Wrapper from Groq Wrapper"
      ],
      "metadata": {
        "id": "aKOLuuFsqwT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from typing import Any, Dict, Iterator, List, Optional\n",
        "\n",
        "import torch  # only if you need torch\n",
        "from pydantic import Field\n",
        "\n",
        "from langchain_community.llms import VLLM\n",
        "\n",
        "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    AIMessageChunk,\n",
        "    BaseMessage,\n",
        "    HumanMessage,\n",
        ")\n",
        "from langchain_core.messages.ai import UsageMetadata\n",
        "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class ChatVLLMWrapper(BaseChatModel):\n",
        "    model_name: str = Field(alias=\"model\")\n",
        "    trust_remote_code: bool = True\n",
        "    max_new_tokens: int = 512\n",
        "    top_k: int = 10\n",
        "    top_p: float = 0.95\n",
        "    temperature: float = 0.8\n",
        "    dtype: str = \"float16\"\n",
        "    max_retries: int = 2\n",
        "    timeout: Optional[int] = None\n",
        "    stop: Optional[List[str]] = None\n",
        "    _vllm: Any = None\n",
        "\n",
        "    def __init__(self, **kwargs: Any):\n",
        "        super().__init__(**kwargs)\n",
        "        logger.debug(f\"Initializing VLLM with model={self.model_name}\")\n",
        "        self._vllm = VLLM(\n",
        "            model=self.model_name,\n",
        "            trust_remote_code=self.trust_remote_code,\n",
        "            max_new_tokens=self.max_new_tokens,\n",
        "            top_k=self.top_k,\n",
        "            top_p=self.top_p,\n",
        "            temperature=self.temperature,\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "\n",
        "        prompt = self._prompt_from_messages(messages)\n",
        "        logger.debug(\"Invoking VLLM with input=%s\", prompt)\n",
        "\n",
        "        # NOTE: VLLM's signature is invoke(input=..., stop=..., etc.)\n",
        "        # so we call it with \"input=\":\n",
        "        output_text = self._vllm.invoke(\n",
        "            input=prompt,\n",
        "            stop=stop,\n",
        "            max_new_tokens=self.max_new_tokens,\n",
        "            top_k=self.top_k,\n",
        "            top_p=self.top_p,\n",
        "            temperature=self.temperature,\n",
        "        )\n",
        "\n",
        "        input_tokens = len(prompt)  # naive measure\n",
        "        output_tokens = len(output_text)\n",
        "        usage_metadata = UsageMetadata(\n",
        "            {\n",
        "                \"input_tokens\": input_tokens,\n",
        "                \"output_tokens\": output_tokens,\n",
        "                \"total_tokens\": input_tokens + output_tokens,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        message = AIMessage(\n",
        "            content=output_text,\n",
        "            additional_kwargs={},\n",
        "            response_metadata={\"time_in_seconds\": 1},\n",
        "            usage_metadata=usage_metadata,\n",
        "        )\n",
        "        return ChatResult(generations=[ChatGeneration(message=message)])\n",
        "\n",
        "    def _stream(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> Iterator[ChatGenerationChunk]:\n",
        "\n",
        "        prompt = self._prompt_from_messages(messages)\n",
        "        logger.debug(\"Streaming from VLLM with input=%s\", prompt)\n",
        "\n",
        "        output_text = self._vllm.invoke(\n",
        "            input=prompt,\n",
        "            stop=stop,\n",
        "            max_new_tokens=self.max_new_tokens,\n",
        "            top_k=self.top_k,\n",
        "            top_p=self.top_p,\n",
        "            temperature=self.temperature,\n",
        "        )\n",
        "\n",
        "        input_tokens = len(prompt)\n",
        "        for char in output_text:\n",
        "            usage_metadata = UsageMetadata(\n",
        "                {\n",
        "                    \"input_tokens\": input_tokens,\n",
        "                    \"output_tokens\": 1,\n",
        "                    \"total_tokens\": input_tokens + 1,\n",
        "                }\n",
        "            )\n",
        "            chunk = ChatGenerationChunk(\n",
        "                message=AIMessageChunk(content=char, usage_metadata=usage_metadata)\n",
        "            )\n",
        "            if run_manager:\n",
        "                run_manager.on_llm_new_token(char, chunk=chunk)\n",
        "            yield chunk\n",
        "            # Avoid double-counting\n",
        "            input_tokens = 0\n",
        "\n",
        "        # final chunk\n",
        "        chunk = ChatGenerationChunk(\n",
        "            message=AIMessageChunk(content=\"\", response_metadata={\"stream_done\": True})\n",
        "        )\n",
        "        if run_manager:\n",
        "            run_manager.on_llm_new_token(\"\", chunk=chunk)\n",
        "        yield chunk\n",
        "\n",
        "    ## Approach 1: Minimal override that just stores `tools` and returns self.\n",
        "    # def bind_tools(self, tools, **kwargs):\n",
        "    #     self._tools = tools\n",
        "    #     return self\n",
        "\n",
        "    ## Approach 2: Return a Runnable\n",
        "    def bind_tools(self, tools, **kwargs):\n",
        "        \"\"\"\n",
        "        Return a Runnable that calls this LLM.\n",
        "        (Adjust as needed if you want a tool-using agent approach.)\n",
        "        \"\"\"\n",
        "        from langchain.schema.runnable import Runnable, RunnableLambda, RunnableConfig\n",
        "\n",
        "        logger.debug(f\"Binding tools in ChatVLLMWrapper. Tools: {tools}\")\n",
        "        self._tools = tools\n",
        "\n",
        "        def _call_llm(input_str: str, config: RunnableConfig = None) -> str:\n",
        "            # If you want real tool usage, you'd parse input_str or augment it with tools.\n",
        "            response = self.invoke([HumanMessage(content=input_str)])\n",
        "            return response.content\n",
        "\n",
        "        return RunnableLambda(func=_call_llm)\n",
        "\n",
        "    def _prompt_from_messages(self, messages: List[BaseMessage]) -> str:\n",
        "        return \"\\n\".join([msg.content for msg in messages])\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"vllm-chat-model\"\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "            \"max_new_tokens\": self.max_new_tokens,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"top_k\": self.top_k,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"dtype\": self.dtype,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "4I3MjgGGcDWp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Executing the Wrapper"
      ],
      "metadata": {
        "id": "FUl4Dbhsq5xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://lunary.ai/blog/vllm-langchain-tutorial#leveraging-quantization-for-improved-efficiency\n",
        "# Original implementation from LangChain for non-Chat vLLM\n",
        "# import torch\n",
        "# from langchain_community.llms import VLLM\n",
        "\n",
        "# # Initializing the vLLM model\n",
        "# model = VLLM(\n",
        "#     model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
        "#     trust_remote_code=True,  # mandatory for Hugging Face models\n",
        "#     max_new_tokens=512, # max context window of 8192\n",
        "#     top_k=10,\n",
        "#     top_p=0.95,\n",
        "#     temperature=0.8,\n",
        "#     dtype=\"float16\",\n",
        "# )\n",
        "\n",
        "# Instantiate your custom chat model\n",
        "model = ChatVLLMWrapper(\n",
        "    model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
        "    trust_remote_code=True,\n",
        "    max_new_tokens=256,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    temperature=0.8,\n",
        "    dtype=\"float16\",\n",
        ")\n",
        "\n",
        "# Invoke a single question\n",
        "#result = llm.invoke([HumanMessage(content=\"What is Deep Learning?\")])\n",
        "#print(\"Response:\", result.content)\n",
        "\n",
        "# Or call batch\n",
        "# batch_inputs = [\n",
        "#     [HumanMessage(content=\"What is Deep Learning?\")],\n",
        "#     [HumanMessage(content=\"Explain the concept of attention in Transformers.\")],\n",
        "# ]\n",
        "# batch_outputs = llm.batch(batch_inputs)\n",
        "# for i, out in enumerate(batch_outputs):\n",
        "#     print(f\"Response #{i+1}:\", out.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648,
          "referenced_widgets": [
            "770f028885af4f9392e82d7bbc714879",
            "16bf7cd9702948ef88feb8a0e7515677",
            "0f5d34399a024201bd1c421eb5363d49",
            "d646df1f3dad448192fc7f18fb8613c8",
            "897a5d61912541179f776754908b89e6",
            "e8dfb7e1449c44e88d0e10c750cd120f",
            "f24520b133674d2e84828aaaa7aa55c0",
            "ca2b82e08bd9418eb14a922fa86f6bfc",
            "e27eb3f0cbb64c5d9a41e9eaa93ee1cb",
            "41b86ad9f3a44c0fb5be585a66e6555f",
            "5391b26c09b24610b2feeb07121fe36d",
            "89a90466976e4e548d44235cd040d4ad",
            "605aa96cbede40f0b9f143a7b7427df2",
            "fa5cc9a685b14a1bb22913588ecf16da",
            "75e23f90ce654da2a04fbab4cfb49bb4",
            "53b20ca01a5c4c5894bee59aceffd456",
            "b2ea99ff81e4459cac0f2500f16350e4",
            "71ad3e37a0974f0481151e372a3c5bbd",
            "c8beba89a63748c0817b8c010c7f1799",
            "07b6da1e83084ccca9059a3c8e030891",
            "d4b70ef492784b13804ed5d2b6d1830a",
            "1d99568f5490458c99f15a107ae357ea",
            "4d80a73aed274023b8cbde3f59b360de",
            "a1848d43a49d42bf949bfeac87272afd",
            "486a17181c9d46c0b46f102c30da0b7d",
            "5f54ace96b9d40adb116b36076f734e4",
            "bc0f7a37875b44f78e62574c3ad32abc",
            "f9309bbebf414c8586ea84a452eb6ff1",
            "ababd225747f437f9d769a23aedbbd0d",
            "1bd3fa0008a54f51b89f3ffc334f8915",
            "05960370cd4d4affbeecc954f114c387",
            "033b160d32664bf484db3fc69d95f856",
            "4869693d21cd4c719d23e4705f3268bc",
            "7e7267a30c39446c9023fc01904ff3bc",
            "1e004d1523bc49ce91a0e13bfe21f971",
            "897038ad2303471da212889a4306d536",
            "f7176074dba142339349a1fd89895753",
            "9fca3dd3f5a742fd96dc78c6f72c8ea5",
            "5677eb6d99914ef582fe37b5e88746db",
            "50ce67ea0ef04fa69a7078f43d3f595b",
            "84a5e7c71d14442dab690bad1fef7e1b",
            "2593399f84b34e769f5fbad6e804fcca",
            "c1e0dea3e0f049b5946ac04f639e8db1",
            "6101d082fde94635988b73a7323a2f1d",
            "1a4365bcab484d1eadf164cd83e07f09",
            "bddbcd1040cf413587a3d36e6b732a96",
            "60d28df8c72842a89c66f2357cee2377",
            "b50435e13cea4f3982568097233ab2de",
            "65dacbd22a07491187557bb1707b5677",
            "7aabbd856b17404b95f00009702aeb54",
            "639a15d578784b2aa39a96136125d4b3",
            "8b2fd43a8934456286f159bd8f3e6732",
            "b3de0fb22e45454d80aca6cdaf52ea48",
            "8fe5af6bf42f4235a6374c3f39ac3110",
            "2c48620a0daa4343b7712daf6c1f8713",
            "ef325101d5d64f4388ed7cd61396f5d4",
            "f2e58daff32d41058fbc4c04e764e3e3",
            "b8f37919337349f2be758a9cbf8c8a2a",
            "7816fe00c5cb4e68928bbff96e0cfec6",
            "204b5e765c0b449fb97b842e0707e202",
            "43bef004f83d4676b45e11b4d5ce306c",
            "eeb39ed614a644d499b19f0688bf3388",
            "77986ef0e37a4c1b85b1ac9cdd159cc0",
            "ab22126c60e64fdfb1d07dd2a89fb13c",
            "3bdbed976bc444f4a9a2517af21986bb",
            "80767c88cf354d8c958ba6ab540bec8c",
            "86e17d77e1eb42f9be9644a61693603d",
            "cf77a7a6e6d74f6da05928ee5c3901b4",
            "f5d5d043eebb41f4ab22491729856787",
            "3970f6fae8af4613928bb5a60307c028",
            "70f18d8c74a94b549bcc492e09dc8373",
            "8e4483e50f174d2f875d938bfb97c4fd",
            "ff35643236f246e28f8512cb08cc2d93",
            "4080b87eed094001bae4aef25ad67926",
            "a578d517f0254620be90965e64c0ad30",
            "50790f8cd8d944ca883e589da30223cc",
            "75d51412fb3b44df92f83a8159aed242",
            "f4d6407b792e48f998c47e0109f6a498",
            "4022c089a83b4f8097c665685f7a2ce2",
            "c8afbd7c635d4b628b35102fa7b649c3",
            "6eb6ae513fb64927b1b357c28c22e6ad",
            "5f526680d15d440ebe78556b5273b992",
            "f3fc11603cf544568944077a39f4ea6f",
            "4c6efd5d702c424fb47c89a88d9608ce",
            "997fce5689fd44e989b131109deb4589",
            "de5210e75e384cba91d5e1ebb73c782f",
            "cecb5515399c41c6beed3043f5449f49",
            "0caac675384e4423948096bf8004f80b",
            "9a0aede5c40c42608a454242cb38d1bb",
            "05fddb3210ff47dfb2ca26611a862665",
            "026da44b05864637a7600a5b16ddeb90",
            "f5c7d57156a74ee09560812c0d77b054",
            "3c9cc8340d9d48aebd8f59c75b98aca6",
            "be38bce41fcc48949c060cdeb7c362df",
            "eed7922cb2fa4045956277a08b535ef6",
            "f2476fea509847b184f2aed5556ee6f5",
            "6a3aef9a80294c84b952061a5f6ea818",
            "994e19edf255417d972ba925f0fb7bb5",
            "943c1bdc51e741529fed198646746241"
          ]
        },
        "id": "AzGYC78jcI-i",
        "outputId": "e0c80a7b-2a43-42d9-998d-99cdbf0a327b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-14 15:58:31 __init__.py:190] Automatically detected platform cuda.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "770f028885af4f9392e82d7bbc714879"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 02-14 15:58:34 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 02-14 15:58:48 config.py:542] This model supports multiple tasks: {'generate', 'reward', 'classify', 'score', 'embed'}. Defaulting to 'generate'.\n",
            "INFO 02-14 15:58:48 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='HuggingFaceTB/SmolLM2-1.7B-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-1.7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-1.7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89a90466976e4e548d44235cd040d4ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d80a73aed274023b8cbde3f59b360de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e7267a30c39446c9023fc01904ff3bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a4365bcab484d1eadf164cd83e07f09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef325101d5d64f4388ed7cd61396f5d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86e17d77e1eb42f9be9644a61693603d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-14 15:58:50 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 02-14 15:58:50 cuda.py:227] Using XFormers backend.\n",
            "INFO 02-14 15:58:51 model_runner.py:1110] Starting to load model HuggingFaceTB/SmolLM2-1.7B-Instruct...\n",
            "INFO 02-14 15:58:51 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4d6407b792e48f998c47e0109f6a498"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-14 16:00:13 weight_utils.py:297] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a0aede5c40c42608a454242cb38d1bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-14 16:00:21 model_runner.py:1115] Loading model weights took 3.1887 GB\n",
            "INFO 02-14 16:00:23 worker.py:267] Memory profiling takes 2.10 seconds\n",
            "INFO 02-14 16:00:23 worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
            "INFO 02-14 16:00:23 worker.py:267] model weights take 3.19GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.48GiB; the rest of the memory reserved for KV Cache is 9.57GiB.\n",
            "INFO 02-14 16:00:24 executor_base.py:110] # CUDA blocks: 3265, # CPU blocks: 1365\n",
            "INFO 02-14 16:00:24 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 6.38x\n",
            "INFO 02-14 16:00:29 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:41<00:00,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-14 16:01:11 model_runner.py:1562] Graph capturing finished in 42 secs, took 0.16 GiB\n",
            "INFO 02-14 16:01:11 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 50.12 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running a simple query\n",
        "model.invoke(\"What is Deep Learning?\").pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9mSwucUd1yn",
        "outputId": "a07910c2-0993-403a-ea7c-befd25e8e429"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.46s/it, est. speed input: 1.12 toks/s, output: 57.41 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "\n",
            "Deep learning is a subset of machine learning that uses artificial neural networks to model complex patterns in data. These neural networks are designed to mimic the structure and function of the human brain, with multiple layers of interconnected nodes or \"neurons\" that process and transform the input data.\n",
            "\n",
            "The key characteristics of deep learning include:\n",
            "\n",
            "1. **Large amounts of data**: Deep learning requires massive amounts of data to train the neural network, which can be difficult to obtain.\n",
            "2. **Complex data structures**: Deep learning is particularly well-suited for dealing with complex data structures, such as images, audio, and text.\n",
            "3. **Large computational resources**: Deep learning requires significant computational resources to train and deploy the models.\n",
            "\n",
            "Some of the most common deep learning models include:\n",
            "\n",
            "1. **Convolutional Neural Networks (CNNs)**: Used for image classification, object detection, and segmentation.\n",
            "2. **Recurrent Neural Networks (RNNs)**: Used for natural language processing, speech recognition, and time-series forecasting.\n",
            "3. **Long Short-Term Memory (LSTM) Networks**: A type of RNN that is well-suited for long-term dependencies in sequential data.\n",
            "4. **Graph Neural Networks**: Used for graph\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangGraph"
      ],
      "metadata": {
        "id": "k7soxxBDs_qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install langchain-groq"
      ],
      "metadata": {
        "id": "zfpYXOlODQ-u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Initialize Groq LLM\n",
        "model = ChatGroq(\n",
        "    model_name=\"deepseek-r1-distill-llama-70b\",   #\"llama-3.2-3b-preview\", \"deepseek-r1-distill-llama-70b\"\n",
        "    temperature=0.6,\n",
        "    api_key=GROQ_API_KEY,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(model.invoke(\"Who are you?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyup3FLGDE34",
        "outputId": "6fe4c44b-6e34-400d-e7d1-b685ec1af0ee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"<think>\\n\\n</think>\\n\\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 7, 'total_tokens': 51, 'completion_time': 0.16, 'prompt_time': 0.003493012, 'queue_time': 0.048457706, 'total_time': 0.163493012}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_95405939f8', 'finish_reason': 'stop', 'logprobs': None} id='run-87cc9e4e-e673-4595-9e0f-2cc6f920a6c1-0' usage_metadata={'input_tokens': 7, 'output_tokens': 44, 'total_tokens': 51}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "SXMyAFM6bSiz",
        "outputId": "5b000888-ea13-48a8-ffda-7e348955e43d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-89c04fcc66da>:36: LangChainBetaWarning: The function `init_embeddings` is in beta. It is actively being worked on, so the API may change.\n",
            "  \"embed\": init_embeddings(\"huggingface:sentence-transformers/all-MiniLM-L6-v2\"),\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOEAAAFNCAIAAAB5eJ8MAAAAAXNSR0IArs4c6QAAIABJREFUeJzt3XdcU1f/B/CT5GYPEjZhr6KCuFFRcc/iwlXRqrXDWmuHta212vq4a7VqtfNxdGjdrYqiIA4QEbV1r9aBIiNABtnr5ub3R/xRHkwQMck9Ief9R18Ycu/5pvlw7jr3XIrVagUIAjEq2QUgyFOgjCKwQxlFYIcyisAOZRSBHcooAjuM7AKeS3WZQaey6NQWs5Ew6gmyy2kSJptKwygcPo3DpwVHsckuxwNQPPH86IOb2vvXtCXXteEJbKOO4PBpwkCGxewZH4TBpiqqTDq1hUIBD25qo5O4MW15CZ34ZNcFLw/L6IOb2qIsWVAEMziKFZ3E5fA9eztgwa0l17X3r2ke3NSlDvdLSvUhuyIYeUxGrYQ159cqs5FIHe7nF8IkuxwnM+otRVmysju6odNC/ENb2qd7Tp6R0Zpy4641j8a+GxYcySK7FhdSyc2HN1d27CtK6Iw2/f/ygIyq5ObszZUvfRhBdiFukrtN8kJHflQbLtmFwAL2jJbf1Rful06YG052IW6V87MkIJzZsZ+I7EKgAPX5UYPWkr210tsCCgAYPDW47I7+4S0t2YVAAeqMHttelfmx1wXUZsQM8fUzKrXCTHYh5IM3o5dOKkRBDK6ATnYhpEnozC88ICW7CvLBm9GiLFnqcD+yqyBTXHueSopXlxnILoRkkGb04glFWoY/lUohuxCS9Rjpd6NIRXYVJIM0o7fOq0LjOO5py2KxXL58mazFGxcWz/n7T7XZ5BlDEVwExowqpWYLbvUNZrinuSVLlixfvpysxZ8qOolbct2rD/BhzGjpbW2rLgK3NWc0Gpu3oO3UcrMXb6L4DryK+3qXNgE5GMdkyCpNfmKXXLMuLCzcsGFDWVmZWCweO3bshAkTFi1adOzYMQBA586dAQAHDx4Ui8UHDx7cvXv33bt3ORxO9+7d586dKxKJAAB5eXnz5s1bvXr1r7/+euPGjalTp1ZVVT25uHNr5ovokgdefdgEY0a1KktEK5rTV6vT6T7++OOYmJgFCxbcvXu3pqYGADB9+vSqqqry8vLFixcDAPz9/QEA165di4qKGjZsmFwu37lzp1arXbduXd16vvjii1mzZs2cOTMiIsJgMDy5uHNxBDSdyuL01XoQODOKcwXOL0wulxuNxn79+g0dOrTuxYiICKFQKJPJ2rdvX/fi/PnzKZTHpxQwDNuyZYvRaGQyH3ftEyZMSE9Pr3vzk4s7F1eA6dQWq9VaV5K3gTGjGJ1Cc0FdoaGhycnJmzdvZrPZGRkZDIbDYzKz2bxz587s7GyJRMJisQiCUCgUwcHBtt+mpKQ4v7hGcQU0ArfS6F6aURiPmegMqkbp/K0bhUL5+uuv09PT161bl5GRcfHiRbtvs1qt77333pYtW0aMGLFx48Zhw4YBAAji37M/HI6bTorZ6LUWC26l0WH8ptwDxk/OFWBaFe6KNfN4vHnz5u3bt4/H482ZM0en09lerz/46+LFi+fPn583b15mZmZSUlJcXNxTV+vSsWM6Fc5xwZ6PB4Exo74hdLPRJWetbeeJQkNDX3rpJY1GU1FRAQBgs9kymayup6ytrQUAtGrVqv4/6/ejDTRY3Ol0alwc05JHdj8VbdGiRWTX0BCNTr1wVJ7Uw8k395jN5oyMjJqaGqlUumvXLqPR+NZbb2EYplarc3JyampqVCqVRCJJTEzcs2dPZWUll8s9ceLEpk2bzGZz586do6Ki7t+/n5eXN378eKFQWLfaBotHRkY6t+yLx2sDwpiB4d4bUxgzyhVgF47J49vzGSxndvNarba0tPTkyZMnTpwICAhYtGhRWFgYACAuLk6pVB49evTixYtCobBv374xMTFZWVlZWVk4ji9durS6uvry5cvp6el2M9pgcacfUZ3cVd1zlD+DCeMWzz0gHYdffEQm9Ke782oTnGrKDJdO1g56OZjsQsgE6c54u17C7SsfNpLR06dPL1y48MnXmUymo4uTW7dujY6OdmqZDWk0mvqnTutLTk6+evXqk69/8MEHw4cPd7TCs4fl7dK8/YZmSPtRAEDhASlXQOvQ1/49PQaDQS6XP/m6yWRydOIzMDAQw1z7N0kQhEQieaZFfHx8uFz7t9dV3NOfzZaNmR3mpOo8FbwZtViIg99VjH7be7+h4zuqErsJgqO9fb4dePfEaTRq6nD/3WsfkV0IOfL31QSEMVFAoc4oACAokpXYXXDkp0qyC3G38zkyArcm9xI24b0tH7zb+jqP/tZdP6scOi2E7ELc5EKunEqjdOqPbq5/DOp+1CY8gROTxNuxqtTkmotPUMn9VWI2Eiig9XlAP2ojrTCe2lMtjmGnDnf+GE0YXD1de+6oPC0jAM3z2IDHZNTmr+OKs4dk3Yb5hsazQ1rEBLPyKlPJNe3VwtqYtrzu6b4MpvMHd3s6D8uozeV8xd1L2lqpqU03AbACrgAT+NE95WPQaBSVzKxV4Rbcev+alkoF0W25yT2FPCGk11NI55EZtdFrLI/u6NRyXKvCrRagUTp5OJ9UKtVqtU4fIyLwpVssBFeA8UVYcBRLGOCm2189lwdn1NX2799/7do1u1dcEXfygON6xMuhjCKwQxl1iMlk2m6rR8iFMuqQ0WhUKBRkV4GgjDpGo9Hq7qlHSIQy6pDFYnH1XE5IU6CMOkSn0x2NPkbcCWXUIbPZrNV69aSKkEAZdYjFYvn5efVk55BAGXXIYDDIZDKyq0BQRhHooYw6hGEYm90Shv95OpRRh3Ac1+u9epJvSKCMOoRhGIvlvbMswQNl1CEcxw0Gr56IHhIoowjsUEYdYjKZPj7ePtcSDFBGHTIajUqlkuwqEJRRBHooow6ha6GQQBl1CF0LhQTKKAI7lFGHWCyWK56tiDwrlFGHDAaDVColuwoEZRSBHsqoQ+jeZUigjDqE7l2GBMooAjuUUYfQ/fWQQBl1CN1fDwmUUYcYDAYa9wQDlFGHTCYTGvcEA5RRBHYoow7R6XQOh0N2FQjKqGNms1mn05FdBYIy6hgaPwoJlFGH0PhRSKCMOoT6UUigjDqE+lFIoIw6xGAweDwe2VUg6BliTxg9ejSO4wAAvV5vNpsFAoHt57y8PLJL81LoGZUNJSUlHT58mEp9vIXRarVWqzUhIYHsurwX2tY3NG3atJCQkPqvMJnMSZMmkVeRt0MZbSg2NrZTp071d4EiIiJefPFFUovyaiijdkyaNCkwMND2M4PBmDJlCtkVeTWUUTsSEhJSUlJsP0dHRw8bNozsirwayqh9kydPDgwM5HK5kydPJrsWb+fZx/VmEyGrMOk0FqevmQbE3duNKisrSwhPu3/d+U9pYjCpfmIGm0tz+ppbHg8+P5q/t+buFQ3fl87ieN43zWBRH/2tDYvnDHo5iIZRyC4Hap6a0cObKwPC2a27Csku5LlIHugu5EjHzA5lsj3vz8xtPDKjOb9I/MPYL3RqCTcbKaWmk7sqX54fSXYh8PK8YybJA70Zt7aMgAIAfPwZUW14N86iG6cc8ryMyiVmOuZ5ZTeCI8CqStFN0g553petVeE+gS1qagaBP8OoJ8iuAl6ed+7JgltxvEV9o1YLMGidf/qsxfC8fhTxNiijCOxQRhHYoYwisEMZRWCHMorADmUUgR3KKAI7lFEEdiijCOxQRhHYoYw6081b19FjHpwOZdRpjuZkzXp7msGgJ7uQlgZl9F9KZa1KrWr24qgHdRHPG5vXDEeOHty/f/f9krtsNielS/e3Z80VCh8/CDQn59D2HVurqyXRUbEUKjU4KOSzhSsAAJWSim+//eqvi+cYDOYL8a2mT3+rVUIbAMCCzz4ID4vEMOzQ4T9ws7lbt57vvjOPx+Mdzclat34lAGBUxgAAwLyPFg0enE72524hvKIfvXnzWkRE1Iw33hmennGmKP+LL/9je73wzKmVqxa1S+64YP4yOoNx69b1sWMyAQAymXT2O9NVauXbs+bOeOMds9n87nuvlZTcsy21e882iaRi+bJ1b8+aeyo/b9v2zQCArik9xo+bDABYsWzd1+s2paSkkvqJWxSv6EfnvD+fQnl8fzCGYdu2bzEajUwm88CBPVFRMR/M+RQA0KpV4rgJQ4vPFbZp0/bXbZtEQt81X36HYRgAYOCAYZOnjDqU/cfsWXMBAGFhEfM/WUKhUFq3SiwoPHHhz7NvznhXJPIVi8MAAK1bJ/n4ePbdqrDxioyazebf/9h5LC+7ulrCZLIIgqitVQQFBVfXVIWFRdje4+8fwGKx1GoVAODcuTPVNVXD0nvVX0NNdZXtZxaTVZf4oKCQ69evkPGZvEjLz6jVap3/6Xt//3Nz6pQ32rRJPn36xM5dvxBWAgAgFof9/fdNk8nEYDDu379rMBji4hIAAHKFrHv3Xm+8Nrv+erhcO3M60zE6QaDbPFyr5Wf0ypWLf108/+n8pQP6DwEAlJeV1v1q4oSpc+a+OWfum506phw7lt0qoc3gQekAAD5foFTWRkRENaM5T5yvAHIt/5hJqaoFALwQ36r+PwmCAAAkJbUbkzGRIIiKirIJE6asW/tf2w5ox44p169f+fufW3Ur0eufftaTzWIDAKTSGld+Gm/U8vvRNq3bMhiM/27a+OKLo+/fv/Pbjq0AgJL7d0PFYXv2br906cL48S9TKBQMw8rKSmNj4wEAU6e8UVxc+OFHs8aPmywS+Z4/X2QhLEsXr2m8ocSkdjQabeO3q4cOHmEym4anZ7jrI7ZwtEWLFpFdw7Mpu6MnCEpwFLuJ7+dyuVFRMUdzso7mZOE4/un8pVJp9fXrlwcPTsfNeM6xQzm5hwpOnziVn3cwa59cLu3evZeAL+iR2vthacmxY4cv/HmWy+W9OGxUVFQMAODEyVydVluXvz//LL5z93bmxGkAAAFfEBAQdOrUsbNnT2s06kGDmjr1s0aB1zzSt04RNPd/SQvnefM9FWfLcJzSrrevU9ZmsVhoNJrtSeA//Pfr/ft35xwpsm3x3abyvv5GkXz0rFB3NupBWv62vhG5uYc3bfmmb59BISGhCoXs9OkTUVExbg4o8lRe/X1ERsW0TWqfd/yISqX08/Pvkdp78qRXyS4KacirM5rwQuuFC5aTXQXyFC3/3BPi6VBGEdihjCKwQxlFYIcyisAOZRSBHcooAjuUUQR2KKMI7FBGEdh5XkZZHBrG8LyyG2EFwMefTnYV8PK8L9vHn171QEd2Fc5UU6Zn89DzQh3yvIyGvcBqYU8zUtaYotpwyK4CXp6XUTqD1mWw77Ffy8kuxDmKsqr9xQy1+RHZhcDL88bh25Td0R/7rSq5l0gUxOTwPW+EIW4masoMFfe0IVGsjv1Eq1at4vP5M2fOJLsuGHnet2sTFs/+q/KbmNqP7l9Va5W4K5rAzWYLQTCZLnk2qW8wk82jJqUKIltxAQAfffTRwYMHAQBVVVVBQUGuaNFzeWo/umLFikGDBnXq1Ml1TUyfPl2pVG7YsEEsFruulQZOnDhRXFw8f/58t7UIP8/bHz106BAA4JNPPnFpQLOzsx8+fPjw4cO9e/e6rpUn9evXLyEh4fr16yaTyZ3twszDMrpx40a1Wu2Ghnbu3KlUKgEABQUFZWVlbmixzpgxY1q3bq3T6ZYsWeLOdqHlMRmVSCQAgJ49e06cONHVbWVnZz948MD284MHD/bt2+fqFhug0WhCobBt27Z5eXkeujPmRJ6R0cLCwo0bNwIA2rdv74bmdu3apdP9e5mgoKDg0SMSzg2NGjWqT58+Vqt1xYoV7m8dHp6R0ZKSkqVLl7qnrcOHD9d1ojalpaU7d+50T+sNYBhGpVLj4+Pff/99UgqAAdTH9TiO//zzz6++6tZ73jMzM2/fvl03w6hNSEiI7ViNLGq1ms/nFxUVpaZ63wzRVlgRBJGSklJVVUVWAb///vuSJUvIat2uy5cvjxw5EsdxsgtxK0i39Xfu3MFx/Ny5c4GBgWTVQKfT/f39yWrdrnbt2m3YsEGn05WXt5BLwU0BY0bXrFkjl8vpdJKHqymVyvpHTpAIDw/n8/lGo3H27NlNeHtLAF1GNRpNSEhI165dyS4EmEwmoRDSpy/ExMRMnDjx6NGjZBfiDnBltKCggMViZWZmkl0IsJ2RFQjgnRM0NTV1yJAhAIDNmzeTXYtrwZJRgiDS0tI6duwIz9SKOp3OI4Z3EATxyy+/kF2FC0GRUYPBUFZWduTIER7PzrM7yHLr1q2wsDCyq3i6119/vVevXgCAyspKsmtxCfIzWlFRUVhYGBERweVyya7lf4hEoqio5jxaxP2io6MBAGvXrj137hzZtTgfyRklCGLGjBkDBgwgt4wn2UYeNTiTD7lVq1ZdvXqV7Cqcj+SM6vX6rKwscmuw6/bt2zCcW3hWr7/+OgBg69atZBfiTKRlVKPRLF++HLbte51Tp0516NCB7CqaqVOnTgsWLCC7Cqch53o9QRDjxo1z/5i3JjIajX379i0qKiK7kOa7d+9ebGws2VU4Bzn9KJVKhTagAICioqLx48eTXcVzsQV08eLFZBfiBCRk9MCBAw0Gv8Fm27Ztffr0IbsKJ5g3b97LL79MdhXPy90Z/emnn0pLS2E+p3P37l2NRuOewdSuxmAwWsDxk1v3R61WK47jpA8WadyaNWuSkpIGDx5MdiFOo1Aoli9f/uWXX5JdSDO5tR89efKk2Wx2Z4vP6t69e+fOnWtJAbVdjJg1a5bnXi9138XxHTt2lJeX9+vXz20tNsP69evfffddsqtwvqioKJj3rxrnvn7UYrHMnTvXbc01Q35+PpPJ7NGjB9mFuMquXbtu375NdhXPDOr7mdyse/fu+fn5DAaD7EJchSCIrl27XrhwgexCno2b+tHPPvtMLpe7p63mWbp06cKFC1twQG2npYuLiz1uBhR3ZPTy5cvl5eW+vs554rwrHDlyxGAwDBs2jOxCXI5Go129elWlUpFdyDNwx7beYDDQaDRoTzmVl5fPnDnTNm2dNygsLNyzZ8/69evJLqSp3HFcz2Kx3NBKs82fP78FnOhuup49e0okEqlUCttdr464ox/t37//8ePHXd1K88ycOfOVV15JSUkhuxDEIZfvjz58+NDHx8fVrTTPsmXLBg4c6IUBJQhi5cqVZFfRVC7PaFhY2I4dO1zdSjPs3bs3JCQkIyOD7EJIQKVSKyoqzpw5Q3YhTeKl50cPHjx46dKlzz//nOxCSKNQKIxGY3BwMNmFPJ3L+9Hc3NyFCxe6upVncurUqfz8fG8OqO0ivkcE1B0ZpdFoRqPR1a003ZUrVwoKCtasWUN2ISQjCGLWrFlkV9EkLj/3lJaW1q1bN1e30kR//fXXDz/88OOPP5JdCPmoVGppaWlFRYU7n0jRPF60P7p///7s7GwU0DoVFRUikYjNZpNdyFO4fFtvMBhgGElUUFBQUlKCAlqfWCyGP6DuyCiLxRKJRORO83LixIk//vjDm6frtmvLli1//fUX2VU8nTu29Xq9fsKECTqdTqVSBQYGunnS7r179z548ADyoavuNG7cOAzDKBRKVVUVj8fjcDi26Vh+++03skuzz4XHTGlpabY5Zq1Wq+3/gtVqbdOmjetafNLWrVslEsknn3zizkYhZzabS0pKbD/bnkFFEAQM+2OOuHBb369fPyqVCgComzWJyWS6c4Ka1atXG41GFNAG0tPTG8xj5e/v/9prr5FX0VO4MKOLFi1q06ZN/X2JgICAdu3aua7F+hYvXhwaGvrmm2+6pzkP8tJLL9Wfs9JqtbZt29Zt30szuPaY6Ysvvqi71ctqtXI4nLi4OJe2aDNjxozU1FQ3PBHPE/F4vPqjuf38/KZMmUJqRU/h2owGBQW9//77tnGKFArFPX+sY8aMef311yGcLxIemZmZkZGRtp+Tk5Nh7kTdce6pZ8+eGRkZXC6Xx+O5emfUZDL16dNnzZo1nTt3dmlDno7L5Q4fPpxGo/n6+kLeiTb1uB43E3oN0ew2Jo6b/vBe9b1792IiEtUKvNnraZxWox09fkhubi6fz3dRE06nU+EWCzlNDxkw+kjWqZiYmOjwNq77UhrH4dNo2NNnIX7K+dFb51VXTyvlEhObR3ueaupOP7kOxjaoqrCYZF6XgSI/MdOlbT2/okPS2xfUwgCGSgb1xC0uZdBZhAH0dmnC1imNPb+lsYyez5VLK8zte/vyfSG9Xa4BwmKtrTEV7JMMyAwKiYL0JirCYt37dVlcB0FoHJfDh+UhKmRRy81XTsn8QxldBjm8bdhhRs8dlatkeLd00h6F+DwOfFs6cFJgUASMMd391aO2ab5h8ZBOYE2K4sPVAl+s6xD7MbV/zKSoNknLjR4aUABAv4khf+YqyK7CjhtnlaHxXBTQBrq9GCgtNyqq7U9OYT+j0nKj1epJj9RogC+iP7qjMxmbf5znIpUlBrR9d4BSU2Z/LLz9jGqUloBwGDeUTRfZhiuvhGj8v40FtwqDWvJ0Pc0WGMFydHrB/t+02UiYDS4uysVUMhwA6DYFKhluJelkE+RMBiuNZn+7R/5z7hCkcSijCOxQRhHYoYwisEMZRWCHMorADmUUgR3KKAI7lFEEdiijCOxQRhHYOTOjN29df85pHE/l5/Xt37m0FOonhyNu5rSMHs3JmvX2NINB76wVIoiN0zIK1US4iDu5esow54y3PZqTtW79SgDAqIwBAICPP/p8yODhAIDc3MPbd2ytqCjz8/N/cdjoSZmv2GbXwXF860/f5+QeUiprIyOjp02d0bNHnydXW1xc+OOmDRUVZcHB4hHDx2aMnuCUaj2IwWBY9/XKoqICAEBycoe335obHBwy+91X2Sz2qi822t6za/ev3/+w/mj2GSaTOXxkn9mzPjx+MufSpQs8Hn9A/6HJyR22/vR9WVlpdFTs++/PT3ihNQBgwWcfRIRHGYyG3NxDVqu1Y4eUMRkTt23ffP3GFV+R3yvT3hw4cBgAoLq6avPWb8+dO6PVasLDIzMnvjKg/xBbo6+8Oj46KjYqKvb3P3YajYYJ46f8tmPrnt1HfQSPnyGzbMVCGpU27+NFz/8/wTn9aNeUHuPHTQYArFi27ut1m7qm9AAA5OQcWvHF5/HxrRYuWN6n98AtW7/b/tvjR3WtXrN01+5f018c/en8pcHB4oWfzb169VKDdep0ukWLP2bQGR/MWZDaPU0mq3FKqZ7ltx1bc3IOjR2TOeONd1QqZVOmC12zdllq97T16zYlt+2wZ+/2detXvjZ91soVX+sN+v/852McfzyOeMfOnwEAX635YcL4KYVnTn348awePfqs/erHuLiElasW2Q4JcAt++/aNkSPGzpzxnkDgs2z5glu3b9Q1dOHC2dt/31i+dO2SxWuGp2dYLJaTJ3NtvzKbzcXFpzu0d84sB87pR0UiX7E4DADQunWSj4/Q1v9v2vJN27btF8xfCgBI69VPrVbt3PXzmIyJUml1Tu6hKS+/Nm3qDABA77T+k6eM/unnH75a8339dSpq5UajsVevfgMHDHVKkZ6oUlLBZrMzJ07DMOzFYaOassjQISNGjhgLAJgx4938guOTMqd3794LADBp4isrvvi8oqIsIiIKABAZGf3O2x8CAF6Ib5V9ZH+rhMTRo8YDAGa99cHpwpOXr/wVERElDgn9acse203nQ4eOHD1mwJkzp1q3SrQ1RMOwhZ8ur/uz6dKle07uoVEjxwEA/vyzWKPRtHdSRl117qmsrFQqrUnr1a/ulS5duut0urLy0itXLwIAevbsa3udQqF06dzt739uNliDOCQ0MTF52/bN+37f6XGPCnaWAf2HGgyGj+fNvn//bhMXYTIf3+TDoDMAAHVPkg4IDAIAKJW1j9/G+HcKAgaDif3/01wD//dtd+/98+nCOWPHD3l56miLxSKXy+qWat06qX6/PmTw8Nu3b9g64FMFebGx8UFBznluiasyqtFqAABC4b93o/L5AgCAtKZaq9UAAET1fiUQ+Oh0Oq1WW38NFApl5fKvBw9K//6HdVOmZVy5ctFFpcKsa0rqiuXr5QrZq6+/tHrN0rottevUzRQLALh46cJbs6aaTaaPPvz8P5+vEgh8COu/t3OwWf+z49EjtbdA4JOTe8hsNhedye/fb4izSnJyRusO8QID/ufPEQCgUMhtSfX3DwQAqFTKul/J5TIMw5589C2Px3vv3Xk//7SPy+UtWDjHNuOut+makrr5vzvfmvn+4ez9tp1IV8/4UufXXzeJxWHLl61L6dI9MTG5QSgboNPpAwYMzT12+Pz5Io1W06/vYGeV4bSM2j6AVPr4yMbPzz84KOT8+X+f9pefn8diseLiElq3TqJQKMXnCm2vm0ym4nOFiYnJNBrNtnmqi6/tfJY4JDRj9EsarUYiqXBWtZ7CtpNDpVLHjZ3k7x9w585tAIDQRySTS+ve47r/LUpVbVzsCxiG2SrR6XUE0djt4EMGD5dKa779fm3btu2dtaF35lzjiUntaDTaxm9XDx08wmgyjhg+ZtrUGStXLfpy9ZIuXbpfvHi+8MypqVPeYLPZoeywwYPSf/r5B4vFIhaHHT78h1wum//JEgBAdEwclUpdu37F27PmJiW2m/rKmD69B0ZHxR44sIfH5dkOy7zK73/sPFOUP3DAMJmsRiqtSUhoY9uzP7325O4929q371xUlH84e7+LWm/fvnNOTlb2kQMCvs+efdvVatWDknuNTN0VH5cQERFVWvrAdpLHWZyW0VBx2AdzPt20+ZuN36yOj281YviYwYPTDUbDnr3bc48d9vcLeOP12S9NeDyN4HvvzuNyeX/s36VWq6KjYpcvXduxQxcAQEiw+OMPP/9l26bi4sLY2Bc6tO+Sd/yIVquJjo5bvmzdkzsDLZ5YHGY2mb77fi2Xy8vIeGnC+JdtR+5lZaU7d/3y67ZNab36jx83ue6knnNNnzZTLpNu2Pglny9IfzFj/NjJX61bfulS81u+AAAKWUlEQVTyn7Yvy642rdtWVJT16e3MyV/tz/d0PkduMoB2fRxOEwW/7M1lvTP8gyGbmWzP2rJOA/09fX6NRiz8bC5uwVcsW/esC14tUNBoRLdhfk/+Cs3rgjjHsbwjecePXLhwds3q75y7ZpRRxDmOHDlgxs1frNzgrMtLdVBGEedocJnQidAYZwR2KKMI7FBGEdihjCKwQxlFYIcyisAOZRSBHcooAjuUUQR2KKMI7OxfC2WwKAR8D+V4Jj4BdAp8f4A+AXQKuvxsD51FodPsP5PW/tfIF9FrHnr2jCMlVzV+IdA9CQmjU+QVaLIMO6oe6Pl+9v987Wc0MJzprntmXKK2xhSVyMHo0HWk4hiWTk3Og7ghRwEgKML+07Id9qOhcayCfRIXF+Yqx7dX2B0tS7pWXQSycsOdS8omvNeLFOyThMax+CL7j/du7NngN84q71zWtOvtJwpi0DDo+qQn6TW4Umou2CsZMztUGAjdht7GarUe+m9lQARbHMsRBdrvObyEBbcqqoxX8uXxHbiJ3Xwcva2xjAIASm5oL+fXSkoMNAz2bb9vCFNZY4pJ4qQM9eMKYD8wuXhCcfuCGqNTa2tIm96CsBIUQHHbndB2CiCswZGsdr2F0YmNPYj6KRmtY9RD9wzjBqxWwOJ4QGdfH45bLWbXTjrXiGXLlqWkpAwcOJCsApjsJn1fTe1vmrg65JlgGAUjbwNlpZiomAX+bxb2+hAEZdR7iUSiuhnLYIYy6r0UCoVHTEiIMuq9AgICUD+KQK2mpgb1owjU/P39mUwPuIiAMuq9pFKpRzwNBmXUezGZTNtjXiDnASUiLmI0Ghuf8xYSKKMI7FBGvRc694TADp17QhDnQBn1Xj4+PnS6/aHvUEEZ9V5KpdJsNpNdxdOhjCKwQxn1XiwWC53DR6BmMBjQOXwEalQqlcQb7poOZdR7EQTRxDsuyYUyisAOZdR7sVgsmoNpwKCCMuq9DAaDxWIhu4qnQxlFYIcy6r3QvcsI7NC9ywjiHCij3guNcUZgh8Y4I4hzoIx6L3TvMgI7dO8yAjt0zITADh0zIbDj8/kYBvvDLVBGvZparcZxD3igGcqo90L9KAI71I8isPP390fH9QjUpFKpRxzXN/U5d0iLMXz48MrKSqvVSqFQ6v7bvn37zZs3k12afagf9Tq9evUCANjuWrb9VygUTps2jey6HEIZ9TqTJ08ODw+v/0p8fLwtuHBCGfU6YrG4X79+df/08fHJzMwktaKnQBn1RmPHjo2MjLT9HB8fn5aWRnZFjUEZ9UYhISG9e/e2daITJ04ku5ynQBn1UmPHjg0PD4+NjbWFFWbo3JMHqC413Lumkzw06DUWvcbCYFK1aidcH8JxnEqlOmWYsyiIpVebWVyajx89JIoZ247LFzlthmiUUXhZLNbibPnNYhXGoPECuEwuHWPQMCaNhtEAbNPdWQFuwnGjBccJrUyvlemYHFq7Xj7t0nyef90oo5AqOiS/dEIe0tqPH8ChMz1g5EcDerVRWaHRynU9R/oldOI/z6pQRqEjr7Yc2VqJcVhBcSKya3leJr25+o6CwwMjZoQ0e4gVyihcyu/ps36sjE0NpTM8r+90RFmlVTxUTFkYQaU2Zx8FZRQi0grj4a3VkR3FZBfifEatSXpPOmFOKEZ/5kM0dO4JFnKJ8cD3khYZUAAAk8sIiA/8eUlpM5ZFGYXFji8fxXQNJbsKF2KwsYBYv9+/qXjWBVFGoXB4sySyfRClWbtrHkQQyLEA+tXC2mdaCmWUfJUlemmlmefPIbsQd/CNEJ45KHumRVBGyZe/T+Yf40t2FW5Cw6h+EYLiI/KmL4IySjLJQwNuoXBFLLILsePcnwfmLuyqUkmdu1q/COGt8+qmvx9llGT3r2mYPBgD6jo0OpVKo1aW6Jv4fpRRkt27quMHeMWeaH0cX86dy9omvrnlXMzwRDo1TqNTWXyX3EBsMhmO5H136WqO2WwM8I/s03NS+7YDAQAFRTsuX8tLS514JO87tVoaKm41buQngQFRtqXKK/7en/3Vo/KbAr5/gF+EKwoDAPD82IpqZRPfjDJKJr3aYtS55AlJBEFs2f6BQlHZL20qj+d77/5f23YvMJr0XTuNAACUll3PP7N93Mj5Fgu+9+CKnb8vfmfGFgBAVc2D77bM5HKEwwa+RaNix0656k5RjEErf2Ro6ptdVATSFFqVhc5yyVdw7ebJkgeX53+w30cQAADomDzYaNIVnt1lyygA4JVJqwV8PwBAz27js46u1+qUXI7P4ZwNFAp19ozNPK4IAEChUn/PWuWK8jAmzaBt6h8nyiiZDFoLk+eSDf2tv89YCHz5V6PrXiEIC5vFq/snk8G2/SAShgAAVKoaOsb8+25x9y5jbAEFANCorooHhULxD2NrlWauz9OHQqOMkonOpBq1ZlesWa2RCfj+b77yTf0XqfYyh9HotgSr1FKLBfcVhbiinifJKvQsbpPihzJKJo6Ahhtdsj/KYQs0WoVIGEKnM5u4iK371GgUrqinAdxswehUGtaka7/o3BOZuALMRRmNi+1CEJai8/vqXjGannI+ksXi+vuFX7lxHMdd0rXXhxstbH5TH/mM+lEy8YSY1WrFjRaM6eRndHdqN/Tcn/sP5WxQ1FaGhiRUSO5cu3nqo3d2MRiNXS8Y1Pe13/Z+vuHH11I6plOo1NNndzm3qjp6lTEwrKlXLlBGSRaVyFHVaH3DBM5dLYbRX5/6dXbuN5eu5p698EeAX0RqSgaN9pSvu2O7IXq9+tSZ7YdyNwQFxESGJ9VIHzq3MButTJc8pKkfGY3DJ1nJde2Zw7VhycFkF+JWN/JK3lwVS6M1aX8U9aMki07inj4gs+AEDXN4bLBgWX+7r/M4Qo3OzljMxFZpE8d87qwK9QbNsjUj7f4qMrztw0fXnnydy/b5ZM7vjlaorNLGdxA0MaCoH4XC9SLl9XP64FYBjt4gV9gfu47jZgyzc36RwWDXneN8fgRB1Col9n9npQCKnfxQKFSR0OGW4U5h6cQPw3nCpvaPqB8lX1Kqz4VjCqPOzOTYP6HtKyLzJicqlerEAuSPVLHJ3KYHFJ17gsXgKYGy+882Ot0TWXBCXaXqO97hFsMulFEoiKM5Sd15Vf84eTQxbErOlWW8LbZNHt10KKOwSO7pE5vIrLzdYmNadlUy7NXgZsxVhjIKkc4DhFEvYJLbNWQX4mSEhbh39tGAiX7iaHYzFkfH9dC5Uay6flbDDxFyfJp6qR1mtRWaits1mR9FCAOaOcILZRRGVY8Mx3fUEIAWGOvLcHCwDz9VjVZ6TxEcxRz2ynNdoUAZhde9q5pL+Sq1HOf6cfiBHBaXAf8kEYSF0CoMGqlOI9UFRDB7jfDzC3nerQHKKOyqywz3LmtL/zFIy/QYg8pgYyw+hhsJsuv6Hyw+XVVjMOlxNg/jCbGEjrzopGc7CdoIlFFPotdYtCrcqIMroLZx9Ww+lSvAGCznH4WjjCKwQ+eeENihjCKwQxlFYIcyisAOZRSBHcooArv/A6atJaQAU6mvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import json\n",
        "from typing import (\n",
        "    Annotated,\n",
        "    Sequence,\n",
        "    TypedDict,\n",
        "    List,\n",
        ")\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# LangChain / LangGraph imports\n",
        "from langchain_core.messages import (\n",
        "    SystemMessage,\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    BaseMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "from langchain_core.tools import StructuredTool\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "from langgraph.prebuilt import InjectedStore\n",
        "from langgraph.store.base import BaseStore\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain.embeddings import init_embeddings\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "###############################################################################\n",
        "# 1. Initialize memory + config\n",
        "###############################################################################\n",
        "in_memory_store = InMemoryStore(\n",
        "    index={\n",
        "        \"embed\": init_embeddings(\"huggingface:sentence-transformers/all-MiniLM-L6-v2\"),\n",
        "        \"dims\": 384,  # Embedding dimensions\n",
        "    }\n",
        ")\n",
        "\n",
        "# A memory saver to checkpoint conversation states\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "# Initialize config with user & thread info\n",
        "config = {}\n",
        "config[\"configurable\"] = {\n",
        "    \"user_id\": \"user_1\",\n",
        "    \"thread_id\": 0,\n",
        "}\n",
        "\n",
        "###############################################################################\n",
        "# 2. Define MessagesState\n",
        "###############################################################################\n",
        "class MessagesState(TypedDict):\n",
        "    \"\"\"The state of the agent.\n",
        "\n",
        "    The key 'messages' uses add_messages as a reducer,\n",
        "    so each time this state is updated, new messages are appended.\n",
        "    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "###############################################################################\n",
        "# 3. Memory Tools\n",
        "###############################################################################\n",
        "def save_memory(summary_text: str, *, config: RunnableConfig, store: Annotated[BaseStore, InjectedStore()]) -> str:\n",
        "    \"\"\"Save the given memory for the current user.\"\"\"\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\", \"unknown_user\")\n",
        "    namespace = (\"memories\", user_id)\n",
        "    store.put(namespace, f\"memory_{len(store.search(namespace))}\", {\"data\": summary_text})\n",
        "\n",
        "\n",
        "\n",
        "# Define a Pydantic schema for the save_memory tool\n",
        "class RecallMemory(BaseModel):\n",
        "    query: str = Field(..., title=\"Search Text\", description=\"The text to search from memories for similiar records.\")\n",
        "    top_k: int = Field(10, title=\"Number of Results\", description=\"Number of results to retrieve.\")\n",
        "\n",
        "def recall_memory(query: str, top_k: int = 10) -> str:\n",
        "    \"\"\"Retrieve user memories from in_memory_store.\"\"\"\n",
        "    user_id = config.get(\"configurable\", {}).get(\"user_id\", \"unknown_user\")\n",
        "    namespace = (\"memories\", user_id)\n",
        "\n",
        "    memories = [m.value[\"data\"] for m in in_memory_store.search(namespace, query, limit=top_k)]\n",
        "    joined = f\"User memories: {', '.join(memories)}\"\n",
        "    return joined\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "recall_memory_tool = StructuredTool.from_function(\n",
        "    func=recall_memory,\n",
        "    name=\"Recall Memory Tool\",\n",
        "    description=\"\"\"\n",
        "      Retrieve memories relevant to the user's query from the vector store of saved data.\n",
        "      Input must be a JSON string with the schema:\n",
        "      {\n",
        "        \"query\": \"<text to search>\",\n",
        "        \"top_k\": 10\n",
        "      }\n",
        "      \"\"\",\n",
        "    args_schema=RecallMemory,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "###############################################################################\n",
        "# 4. Summarize Node\n",
        "###############################################################################\n",
        "# Define a Pydantic schema for the Summary tool\n",
        "class SummariseConversation(BaseModel):\n",
        "  summary_text: str = Field(..., title=\"text\", description=\"Summary of Conversation\")\n",
        "\n",
        "def summarise_node(summary_text: str):\n",
        "    \"\"\"\n",
        "    Final node that summarizes the entire conversation for the current thread,\n",
        "    saves it in memory, increments the thread_id, and ends the conversation.\n",
        "    \"\"\"\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "    current_thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "    new_thread_id = str(int(current_thread_id) + 1)\n",
        "\n",
        "    # Save summary to the user's memory under the new thread ID\n",
        "    config_for_saving = {\n",
        "        \"configurable\": {\n",
        "            \"user_id\": user_id,\n",
        "            \"thread_id\": new_thread_id\n",
        "        }\n",
        "    }\n",
        "    save_memory(summary_text, config=config_for_saving, store=in_memory_store)\n",
        "\n",
        "# Create a StructuredTool from the function\n",
        "summarise_tool = StructuredTool.from_function(\n",
        "    func=summarise_node,\n",
        "    name=\"Summary Tool\",\n",
        "    description=\"\"\"\n",
        "      Summarize the current conversation in no more than\n",
        "      1000 words. Also retain any unanswered quiz questions along with\n",
        "      your internal answers so the next conversation thread can continue.\n",
        "      Do not reveal solutions to the user yet. Use this tool to save\n",
        "      the current conversation to memory and then end the conversation.\n",
        "\n",
        "      Input must be a JSON string with the schema:\n",
        "      {\n",
        "        \"summary_text\": \"<Your summary of conversation>\"\n",
        "        }\n",
        "      \"\"\",\n",
        "    args_schema=SummariseConversation,\n",
        "    response_format=\"content\",\n",
        "    return_direct=False,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "###############################################################################\n",
        "# 5. Build the Graph\n",
        "###############################################################################\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "\n",
        "# The built-in ToolNode from langgraph that calls any declared tools\n",
        "tools = [\n",
        "    mcq_retriever_tool,\n",
        "    web_extraction_tool,\n",
        "    ensemble_retriever_tool,\n",
        "    general_retriever_tool,\n",
        "    in_memory_retriever_tool,\n",
        "    recall_memory_tool,\n",
        "    summarise_node,\n",
        "]\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "end_node = ToolNode(tools=[summarise_node])\n",
        "\n",
        "# Wrap your model with tools\n",
        "llm_with_tools = model.bind_tools(tools)\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 6. The agent's main node: call_model\n",
        "###############################################################################\n",
        "\n",
        "def call_model(state: MessagesState, config: RunnableConfig):\n",
        "    \"\"\"\n",
        "    The main agent node that calls the LLM with the user + system messages.\n",
        "    If the LLM requests a tool call, we'll route to the 'tools' node next\n",
        "    (depending on the condition).\n",
        "    \"\"\"\n",
        "    response = llm_with_tools.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 7. Add Nodes & Edges, Then Compile\n",
        "###############################################################################\n",
        "graph_builder.add_node(\"agent\", call_model)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "graph_builder.add_node(\"summary\", end_node)\n",
        "\n",
        "# Entry point\n",
        "graph_builder.set_entry_point(\"agent\")\n",
        "\n",
        "# If LLM requests a tool, go to \"tools\", otherwise go to \"summary\"\n",
        "# def router(state: MessagesState):\n",
        "#     if state[\"value\"] == \"end\":\n",
        "#         return \"summary\"\n",
        "#     else:\n",
        "#         return \"tools\"\n",
        "graph_builder.add_conditional_edges(\"agent\", tools_condition, [\"tools\", \"summary\"])\n",
        "\n",
        "#graph_builder.add_conditional_edges(\"agent\", tools_condition)\n",
        "#graph_builder.add_conditional_edges(\"tools\",lambda state: \"success\" if state.tool_output else \"fail\", {\"success\": \"agent\", \"fail\": \"agent\"})\n",
        "\n",
        "# If we used a tool, return to the agent for final answer or more tools\n",
        "graph_builder.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "# Final node\n",
        "graph_builder.set_finish_point(\"summary\")\n",
        "\n",
        "# Compile\n",
        "graph = graph_builder.compile(checkpointer=checkpointer, store=in_memory_store)\n",
        "\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "2hbnTzV8d8po",
        "outputId": "28ca8ae7-9bca-4d2e-e3c1-9af6d69b3283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversation Thread ID: 0 -> 1\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Based on your memory, provide a summary of our conversation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "3 validation errors for HumanMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value=[HumanMessage(content='Ba...ff0-bca1-f2b36597a8c7')], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].0.str\n  Input should be a valid string [type=string_type, input_value=HumanMessage(content='Bas...4ff0-bca1-f2b36597a8c7'), input_type=HumanMessage]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].0.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=HumanMessage(content='Bas...4ff0-bca1-f2b36597a8c7'), input_type=HumanMessage]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-60a65d2ce406>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mstream_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"values\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   1725\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    231\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m                 )\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-89c04fcc66da>\u001b[0m in \u001b[0;36mcall_model\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mdepending\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \"\"\"\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_with_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4719\u001b[0m         \"\"\"\n\u001b[1;32m   4720\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"func\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4721\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m   4722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4723\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1920\u001b[0m             output = cast(\n\u001b[1;32m   1921\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1922\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1923\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1924\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   4573\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4575\u001b[0;31m             output = call_func_with_variable_args(\n\u001b[0m\u001b[1;32m   4576\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4577\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-f23e936b602e>\u001b[0m in \u001b[0;36m_call_llm\u001b[0;34m(input_str, config)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_call_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_str\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunnableConfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# If you want real tool usage, you'd parse input_str or augment it with tools.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/messages/human.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, content, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAdditional\u001b[0m \u001b[0mfields\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mpass\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/messages/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, content, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAdditional\u001b[0m \u001b[0mfields\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mpass\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 3 validation errors for HumanMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value=[HumanMessage(content='Ba...ff0-bca1-f2b36597a8c7')], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].0.str\n  Input should be a valid string [type=string_type, input_value=HumanMessage(content='Bas...4ff0-bca1-f2b36597a8c7'), input_type=HumanMessage]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].0.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=HumanMessage(content='Bas...4ff0-bca1-f2b36597a8c7'), input_type=HumanMessage]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type"
          ]
        }
      ],
      "source": [
        "# URL\n",
        "url1 = \"https://www.ibm.com/think/topics/artificial-intelligence\"\n",
        "url2 = \"https://www.ibm.com/think/topics/machine-learning\"\n",
        "\n",
        "question_1 = (f\"Find out about Deep Learning from databases. Then search from website: {url1} and {url2} with web extraction tool\")\n",
        "question_2 = \"Provide 5 MCQ questions on Artificial Intelligence to help me with practice.\"\n",
        "question_3 = \"Here are my answers: 1. A, 2. B, 3. C, 4. D, 5. E. Please check mu answer, provide reasons for my wrong answers and provide the correct answers with explanations.\"\n",
        "question_4 = \"Provide another 5 MCQ questions on Artificial Intelligence to help me with practice.\"\n",
        "question_5 = \"Here are my answers: 1. A, 2. B, 3. C, 4. D, 5. E. Please check mu answer, provide reasons for my wrong answers and provide the correct answers with explanations.\"\n",
        "question_6 = \"Provide a study quide to help me learn for my wrong answers for the MCQ questions.\"\n",
        "question_7 = \"Based on your reference databases only, provide a study quide on Deep Learning.\"\n",
        "question_8 = \"Based on your memory, provide a summary of our conversation.\"\n",
        "\n",
        "# Grab the current user_id and thread_id from config\n",
        "user_id = \"user_1\"\n",
        "\n",
        "# Get last thread_id\n",
        "last_thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "thread_id = str(int(last_thread_id) + 1)\n",
        "\n",
        "# Print Config\n",
        "print(f\"Conversation Thread ID: {last_thread_id} -> {thread_id}\")\n",
        "\n",
        "# Update the config with the new thread_id\n",
        "config = {\"configurable\": {\"thread_id\": thread_id, \"user_id\": user_id}}\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": question_8}]},\n",
        "    config, # Pass the thread-level persistence to the graph\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KmeoL-_lcs8Y",
        "s_2h8tWUc9yA",
        "T_ki8T8DdE8C"
      ],
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNAafbhOnxTFshrfJpZbtVK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "770f028885af4f9392e82d7bbc714879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16bf7cd9702948ef88feb8a0e7515677",
              "IPY_MODEL_0f5d34399a024201bd1c421eb5363d49",
              "IPY_MODEL_d646df1f3dad448192fc7f18fb8613c8"
            ],
            "layout": "IPY_MODEL_897a5d61912541179f776754908b89e6"
          }
        },
        "16bf7cd9702948ef88feb8a0e7515677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8dfb7e1449c44e88d0e10c750cd120f",
            "placeholder": "​",
            "style": "IPY_MODEL_f24520b133674d2e84828aaaa7aa55c0",
            "value": "config.json: 100%"
          }
        },
        "0f5d34399a024201bd1c421eb5363d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca2b82e08bd9418eb14a922fa86f6bfc",
            "max": 792,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e27eb3f0cbb64c5d9a41e9eaa93ee1cb",
            "value": 792
          }
        },
        "d646df1f3dad448192fc7f18fb8613c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41b86ad9f3a44c0fb5be585a66e6555f",
            "placeholder": "​",
            "style": "IPY_MODEL_5391b26c09b24610b2feeb07121fe36d",
            "value": " 792/792 [00:00&lt;00:00, 57.6kB/s]"
          }
        },
        "897a5d61912541179f776754908b89e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8dfb7e1449c44e88d0e10c750cd120f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f24520b133674d2e84828aaaa7aa55c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca2b82e08bd9418eb14a922fa86f6bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e27eb3f0cbb64c5d9a41e9eaa93ee1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41b86ad9f3a44c0fb5be585a66e6555f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5391b26c09b24610b2feeb07121fe36d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89a90466976e4e548d44235cd040d4ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_605aa96cbede40f0b9f143a7b7427df2",
              "IPY_MODEL_fa5cc9a685b14a1bb22913588ecf16da",
              "IPY_MODEL_75e23f90ce654da2a04fbab4cfb49bb4"
            ],
            "layout": "IPY_MODEL_53b20ca01a5c4c5894bee59aceffd456"
          }
        },
        "605aa96cbede40f0b9f143a7b7427df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2ea99ff81e4459cac0f2500f16350e4",
            "placeholder": "​",
            "style": "IPY_MODEL_71ad3e37a0974f0481151e372a3c5bbd",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "fa5cc9a685b14a1bb22913588ecf16da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8beba89a63748c0817b8c010c7f1799",
            "max": 3764,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07b6da1e83084ccca9059a3c8e030891",
            "value": 3764
          }
        },
        "75e23f90ce654da2a04fbab4cfb49bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4b70ef492784b13804ed5d2b6d1830a",
            "placeholder": "​",
            "style": "IPY_MODEL_1d99568f5490458c99f15a107ae357ea",
            "value": " 3.76k/3.76k [00:00&lt;00:00, 231kB/s]"
          }
        },
        "53b20ca01a5c4c5894bee59aceffd456": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2ea99ff81e4459cac0f2500f16350e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71ad3e37a0974f0481151e372a3c5bbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8beba89a63748c0817b8c010c7f1799": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07b6da1e83084ccca9059a3c8e030891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4b70ef492784b13804ed5d2b6d1830a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d99568f5490458c99f15a107ae357ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d80a73aed274023b8cbde3f59b360de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1848d43a49d42bf949bfeac87272afd",
              "IPY_MODEL_486a17181c9d46c0b46f102c30da0b7d",
              "IPY_MODEL_5f54ace96b9d40adb116b36076f734e4"
            ],
            "layout": "IPY_MODEL_bc0f7a37875b44f78e62574c3ad32abc"
          }
        },
        "a1848d43a49d42bf949bfeac87272afd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9309bbebf414c8586ea84a452eb6ff1",
            "placeholder": "​",
            "style": "IPY_MODEL_ababd225747f437f9d769a23aedbbd0d",
            "value": "vocab.json: 100%"
          }
        },
        "486a17181c9d46c0b46f102c30da0b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bd3fa0008a54f51b89f3ffc334f8915",
            "max": 800662,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05960370cd4d4affbeecc954f114c387",
            "value": 800662
          }
        },
        "5f54ace96b9d40adb116b36076f734e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_033b160d32664bf484db3fc69d95f856",
            "placeholder": "​",
            "style": "IPY_MODEL_4869693d21cd4c719d23e4705f3268bc",
            "value": " 801k/801k [00:00&lt;00:00, 11.3MB/s]"
          }
        },
        "bc0f7a37875b44f78e62574c3ad32abc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9309bbebf414c8586ea84a452eb6ff1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ababd225747f437f9d769a23aedbbd0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bd3fa0008a54f51b89f3ffc334f8915": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05960370cd4d4affbeecc954f114c387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "033b160d32664bf484db3fc69d95f856": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4869693d21cd4c719d23e4705f3268bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e7267a30c39446c9023fc01904ff3bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e004d1523bc49ce91a0e13bfe21f971",
              "IPY_MODEL_897038ad2303471da212889a4306d536",
              "IPY_MODEL_f7176074dba142339349a1fd89895753"
            ],
            "layout": "IPY_MODEL_9fca3dd3f5a742fd96dc78c6f72c8ea5"
          }
        },
        "1e004d1523bc49ce91a0e13bfe21f971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5677eb6d99914ef582fe37b5e88746db",
            "placeholder": "​",
            "style": "IPY_MODEL_50ce67ea0ef04fa69a7078f43d3f595b",
            "value": "merges.txt: 100%"
          }
        },
        "897038ad2303471da212889a4306d536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84a5e7c71d14442dab690bad1fef7e1b",
            "max": 466391,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2593399f84b34e769f5fbad6e804fcca",
            "value": 466391
          }
        },
        "f7176074dba142339349a1fd89895753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1e0dea3e0f049b5946ac04f639e8db1",
            "placeholder": "​",
            "style": "IPY_MODEL_6101d082fde94635988b73a7323a2f1d",
            "value": " 466k/466k [00:00&lt;00:00, 3.60MB/s]"
          }
        },
        "9fca3dd3f5a742fd96dc78c6f72c8ea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5677eb6d99914ef582fe37b5e88746db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50ce67ea0ef04fa69a7078f43d3f595b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84a5e7c71d14442dab690bad1fef7e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2593399f84b34e769f5fbad6e804fcca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1e0dea3e0f049b5946ac04f639e8db1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6101d082fde94635988b73a7323a2f1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a4365bcab484d1eadf164cd83e07f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bddbcd1040cf413587a3d36e6b732a96",
              "IPY_MODEL_60d28df8c72842a89c66f2357cee2377",
              "IPY_MODEL_b50435e13cea4f3982568097233ab2de"
            ],
            "layout": "IPY_MODEL_65dacbd22a07491187557bb1707b5677"
          }
        },
        "bddbcd1040cf413587a3d36e6b732a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aabbd856b17404b95f00009702aeb54",
            "placeholder": "​",
            "style": "IPY_MODEL_639a15d578784b2aa39a96136125d4b3",
            "value": "tokenizer.json: 100%"
          }
        },
        "60d28df8c72842a89c66f2357cee2377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b2fd43a8934456286f159bd8f3e6732",
            "max": 2104556,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3de0fb22e45454d80aca6cdaf52ea48",
            "value": 2104556
          }
        },
        "b50435e13cea4f3982568097233ab2de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fe5af6bf42f4235a6374c3f39ac3110",
            "placeholder": "​",
            "style": "IPY_MODEL_2c48620a0daa4343b7712daf6c1f8713",
            "value": " 2.10M/2.10M [00:00&lt;00:00, 10.7MB/s]"
          }
        },
        "65dacbd22a07491187557bb1707b5677": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aabbd856b17404b95f00009702aeb54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "639a15d578784b2aa39a96136125d4b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b2fd43a8934456286f159bd8f3e6732": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3de0fb22e45454d80aca6cdaf52ea48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fe5af6bf42f4235a6374c3f39ac3110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c48620a0daa4343b7712daf6c1f8713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef325101d5d64f4388ed7cd61396f5d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2e58daff32d41058fbc4c04e764e3e3",
              "IPY_MODEL_b8f37919337349f2be758a9cbf8c8a2a",
              "IPY_MODEL_7816fe00c5cb4e68928bbff96e0cfec6"
            ],
            "layout": "IPY_MODEL_204b5e765c0b449fb97b842e0707e202"
          }
        },
        "f2e58daff32d41058fbc4c04e764e3e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43bef004f83d4676b45e11b4d5ce306c",
            "placeholder": "​",
            "style": "IPY_MODEL_eeb39ed614a644d499b19f0688bf3388",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "b8f37919337349f2be758a9cbf8c8a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77986ef0e37a4c1b85b1ac9cdd159cc0",
            "max": 655,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab22126c60e64fdfb1d07dd2a89fb13c",
            "value": 655
          }
        },
        "7816fe00c5cb4e68928bbff96e0cfec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bdbed976bc444f4a9a2517af21986bb",
            "placeholder": "​",
            "style": "IPY_MODEL_80767c88cf354d8c958ba6ab540bec8c",
            "value": " 655/655 [00:00&lt;00:00, 40.5kB/s]"
          }
        },
        "204b5e765c0b449fb97b842e0707e202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43bef004f83d4676b45e11b4d5ce306c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb39ed614a644d499b19f0688bf3388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77986ef0e37a4c1b85b1ac9cdd159cc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab22126c60e64fdfb1d07dd2a89fb13c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3bdbed976bc444f4a9a2517af21986bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80767c88cf354d8c958ba6ab540bec8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86e17d77e1eb42f9be9644a61693603d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf77a7a6e6d74f6da05928ee5c3901b4",
              "IPY_MODEL_f5d5d043eebb41f4ab22491729856787",
              "IPY_MODEL_3970f6fae8af4613928bb5a60307c028"
            ],
            "layout": "IPY_MODEL_70f18d8c74a94b549bcc492e09dc8373"
          }
        },
        "cf77a7a6e6d74f6da05928ee5c3901b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e4483e50f174d2f875d938bfb97c4fd",
            "placeholder": "​",
            "style": "IPY_MODEL_ff35643236f246e28f8512cb08cc2d93",
            "value": "generation_config.json: 100%"
          }
        },
        "f5d5d043eebb41f4ab22491729856787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4080b87eed094001bae4aef25ad67926",
            "max": 132,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a578d517f0254620be90965e64c0ad30",
            "value": 132
          }
        },
        "3970f6fae8af4613928bb5a60307c028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50790f8cd8d944ca883e589da30223cc",
            "placeholder": "​",
            "style": "IPY_MODEL_75d51412fb3b44df92f83a8159aed242",
            "value": " 132/132 [00:00&lt;00:00, 6.25kB/s]"
          }
        },
        "70f18d8c74a94b549bcc492e09dc8373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e4483e50f174d2f875d938bfb97c4fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff35643236f246e28f8512cb08cc2d93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4080b87eed094001bae4aef25ad67926": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a578d517f0254620be90965e64c0ad30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50790f8cd8d944ca883e589da30223cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75d51412fb3b44df92f83a8159aed242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4d6407b792e48f998c47e0109f6a498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4022c089a83b4f8097c665685f7a2ce2",
              "IPY_MODEL_c8afbd7c635d4b628b35102fa7b649c3",
              "IPY_MODEL_6eb6ae513fb64927b1b357c28c22e6ad"
            ],
            "layout": "IPY_MODEL_5f526680d15d440ebe78556b5273b992"
          }
        },
        "4022c089a83b4f8097c665685f7a2ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3fc11603cf544568944077a39f4ea6f",
            "placeholder": "​",
            "style": "IPY_MODEL_4c6efd5d702c424fb47c89a88d9608ce",
            "value": "model.safetensors: 100%"
          }
        },
        "c8afbd7c635d4b628b35102fa7b649c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_997fce5689fd44e989b131109deb4589",
            "max": 3422777952,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de5210e75e384cba91d5e1ebb73c782f",
            "value": 3422777952
          }
        },
        "6eb6ae513fb64927b1b357c28c22e6ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cecb5515399c41c6beed3043f5449f49",
            "placeholder": "​",
            "style": "IPY_MODEL_0caac675384e4423948096bf8004f80b",
            "value": " 3.42G/3.42G [01:21&lt;00:00, 42.1MB/s]"
          }
        },
        "5f526680d15d440ebe78556b5273b992": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3fc11603cf544568944077a39f4ea6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c6efd5d702c424fb47c89a88d9608ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "997fce5689fd44e989b131109deb4589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de5210e75e384cba91d5e1ebb73c782f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cecb5515399c41c6beed3043f5449f49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0caac675384e4423948096bf8004f80b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a0aede5c40c42608a454242cb38d1bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05fddb3210ff47dfb2ca26611a862665",
              "IPY_MODEL_026da44b05864637a7600a5b16ddeb90",
              "IPY_MODEL_f5c7d57156a74ee09560812c0d77b054"
            ],
            "layout": "IPY_MODEL_3c9cc8340d9d48aebd8f59c75b98aca6"
          }
        },
        "05fddb3210ff47dfb2ca26611a862665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be38bce41fcc48949c060cdeb7c362df",
            "placeholder": "​",
            "style": "IPY_MODEL_eed7922cb2fa4045956277a08b535ef6",
            "value": ""
          }
        },
        "026da44b05864637a7600a5b16ddeb90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2476fea509847b184f2aed5556ee6f5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a3aef9a80294c84b952061a5f6ea818",
            "value": 1
          }
        },
        "f5c7d57156a74ee09560812c0d77b054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_994e19edf255417d972ba925f0fb7bb5",
            "placeholder": "​",
            "style": "IPY_MODEL_943c1bdc51e741529fed198646746241",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06&lt;00:00,  6.26s/it]\n"
          }
        },
        "3c9cc8340d9d48aebd8f59c75b98aca6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be38bce41fcc48949c060cdeb7c362df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eed7922cb2fa4045956277a08b535ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2476fea509847b184f2aed5556ee6f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a3aef9a80294c84b952061a5f6ea818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "994e19edf255417d972ba925f0fb7bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "943c1bdc51e741529fed198646746241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}